<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine Controller Manager Design</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">Introduction</a></li><li class="chapter-item expanded "><a href="k8s_facilities.html"><strong aria-hidden="true">1.</strong> Kubernetes Facilities</a></li><li class="chapter-item expanded "><a href="mcm_facilities.html"><strong aria-hidden="true">2.</strong> MCM Facilities</a></li><li class="chapter-item expanded "><a href="machine-controller/index.html"><strong aria-hidden="true">3.</strong> Machine Controller</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="machine-controller/reconcile-cluster-machine-class.html"><strong aria-hidden="true">3.1.</strong> Reconcile Cluster Machine Class</a></li><li class="chapter-item expanded "><a href="machine-controller/reconcile-cluster-secret.html"><strong aria-hidden="true">3.2.</strong> Reconcile Cluster Secret</a></li><li class="chapter-item expanded "><a href="machine-controller/cluster_machine_reconcile.html"><strong aria-hidden="true">3.3.</strong> Reconcile Cluster Machine</a></li><li class="chapter-item expanded "><a href="machine-controller/mc_helper_methods.html"><strong aria-hidden="true">3.4.</strong> Machine Controller Helper Methods</a></li><li class="chapter-item expanded "><a href="machine-controller/node_drain.html"><strong aria-hidden="true">3.5.</strong> Node Drain</a></li><li class="chapter-item expanded "><a href="machine-controller/orphan-safety.html"><strong aria-hidden="true">3.6.</strong> Orphan / Safety Jobs</a></li></ol></li><li class="chapter-item expanded "><a href="machine-controller-manager/index.html"><strong aria-hidden="true">4.</strong> Machine Controller Manager</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="machine-controller-manager/reconcile-cluster-machine-set.html"><strong aria-hidden="true">4.1.</strong> Reconcile Cluster Machine Set</a></li><li class="chapter-item expanded "><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html"><strong aria-hidden="true">4.2.</strong> Reconcile Cluster Machine Deployment</a></li></ol></li><li class="chapter-item expanded "><a href="issues.html"><strong aria-hidden="true">5.</strong> Issues</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Machine Controller Manager Design</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <ul>
<li><a href="intro.html#introduction">Introduction</a>
<ul>
<li><a href="intro.html#project-structure">Project Structure</a></li>
<li><a href="intro.html#deployment-structure">Deployment Structure</a></li>
</ul>
</li>
<li><a href="intro.html#local-development-tips">Local Development Tips</a>
<ul>
<li><a href="intro.html#running-mcm-locally">Running MCM Locally</a></li>
</ul>
</li>
<li><a href="intro.html#change-log">Change Log</a></li>
</ul>
<p>Current location: <a href="https://elankath.github.io/mcmdesign/">MCM Design Book</a>. </p>
<p>(üöß Please see <a href="intro.html#change-log">Change Log</a> for new additions/corrections.Please Check on 8th Oct for v3.1 release!üèó)</p>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>A Kubernetes Controller is a program that watches for lifecycle events on specific resources and triggers one or more <em>reconcile</em> functions in response. A <em>reconcile function</em> is called with the <em>Namespace</em> and <em>Name</em> of an object corresponding to the resource and its job is to make the object <em>Status</em> match the declared state in the object <em>Spec</em>. </p>
<p>Machine Controller Manager aka MCM is a group of cooperative controllers that manage the lifecycle of the worker machines, machine-classes machine-sets and machine deployments. All these objects are custom resources.</p>
<ul>
<li>A worker <a href="./mcm_facilities.html#machine">Machine</a> is a provider specific VM/instance that corresponds to a k8s <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">Node</a>. (k8s doesn't bring up nodes by its own, the MCM does so by using cloud provider API's abstracted by the <a href="./mcm_facilities.html#driver">Driver</a> facade to bring up machines and map them to nodes)</li>
<li>A <a href="./mcm_facilities.html#machineclass">MachineClass</a> represents a template that contains cloud provider specific details used to create machines.</li>
<li>A <a href="./mcm_facilities.html#machineset">MachineSet</a> ensures that the specified number of <code>Machine</code> replicas are running at a given point of time. Analogoues to k8s <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSets</a>.</li>
<li>A <a href="./mcm_facilities.html#machinedeployment">MachineDeployment</a> provides a declarative update for <code>MachineSet</code> and <code>Machines</code>. Analogous to k8s <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>. </li>
</ul>
<p>All the custom resources (<code>Machine-*</code> objects) mentioned above are stored in the K8s <em>control cluster</em>. The nodes corresponding to the machines are created and registered in the <em>target cluster</em>. </p>
<p>For productive Gardener deployments, the <em>control cluster</em> is the control plane of the shoot cluster and since the MCM is running in the shoot's control plane, the kubeconfig for the control cluster is generally specified as the <a href="https://github.com/kubernetes/client-go/tree/master/examples/in-cluster-client-configuration">In-Cluster Config</a>. The target cluster is the shoot cluster and hence the target cluster config is the shoot kube config.</p>
<h2 id="project-structure"><a class="header" href="#project-structure">Project Structure</a></h2>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TB
    subgraph MCM

    mcm[&quot;machine-controller-manager
    (Common MC Code, MachineSet, MachineDeploy controllers)&quot;]
    mcmlo[&quot;machine-controller-manager-provider-local
    (Machine Controller Local atop K8s Kind)&quot;]
    mcmaws[&quot;machine-controller-manager-provider-aws
    (Machine Controller for AWS)&quot;]
    mcmazure[&quot;machine-controller-manager-provider-azure
    (Machine Controller for Azure)&quot;]
    mcmgcp[&quot;machine-controller-manager-provider-gcp
    (Machine Controller for GCP)&quot;]
    mcmx[&quot;machine-controller-manager-provider-X
    (Machine Controller for equinox/openstack/etc)&quot;]
    end

    mcmlo--uses--&gt;mcm
    mcmaws--uses--&gt;mcm
    mcmazure--uses--&gt;mcm
    mcmgcp--uses--&gt;mcm
    mcmx--&gt;mcm
</pre>
<p>The MCM project is divided into:</p>
<ol>
<li>The <a href="https://github.com/gardener/machine-controller-manager">MCM Module</a>. This contains 
<ol>
<li>The <a href="https://github.com/gardener/machine-controller-manager/blob/51cea3373d8be7c78aee3f7a4664ccd31f439269/pkg/controller/controller.go#L421">MCM Controller Type</a> and <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/controller/controller.go#L62">MCM Controller Factory Method</a>. The <code>MCM Controller</code> is responsible for reconciling the <code>MachineDeployment</code> and <code>MachineSet</code> custom resources. </li>
<li><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/cmd/machine-controller-manager/controller_manager.go#L40">MCM Main</a> which creates and starts the MCM Controller.</li>
<li>The <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/controller.go#L252">MC Controller Type</a> and <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/controller.go#L77">MC Controller Factory Method</a>.
<ol>
<li>The <code>MC Controller</code> implements the reconciliation loop for <code>MachineClass</code> and <code>Machine</code> objects but delegates creation/updation/deletion/status-retrieval of Machines to the <code>Driver</code> facade. </li>
</ol>
</li>
<li>The <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/driver/driver.go#L28">Driver</a> facade that abstracts away the lifecycle operations on Machines and obtaining Machine status.</li>
<li>Utility Code leveraged by provider modules. </li>
</ol>
</li>
<li>The provider specific modules named as <code>machine-controller-manager-provider-&lt;providerName&gt;</code>. 
<ol>
<li>Contains a <em>main</em> file located at <code>cmd/machine-controller/main.go</code> that instantiate a <code>Driver</code> implementation (Ex: <a href="https://github.com/gardener/machine-controller-manager-provider-aws/blob/v0.13.0/pkg/aws/core.go#L56">AWSDriver</a>) and then create and start a <code>MC Controller</code> using the <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/controller.go#L77">MC Controller Factory Method</a>, passing the <code>Driver</code> impl.  In other worlds, each provider module starts its independent machine controller.</li>
<li>See <a href="https://github.com/gardener/machine-controller-manager/README.md">MCM README</a> for list of provider modules</li>
</ol>
</li>
</ol>
<p>The MCM leverages the <em>old-school</em> technique of writing controllers directly using <a href="https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md">client-go</a>. Skeleton code for client types is generated using <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/generating-clientset.md">client-gen</a>. A barebones example is illustrated in the <a href="https://github.com/kubernetes/sample-controller">sample controller</a>. </p>
<p>The Modern Way of writing controllers is by leveraging the <a href="https://github.com/kubernetes-sigs/controller-runtime">Controller Runtime</a> and generating skeletal code fur custom controllers using the <a href="https://book.kubebuilder.io/quick-start.html">Kubebuilder Tool</a>.</p>
<p>The MCM has a planned backlog to port the project to the controller runtime. The details of this will be documented in a separate proposal. (TODO: link me in future). </p>
<p>This book describes the current design of the MCM in order to aid code comprehension for development, enhancement and migratiion/port activities.</p>
<h2 id="deployment-structure"><a class="header" href="#deployment-structure">Deployment Structure</a></h2>
<p>The MCM Pod's are part of the <code>Deployment</code> named <code>machine-controller-manager</code> that resides in the shoot control plane. After logging into the shoot control plane (use <code>gardenctl</code>), you can the deployment details using <code>k get deploy machine-controller-manager -o yaml </code>. The MCM deployment has two containers:</p>
<ol>
<li><code>machine-controller-manager-provider-&lt;provider&gt;</code>. Ex: <code>machine-controller-manager-provider-aws</code>.  This container name is a bit misleading as it starts the provider specific machine controller main program responsible for reconciling machine-classes and machines. See <a href="./machine-controller/README.html">Machine Controller</a>. (Ideally the <code>-manager</code> should have been removed)</li>
</ol>
<p>Container command configured on AWS:</p>
<pre><code>./machine-controller
         --control-kubeconfig=inClusterConfig
         --machine-creation-timeout=20m
         --machine-drain-timeout=2h
         --machine-health-timeout=10m
         --namespace=shoot--i034796--tre
         --port=10259
         --target-kubeconfig=/var/run/secrets/gardener.cloud/shoot/generic-kubeconfig/kubeconfig`
</code></pre>
<ol start="2">
<li><code>&lt;provider&gt;-machine-controller-manager</code>. Ex: <code>aws-machine-controller-manager</code>. This container name is a bit misleading as it starts the machine deployment controller main program responsible for reconciling machine-deployments and machine-sets. (See: TODO: link me). Ideally it should have been called simply <code>machine-deployment-controller</code> as it is provider independent.</li>
</ol>
<p>Container command configured on AWS</p>
<pre><code>./machine-controller-manager
         --control-kubeconfig=inClusterConfig
         --delete-migrated-machine-class=true
         --machine-safety-apiserver-statuscheck-timeout=30s
         --machine-safety-apiserver-statuscheck-period=1m
         --machine-safety-orphan-vms-period=30m
         --machine-safety-overshooting-period=1m
         --namespace=shoot--i034796--tre
         --port=10258
         --safety-up=2
         --safety-down=1
         --target-kubeconfig=/var/run/secrets/gardener.cloud/shoot/generic-kubeconfig/kubeconfig
</code></pre>
<h1 id="local-development-tips"><a class="header" href="#local-development-tips">Local Development Tips</a></h1>
<p>First read <a href="https://github.com/elankath/machine-controller-manager/blob/master/docs/development/local_setup.md#local-development">Local Dev MCM</a></p>
<h2 id="running-mcm-locally"><a class="header" href="#running-mcm-locally">Running MCM Locally</a></h2>
<p>After setting up a shoot cluster in the dev landscape, you can run your local copy of MCM and MC to manage machines in the shoot cluster.</p>
<p>Example for AWS Shoot Cluster:</p>
<ol>
<li>Checkout <code>https://github.com/gardener/machine-controller-manager</code> and <code>https://github.com/gardener/machine-controller-manager-provider-aws/</code></li>
<li><code>cd machine-controller-manager</code> and run
<code>./hack/gardener_local_setup.sh --seed &lt;seedManagingShoot&gt; --shoot &lt;shootName&gt; --project &lt;userId&gt; --provider aws</code>
<ul>
<li>Ex: <code>./hack/gardener_local_setup.sh --seed aws-ha --shoot aw2 --project i034796 --provider aws</code> </li>
<li>The above will set the replica count of the <code>machine-controller-manager</code>  deployment in the shoot control plane to 0 and also set an annotations <code>dependency-watchdog.gardener.cloud/ignore-scaling</code> to prevent DWD from scalig it back up. Now, you can run your local dev copy.</li>
</ul>
</li>
<li>Inside the MCM directlry run <code>make start</code>
<ol>
<li>MCM controller should start without errors. Last line should look like: 
<code>I0920 14:11:28.615699   84778 deployment.go:433] Processing the machinedeployment &quot;shoot--i034796--aw2-a-z1&quot; (with replicas 1)</code></li>
</ol>
</li>
<li>Change to the provider directory. Ex <code>cd &lt;checkoutPath&gt;/machine-controllr-manager-provider-aws</code> and run <code>make start</code>
<ol>
<li>MC controller should start without errors. Last line should look like</li>
</ol>
</li>
</ol>
<pre><code>I0920 14:14:37.793684   86169 core.go:482] List machines request has been processed successfully
I0920 14:14:37.896720   86169 machine_safety.go:59] reconcileClusterMachineSafetyOrphanVMs: End, reSync-Period: 5m0s
</code></pre>
<h1 id="change-log"><a class="header" href="#change-log">Change Log</a></h1>
<ul>
<li>WIP Draft of Orphan/Safety</li>
<li>WIP for machine set controller.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="k8s_facilities.html#kubernetes-client-facilities">Kubernetes Client Facilities</a>
<ul>
<li><a href="k8s_facilities.html#k8s-apimachinery">K8s apimachinery</a>
<ul>
<li><a href="k8s_facilities.html#k8s-objects">K8s objects</a>
<ul>
<li><a href="k8s_facilities.html#typemeta">TypeMeta</a></li>
<li><a href="k8s_facilities.html#objectmeta">ObjectMeta</a>
<ul>
<li><a href="k8s_facilities.html#ownerreferences">OwnerReferences</a></li>
<li><a href="k8s_facilities.html#finalizers-and-deletion">Finalizers and Deletion</a></li>
<li><a href="k8s_facilities.html#diff-between-labels-and-annotations">Diff between Labels and Annotations</a></li>
<li><a href="k8s_facilities.html#rawextension">RawExtension</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#api-errors">API Errors</a>
<ul>
<li><a href="k8s_facilities.html#errorsisnotfound">errors.IsNotFound</a></li>
<li><a href="k8s_facilities.html#errorsisnotfound-1">errors.IsNotFound</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="k8s_facilities.html#api-machinery-utilities">API Machinery Utilities</a>
<ul>
<li><a href="k8s_facilities.html#waituntil">wait.Until</a></li>
<li><a href="k8s_facilities.html#waitpollimmediate">wait.PollImmediate</a></li>
<li><a href="k8s_facilities.html#waitbackoff">wait.Backoff</a></li>
<li><a href="k8s_facilities.html#errors">Errors</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="k8s_facilities.html#k8s-api-core">K8s API Core</a>
<ul>
<li><a href="k8s_facilities.html#node">Node</a>
<ul>
<li><a href="k8s_facilities.html#nodespec">NodeSpec</a>
<ul>
<li><a href="k8s_facilities.html#node-taints">Node Taints</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#nodestatus">NodeStatus</a>
<ul>
<li><a href="k8s_facilities.html#capacity">Capacity</a></li>
<li><a href="k8s_facilities.html#conditions">Conditions</a>
<ul>
<li><a href="k8s_facilities.html#nodeconditiontype">NodeConditionType</a></li>
<li><a href="k8s_facilities.html#conditionstatus">ConditionStatus</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#nodesysteminfo">NodeSystemInfo</a></li>
<li><a href="k8s_facilities.html#images">Images</a></li>
<li><a href="k8s_facilities.html#attached-volumes">Attached Volumes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="k8s_facilities.html#pod">Pod</a>
<ul>
<li><a href="k8s_facilities.html#pod-eviction">Pod Eviction</a></li>
<li><a href="k8s_facilities.html#pod-disruption-budget">Pod Disruption Budget</a>
<ul>
<li><a href="k8s_facilities.html#poddisruptionbudgetspec">PodDisruptionBudgetSpec</a></li>
<li><a href="k8s_facilities.html#poddisruptionbudgetstatus">PodDisruptionBudgetStatus</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#pod-volumes">Pod Volumes</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#persistentvolume">PersistentVolume</a>
<ul>
<li><a href="k8s_facilities.html#persistentvolumeclaim">PersistentVolumeClaim</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#secret">Secret</a></li>
</ul>
</li>
<li><a href="k8s_facilities.html#client-go">client-go</a>
<ul>
<li><a href="k8s_facilities.html#client-go-shared-informers">client-go Shared Informers.</a></li>
<li><a href="k8s_facilities.html#client-go-workqueues">client-go workqueues</a></li>
<li><a href="k8s_facilities.html#client-go-controller-steps">client-go controller steps</a></li>
<li><a href="k8s_facilities.html#client-go-utilities">client-go utilities</a>
<ul>
<li><a href="k8s_facilities.html#k8sioclient-goutilretryretryonconflict">k8s.io/client-go/util/retry.RetryOnConflict</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="k8s_facilities.html#references">References</a></li>
</ul>
</li>
</ul>
<h1 id="kubernetes-client-facilities"><a class="header" href="#kubernetes-client-facilities">Kubernetes Client Facilities</a></h1>
<p>This chapter describes the types and functions provided by k8s core and client modules that are leveraged by the MCM - it only covers what is required to understand MCM code and is simply meant to be a helpful review. References are provided for further reading.</p>
<h2 id="k8s-apimachinery"><a class="header" href="#k8s-apimachinery">K8s apimachinery</a></h2>
<h3 id="k8s-objects"><a class="header" href="#k8s-objects">K8s objects</a></h3>
<p>A K8s object represents a persistent entity. When using the K8s client-go framework to define such an object, one should follow the rules:</p>
<ol>
<li>A Go type representing a object must embed the <a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#ObjectMeta">k8s.io/apimachinery/pkg/apis/meta/v1.ObjectMeta</a> struct. <code>ObjectMeta</code> is metadata that all persisted resources must have, which includes all objects users must create.</li>
<li>A Go type representing a <em>singluar</em> object must embed <a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#TypeMeta">k8s.io/apimachinery/pkg/apis/meta/v1.TypeMeta</a> which describes an <em>individual</em> object in an API response or request with strings representing the <em>Kind</em> of the object and its API schema version called <em>APIVersion</em>. </li>
</ol>
<pre class="mermaid">graph TB
    subgraph CustomType

    ObjectMeta
    TypeMeta
    end
</pre>
<ol start="3">
<li>A Go type representing a <em>list</em> of a custom type must embed <a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#ListMeta">k8s.io/apimachinery/pkg/apis/meta/v1.ListMeta</a></li>
</ol>
<pre class="mermaid">graph TB
    subgraph CustomTypeList

    ObjectMeta
    ListMeta
    end
</pre>
<h4 id="typemeta"><a class="header" href="#typemeta">TypeMeta</a></h4>
<pre><code class="language-go">type TypeMeta struct {
	// Kind is a string value representing the REST resource this object represents.
	Kind string 

	// APIVersion defines the versioned schema of this representation of an object.
	APIVersion string
}
</code></pre>
<h4 id="objectmeta"><a class="header" href="#objectmeta">ObjectMeta</a></h4>
<p>A snippet of <code>ObjectMeta</code> struct fields shown below for convenience with the MCM relevant fields that are used by controller code.</p>
<pre><code class="language-go">type ObjectMeta struct { //snippet 
    // Name must be unique within a namespace. Is required when creating resources,
    Name string 

    // Namespace defines the space within which each name must be unique. An empty namespace is  equivalent to the &quot;default&quot; namespace,
    Namespace string

    // An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed.
    ResourceVersion string

	// A sequence number representing a specific generation of the desired state.
	Generation int64 

    // UID is the unique in time and space value for this object. It is typically generated by the API server on successful creation of a resource and is not allowed to change on PUT operations.
    UID types.UID 

    // CreationTimestamp is a timestamp representing the server time when this object was  created.
    CreationTimestamp Time 

    // DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user.  The resource is expected to be deleted (no longer reachable via APIs) after the time in this field, once the finalizers list is empty.
    DeletionTimestamp *Time


    // Must be empty before the object is deleted from the registry by the API server. Each entry is an identifier for the responsible controller that will remove the entry from the list.
    Finalizers []string 

    // Map of string keys and values that can be used to organize and categorize (scope and select) objects. Valid label keys have two segments: an optional prefix and name, separated by a slash (/).  Meant to be meaningful and relevant to USERS.
    Labels map[string]string

    // Annotations is an unstructured key value map stored with a resource that may be  set by controllers/tools to store and retrieve arbitrary metadata. Meant for TOOLS.
    Annotations map[string]string 

    // References to owner objects. Ex: Pod belongs to its owning ReplicaSet. A Machine belongs to its owning MachineSet.
    OwnerReferences []OwnerReference

    // The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters.
    ClusterName string

    //... other fields omitted.
}
</code></pre>
<h5 id="ownerreferences"><a class="header" href="#ownerreferences">OwnerReferences</a></h5>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#OwnerReference">k8s.io/apimachinery/pkg/apis/meta/v1.OwnerReference</a> is a struct that contains <code>TypeMeta</code> fields and a small sub-set of the <code>ObjectMetadata</code> - enough to let you identify an owning object. An owning object must be in the same namespace as the dependent, or be cluster-scoped, so there is no namespace field.</p>
<pre><code class="language-go">type OwnerReference struct {
   APIVersion string
   Kind string 
   Name string 
   UID types.UID 
   //... other fields omitted. TODO: check for usages.
}
</code></pre>
<h5 id="finalizers-and-deletion"><a class="header" href="#finalizers-and-deletion">Finalizers and Deletion</a></h5>
<p>Every k8s object has a <code>Finalizers []string</code> field that can be explicitly assigned by a controller. Every k8s object has a <code>DeletionTimestamp *Time</code> that is set by API Server when graceful deletion is requested.</p>
<p>These are part of the <a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#ObjectMeta">k8s.io./apimachinery/pkg/apis/meta/v1.ObjectMeta</a> struct type which is embedded in all k8s objects. </p>
<p>When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating <code>.metadata.deletionTimestamp</code> aka <code>Deletiontimestamp</code>, and returns a <code>202</code> status code (HTTP <code>Accepted</code>). The target object remains in a terminating state while the control plane takes the actions defined by the finalizers. After these actions are complete, the controller should removes the relevant finalizers from the target object. When the <code>metadata.finalizers</code> field is empty, Kubernetes considers the deletion complete and deletes the object.</p>
<h5 id="diff-between-labels-and-annotations"><a class="header" href="#diff-between-labels-and-annotations">Diff between Labels and Annotations</a></h5>
<p><em>Labels</em> are used in conjunction with selectors to identify groups of related resources and meant to be meaningful to users. Because selectors are used to query labels, this operation needs to be efficient. To ensure efficient queries, labels are constrained by RFC 1123. RFC 1123, among other constraints, restricts labels to a maximum 63 character length. Thus, labels should be used when you want Kubernetes to group a set of related resources. See https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ on label key and value restrictions</p>
<p><em>Annotations</em> are used for ‚Äúnon-identifying information‚Äù i.e., metadata that Kubernetes does not care about. As such, annotation keys and values have no constraints. Can include characters not </p>
<h5 id="rawextension"><a class="header" href="#rawextension">RawExtension</a></h5>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/runtime#RawExtension">k8s.io/apimachinery/pkg/runtime.RawExtension</a> is used to hold extension objects whose structures can be arbitrary. An example of MCM type that leverages this is the <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineClass">MachineClass</a> type whose <code>ProviderSpec</code> field is of type <code>runtime.RawExtension</code> and whose structure can vary according to the provider.</p>
<p>One can use a different custom structure type for each extension variant and then decode the field into an instance of that extension structure type using the standard Go json decoder.</p>
<p>Example:</p>
<pre><code class="language-go">var providerSpec *api.AWSProviderSpec
json.Unmarshal(machineClass.ProviderSpec.Raw, &amp;providerSpec)
</code></pre>
<p>One can</p>
<h4 id="api-errors"><a class="header" href="#api-errors">API Errors</a></h4>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/api/errors">k8s.io/apimachinery/pkg/api/errors</a> provides detailed error types  ans <code>IsXXX</code> methods for k8s api errors.</p>
<h5 id="errorsisnotfound"><a class="header" href="#errorsisnotfound">errors.IsNotFound</a></h5>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/api/errors#IsNotFound">k8s.io/apimachinery/pkg/api/errors.IsNotFound</a> returns true if the specified error was due to a k8s object not found. (error or wrapped error created by <a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/api/errors#NewNotFound">errors.NewNotFound</a>)</p>
<h5 id="errorsisnotfound-1"><a class="header" href="#errorsisnotfound-1">errors.IsNotFound</a></h5>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/api/errors#IsTooManyRequests">k8s.io/apimachinery/pkg/api/errors.IsTooManyRequests</a>  determines if err (or any wrapped error) is an error which indicates that there are too many requests that the server cannot handle.</p>
<h3 id="api-machinery-utilities"><a class="header" href="#api-machinery-utilities">API Machinery Utilities</a></h3>
<h4 id="waituntil"><a class="header" href="#waituntil">wait.Until</a></h4>
<p><a href="https://github.com/kubernetes/apimachinery/blob/v0.25.0/pkg/util/wait/wait.go#L91">k8s.io/apimachinery/pkg/wait.Until</a> loops until <code>stop</code> channel is closed, running <code>f</code> every given <code>period.</code> </p>
<pre><code class="language-go">func Until(f func(), period time.Duration, stopCh &lt;-chan struct{})
</code></pre>
<h4 id="waitpollimmediate"><a class="header" href="#waitpollimmediate">wait.PollImmediate</a></h4>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/util/wait#PollImmediate">k8s.io/apimachinery/pkg/util/wait.PollImmediate</a>  tries a condition func until it returns true, an error, or the timeout is reached.</p>
<pre><code class="language-go">func PollImmediate(interval, timeout time.Duration, condition ConditionFunc) error
</code></pre>
<h4 id="waitbackoff"><a class="header" href="#waitbackoff">wait.Backoff</a></h4>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/util/wait#Backoff">k8s.io/apimachinery/pkg/util/wait.Backoff</a> holds parameters applied to a Backoff function. There are many <code>retry</code> functions in <code>client-go</code> and MCM that take an instance of this struct as parameter.</p>
<pre><code class="language-go">type Backoff struct {
	Duration time.Duration
	Factor float64
	Jitter float64
	Steps int
	Cap time.Duration
}
</code></pre>
<ul>
<li><code>Duration</code> is the initial backoff duration.</li>
<li><code>Duration</code> is multiplied by <code>Factor</code> for the next iteration. </li>
<li><code>Jitter</code> is the random amount of each duration added (between <code>Duration</code> and <code>Duration*(1+jitter)</code></li>
<li><code>Steps</code> is the remaining number of iterations in which the duration may increase.</li>
<li><code>Cap</code> is the cap on the duration and may not exceed this value.</li>
</ul>
<h4 id="errors"><a class="header" href="#errors">Errors</a></h4>
<p><a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/util/errors#Aggregate">k8s.io/apimachinery/pkg/util/errors.Aggregate</a> represents an object that contains multiple errors</p>
<p>Use <a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/util/errors#NewAggregate">k8s.io/apimachinery/pkg/util/errors.NewAggregate</a> to construct the aggregate error from a slice of errors.</p>
<pre><code class="language-go">type Aggregate interface {
	error
	Errors() []error
	Is(error) bool
}
func NewAggregate(errlist []error) Aggregate {//...}
</code></pre>
<h2 id="k8s-api-core"><a class="header" href="#k8s-api-core">K8s API Core</a></h2>
<p>The MCM leverages several types from https://pkg.go.dev/k8s.io/api/core/v1 </p>
<h3 id="node"><a class="header" href="#node">Node</a></h3>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#Node">k8s.io/api/core/v1.Node</a> represents a worker node in Kubernetes. </p>
<pre><code class="language-go">type Node struct {
    metav1.TypeMeta
    metav1.ObjectMeta 
    Spec NodeSpec
    // Most recently observed status of the node.
    Status NodeStatus
}
</code></pre>
<h4 id="nodespec"><a class="header" href="#nodespec">NodeSpec</a></h4>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#NodeSpec">k8s.io/api/core/v1.NodeSpec</a>describes the attributes that a node is created with.  Both <a href="k8s_facilities.html#node">Node</a> and <a href="./mcm_facilities.html#machinespec">MachineSpec</a> use this. A snippet of MCM-relevant <code>NodeSpec</code> struct fields shown below for convenience.</p>
<pre><code class="language-go">type NodeSpec struct {
    // ID of the node assigned by the cloud provider in the format: &lt;ProviderName&gt;://&lt;ProviderSpecificNodeID&gt;
    ProviderID string 
    // podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node.
    PodCIDRs []string 

    // Unschedulable controls node schedulability of new pods. By default, node is schedulable.
    Unschedulable bool

    // Taints represents the Node's Taints. (taint is opposite of affinity. allow a Node to repel pods as opposed to attracting them)
    Taints []Taint
}
</code></pre>
<h5 id="node-taints"><a class="header" href="#node-taints">Node Taints</a></h5>
<p>See <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Tolerations</a></p>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#Taint">k8s.io/api/core/v1.Taint</a> is a Kubernetes <code>Node</code> property that enable specific nodes to repel pods. <em>Tolerations</em> are a Kubernetes <code>Pod</code> property that overcome this and allow a pod to be scheduled on a node with a <em>matching</em> taint.</p>
<p>Instead of applying the label to a node, we apply a taint that tells a scheduler to repel Pods from this node if it does not match the taint. Only those Pods that have a <em>toleration</em> for the taint can be let into the node with that taint.</p>
<p><code>kubectl taint nodes &lt;node name&gt; &lt;taint key&gt;=&lt;taint value&gt;:&lt;taint effect&gt; </code></p>
<p>Example:</p>
<p><code>kubectl taint nodes node1 gpu=nvidia:NoSchedule</code></p>
<p>Users can specify any arbitrary string for the taint key and value. The taint effect defines how a tainted node reacts to a pod without appropriate toleration. It must be one of the following effects;</p>
<ul>
<li>
<p><code>NoSchedule</code>: The pod will not get scheduled to the node without a matching toleration.</p>
</li>
<li>
<p><code>NoExecute</code>:This will immediately evict all the pods without the matching toleration from the node.</p>
</li>
<li>
<p><code>PerferNoSchedule</code>:This is a softer version of NoSchedule where the controller will not try to schedule a pod with the tainted node. However, it is not a strict requirement.</p>
</li>
</ul>
<pre><code class="language-go">type Taint struct {
	// Key of taint to be applied to a node.
	Key string
	// Value of taint corresponding to the taint key.
	Value string 
	// Effect represents the effect of the taint on pods
	// that do not tolerate the taint.
	// Valid effects are NoSchedule, PreferNoSchedule and NoExecute.
	Effect TaintEffect 
	// TimeAdded represents the time at which the taint was added.
	// It is only written for NoExecute taints.
	// +optional
	TimeAdded *metav1.Time 
}
</code></pre>
<p>Example of a PodSpec with toleration below:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    security: s1
spec:
  containers:
  - name: bear
    image: supergiantkir/animals:bear
  tolerations:
  - key: &quot;gpu&quot;
    operator: &quot;Equal&quot;
    value: &quot;nvidia&quot;
    effect: &quot;NoSchedule&quot;
</code></pre>
<p>Example use case for a taint/tolerance: If you have nodes with special hardware (e.g GPUs) you want to repel Pods that do not need this hardware and attract Pods that do need it. This can be done by tainting the nodes that have the specialized hardware (e.g. kubectl taint nodes nodename gpu=nvidia:NoSchedule ) and adding corresponding toleration to Pods that must use this special hardware.</p>
<h4 id="nodestatus"><a class="header" href="#nodestatus">NodeStatus</a></h4>
<p>See <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-status">Node status</a></p>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#NodeStatus">k8s.io/api/core/v1.NodeStatus</a> represents the current status of a node and is an encapsulation illustrated below:</p>
<pre><code class="language-go">type NodeStatus struct {
	Capacity ResourceList 
	// Allocatable represents the resources of a node that are available for scheduling. Defaults to Capacity.
	Allocatable ResourceList
	// Conditions is an array of current observed node conditions.
	Conditions []NodeCondition 
	// List of addresses reachable to the node.Queried from cloud provider, if available.
	Addresses []NodeAddress 
	// Set of ids/uuids to uniquely identify the node.
	NodeInfo NodeSystemInfo 
	// List of container images on this node
	Images []ContainerImage 
	// List of attachable volumes that are in use (mounted) by the node.
  //UniqueVolumeName is just typedef for string
	VolumesInUse []UniqueVolumeName 
	// List of volumes that are attached to the node.
	VolumesAttached []AttachedVolume 
}
</code></pre>
<h5 id="capacity"><a class="header" href="#capacity">Capacity</a></h5>
<p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#capacity">Capacity</a>. The fields in the capacity block indicate the total amount of resources that a Node has. </p>
<ul>
<li>Allocatable indicates the amount of resources on a Node that is available to be consumed by normal Pods. Defaults to Capacity.</li>
</ul>
<p>A Node <code>Capacity</code> is of type <a href="https://pkg.go.dev/k8s.io/api/core/v1#ResourceList">k8s.io/api/core/v1.ResourceList</a> which is effectively a set of set of (resource name, quantity) pairs. </p>
<pre><code class="language-go">type ResourceList map[ResourceName]resource.Quantity
</code></pre>
<p><code>ResourceName</code>s can be cpu/memory/storage</p>
<pre><code class="language-go">const (
	// CPU, in cores. (500m = .5 cores)
	ResourceCPU ResourceName = &quot;cpu&quot;
	// Memory, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
	ResourceMemory ResourceName = &quot;memory&quot;
	// Volume size, in bytes (e,g. 5Gi = 5GiB = 5 * 1024 * 1024 * 1024)
	ResourceStorage ResourceName = &quot;storage&quot;
	// Local ephemeral storage, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
	// The resource name for ResourceEphemeralStorage is alpha and it can change across releases.
	ResourceEphemeralStorage ResourceName = &quot;ephemeral-storage&quot;
)
</code></pre>
<p>A <a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/api/resource#Quantity">k8s.io/apimachinery/pkg/api/resource.Quantity</a> is a serializable/de-serializable number with a SI unit</p>
<h5 id="conditions"><a class="header" href="#conditions">Conditions</a></h5>
<p><a href="https://kubernetes.io/docs/concepts/nodes/node/#condition">Conditions</a> are valid conditions of nodes.</p>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#NodeCondition">https://pkg.go.dev/k8s.io/api/core/v1.NodeCondition</a> contains condition information for a node.</p>
<pre><code class="language-go">type NodeCondition struct {
	// Type of node condition.
	Type NodeConditionType 
	// Status of the condition, one of True, False, Unknown.
	Status ConditionStatus 
	// Last time we got an update on a given condition.
	LastHeartbeatTime metav1.Time 
	// Last time the condition transitioned from one status to another.
	LastTransitionTime metav1.Time 
	// (brief) reason for the condition's last transition.
	Reason string 
	// Human readable message indicating details about last transition.
	Message string 
}
</code></pre>
<h6 id="nodeconditiontype"><a class="header" href="#nodeconditiontype">NodeConditionType</a></h6>
<p><code>NodeConditionType</code> is one of the following:</p>
<pre><code class="language-go">const (
	// NodeReady means kubelet is healthy and ready to accept pods.
	NodeReady NodeConditionType = &quot;Ready&quot;
	// NodeMemoryPressure means the kubelet is under pressure due to insufficient available memory.
	NodeMemoryPressure NodeConditionType = &quot;MemoryPressure&quot;
	// NodeDiskPressure means the kubelet is under pressure due to insufficient available disk.
	NodeDiskPressure NodeConditionType = &quot;DiskPressure&quot;
	// NodePIDPressure means the kubelet is under pressure due to insufficient available PID.
	NodePIDPressure NodeConditionType = &quot;PIDPressure&quot;
	// NodeNetworkUnavailable means that network for the node is not correctly configured.
	NodeNetworkUnavailable NodeConditionType = &quot;NetworkUnavailable&quot;
)
</code></pre>
<p>Note: The MCM extends the above with further custom node condition types of its own. Not sure if this is correct - could break later if k8s enforces some validation ?</p>
<h6 id="conditionstatus"><a class="header" href="#conditionstatus">ConditionStatus</a></h6>
<p>These are valid condition statuses. <code>ConditionTrue</code> means a resource is in the condition. <code>ConditionFalse</code> means a resource is not in the condition. <code>ConditionUnknown</code> means kubernetes can't decide if a resource is in the condition or not. </p>
<pre><code class="language-go">type ConditionStatus string

const (
	ConditionTrue    ConditionStatus = &quot;True&quot;
	ConditionFalse   ConditionStatus = &quot;False&quot;
	ConditionUnknown ConditionStatus = &quot;Unknown&quot;
)
</code></pre>
<h5 id="addresses"><a class="header" href="#addresses">Addresses</a></h5>
<p>See <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#addresses">Node Addresses</a></p>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#NodeAddress">k8s.io/api/core/v1.NodeAddress</a> contains information for the node's address.</p>
<pre><code class="language-go">type NodeAddress struct {
	// Node address type, one of Hostname, ExternalIP or InternalIP.
	Type NodeAddressType 
	// The node address string.
	Address string 
}
</code></pre>
<h5 id="nodesysteminfo"><a class="header" href="#nodesysteminfo">NodeSystemInfo</a></h5>
<p>Describes general information about the node, such as machine id, kernel version, Kubernetes version (kubelet and kube-proxy version), container runtime details, and which operating system the node uses. The kubelet gathers this information from the node and publishes it into the Kubernetes API.</p>
<pre><code class="language-go">type NodeSystemInfo struct {
	// MachineID reported by the node. For unique machine identification
	// in the cluster this field is preferred. 
	MachineID string 
	// Kernel Version reported by the node from 'uname -r' (e.g. 3.16.0-0.bpo.4-amd64).
	KernelVersion string 
	// OS Image reported by the node from /etc/os-release (e.g. Debian GNU/Linux 7 (wheezy)).
	OSImage string 
	// ContainerRuntime Version reported by the node through runtime remote API (e.g. docker://1.5.0).
	ContainerRuntimeVersion string 
	// Kubelet Version reported by the node.
	KubeletVersion string 
	// KubeProxy Version reported by the node.
	KubeProxyVersion string 
	// The Operating System reported by the node
	OperatingSystem string 
	// The Architecture reported by the node
	Architecture string 
}
</code></pre>
<ul>
<li>The <a href="http://man7.org/linux/man-pages/man5/machine-id.5.html">MachineID</a> is a single newline-terminated, hexadecimal, 32-character, lowercase ID. from <code>/etc/machine-id</code></li>
</ul>
<h5 id="images"><a class="header" href="#images">Images</a></h5>
<p>A slice of <a href="https://pkg.go.dev/k8s.io/api/core/v1#ContainerImage">k8s.io/api/core/v1.ContainerImage</a> which describes a contianer image.</p>
<pre><code class="language-go">type ContainerImage struct {
	Names []string 
	SizeBytes int64 
}
</code></pre>
<ul>
<li>Names is the names by which this image is known.
e.g. <code>[&quot;kubernetes.example/hyperkube:v1.0.7&quot;, &quot;cloud-vendor.registry.example/cloud-vendor/hyperkube:v1.0.7&quot;]</code></li>
<li><code>SizeBytes</code> is the size of the image in bytes.</li>
</ul>
<h5 id="attached-volumes"><a class="header" href="#attached-volumes">Attached Volumes</a></h5>
<p><a href="https://pkg.go.dev/k8s.io/api/core/v1#AttachedVolume">k8s.io/api/core/v1.AttachedVolume</a> describes a volume attached to a node.</p>
<pre><code class="language-go">type UniqueVolumeName string
type AttachedVolume struct {
	Name UniqueVolumeName 
	DevicePath string 
}
</code></pre>
<ul>
<li><code>Name</code> is the Name of the attached volume. <code>UniqueVolumeName</code> is just a typedef for a Go string.</li>
<li><code>DevicePath</code> represents the device path where the volume should be available</li>
</ul>
<h3 id="pod"><a class="header" href="#pod">Pod</a></h3>
<p><a href="k8s.io/api/core/v1#Pod">k8s.io/api/core/v1.Pod</a> struct represents a k8s <a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pod</a> which a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts.</p>
<pre><code class="language-go">type Pod struct {
	metav1.TypeMeta
	metav1.ObjectMeta

	// Specification of the desired behavior of the pod.
	Spec PodSpec 

	// Most recently observed status of the pod.
	Status PodStatus
}
</code></pre>
<p>See <a href="https://pkg.go.dev/k8s.io/api/core/v1#PodSpec">k8s.io/api/core/v1.PodSpec</a>. Each <code>PodSpec</code> has a priority value where the higher the value, the higher the priority of the Pod.</p>
<p><code>PodSpec.Volumes</code> slice is List of volumes that can be mounted by containers belonging to the pod and is relevant to MC code that attaches/detaches volumes .</p>
<pre><code class="language-go">type PodSpec struct {
	 Volumes []Volume 
	 //TODO: describe other PodSpec fields used by MC
}
</code></pre>
<h4 id="pod-eviction"><a class="header" href="#pod-eviction">Pod Eviction</a></h4>
<p>A <a href="k8s.io/api@v0.25.2/policy/v1#Eviction">k8s.io/api/policy/v1.Eviction</a> can be used to evict a <a href="k8s_facilities.html#pod">Pod</a> from its <a href="k8s_facilities.html#node">Node</a> - eviction is the <em>graceful</em> terimation of Pods on nodes.See <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/">API Eviction</a></p>
<pre><code class="language-go">type Eviction struct {
	metav1.TypeMeta 

	// ObjectMeta describes the pod that is being evicted.
	metav1.ObjectMeta 

	// DeleteOptions may be provided
	DeleteOptions *metav1.DeleteOptions 
}

</code></pre>
<p>Construct the <code>ObjectMeta</code> using the Pod and namespace and then use instance of typed <a href="https://pkg.go.dev/k8s.io/client-go/kubernetes#Interface">Kubernetes Client Interface</a>, and get the <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/policy/v1#PolicyV1Interface">PolicyV1Interface</a>, get the <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/policy/v1#EvictionInterface">EvictionInterface</a> and invoke the invoke the <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/policy/v1#EvictionExpansion">Evict</a> method</p>
<p>Example</p>
<pre><code class="language-go">client.PolicyV1().Evictions(eviction.Namespace).Evict(ctx, eviction)
</code></pre>
<h4 id="pod-disruption-budget"><a class="header" href="#pod-disruption-budget">Pod Disruption Budget</a></h4>
<p>A <a href="https://pkg.go.dev/k8s.io/api@v0.25.2/policy/v1#PodDisruptionBudget">k8s.io/api/policy/v1.PodDisruptionBudget</a>is a struct type that represents a <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget">Pod Disruption Budget</a> which is the max disruption that can be caused to a collection of pods.</p>
<pre><code class="language-go">type PodDisruptionBudget struct {
	metav1.TypeMeta 
	metav1.ObjectMeta

	// Specification of the desired behavior of the PodDisruptionBudget.
	Spec PodDisruptionBudgetSpec 
	// Most recently observed status of the PodDisruptionBudget.
	Status PodDisruptionBudgetStatus 
}
</code></pre>
<h5 id="poddisruptionbudgetspec"><a class="header" href="#poddisruptionbudgetspec">PodDisruptionBudgetSpec</a></h5>
<pre><code class="language-go">type PodDisruptionBudgetSpec struct {
	MinAvailable *intstr.IntOrString 
	Selector *metav1.LabelSelector 
	MaxUnavailable *intstr.IntOrString 
}
</code></pre>
<ul>
<li><code>Selector</code> specifies  Label query over pods whose evictions are managed by the disruption budget. A null selector will match no pods, while an empty ({}) selector will select all pods within the namespace.</li>
<li>An eviction is allowed if at least <code>MinAvailable</code> pods selected by <code>Selector</code> will still be available after the eviction, i.e. even in the absence of the evicted pod.  So for example you can prevent all voluntary evictions by specifying &quot;100%&quot;.</li>
<li>An eviction is allowed if at most <code>MaxUnavailable</code> pods selected by <code>Selector</code> are unavailable after the eviction, i.e. even in absence of  the evicted pod. For example, one can prevent all voluntary evictions  by specifying 0. </li>
<li><code>MinAvailable</code> is a mutually exclusive setting with <code>MaxUnavailable</code></li>
</ul>
<h5 id="poddisruptionbudgetstatus"><a class="header" href="#poddisruptionbudgetstatus">PodDisruptionBudgetStatus</a></h5>
<p>See godoc for <a href="https://pkg.go.dev/k8s.io/api@v0.25.2/policy/v1#PodDisruptionBudgetStatus">k8s.io/api/policy/v1.PodDisruptionBudgetStatus</a></p>
<h4 id="pod-volumes"><a class="header" href="#pod-volumes">Pod Volumes</a></h4>
<p>A <a href="https://pkg.go.dev/k8s.io/api/core/v1#Volume">k8s.io/api/core/v1.Volume</a> represents a named volume in a pod - which is a directory that may be accessed by any container in the pod. See <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Pod Volumes</a></p>
<p>A <code>Volume</code> has a <code>Name</code> and embeds a <code>VolumeSource</code> as shown below. A <code>VolumeSource</code> represents the location and type of the mounted volume.</p>
<pre><code class="language-go">type Volume struct {
	Name string 
	VolumeSource
}
</code></pre>
<p><code>VolumeSource</code> which represents the source of a volume to mount should only have ONE of its fields popularted. The MC uses <code>PersistentVolumeClaim</code> field which is pointer to a <code>PersistentVolumeClaimVolumeSource</code> which represents a reference to a <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolumeClaim">PersistentVolumeClaim</a> in the same namespace.</p>
<pre><code class="language-go">type VolumeSource struct {
	PersistentVolumeClaim *PersistentVolumeClaimVolumeSource
}
type PersistentVolumeClaimVolumeSource struct  {
	ClaimName string
	ReadOnly bool
}
</code></pre>
<h3 id="persistentvolume"><a class="header" href="#persistentvolume">PersistentVolume</a></h3>
<p>A <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolume">k8s.io/api/core/v1.PersistentVolume</a> (PV) represents a piece of storage in the cluster.
See <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">K8s Persistent Volumes</a></p>
<h4 id="persistentvolumeclaim"><a class="header" href="#persistentvolumeclaim">PersistentVolumeClaim</a></h4>
<p>A <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolumeClaim">k8s.io/api/core/v1.PersistentVolumeClaim</a> represents a user's request for and claim to a persistent volume. </p>
<pre><code class="language-go">type PersistentVolumeClaim struct {
	metav1.TypeMeta 
	metav1.ObjectMeta
	Spec PersistentVolumeClaimSpec
	Status PersistentVolumeClaimStatus
}
type PersistentVolumeClaimSpec struct {
	StorageClassName *string 
	//...
	VolumeName string
	//...
}
</code></pre>
<p>Note that <code>PersistentVolumeClaimSpec.VolumeName</code> is of interest to the MC which represents the binding reference to the <code>PersistentVolume</code> backing this claim. Please note that this is different from <code>Pod.Spec.Volumes[*].Name</code> which is more like a label for the volume directory.</p>
<h3 id="secret"><a class="header" href="#secret">Secret</a></h3>
<p>A <a href="https://pkg.go.dev/k8s.io/api/core/v1#Secret">k8s.io/api/core/v1.Secret</a> holds secret data of a secret type whose size &lt; 1MB.  See <a href="https://kubernetes.io/docs/concepts/configuration/secret/">K8s Secrets</a></p>
<ul>
<li>Secret Data is in <code>Secret.Data</code> which is a <code>map[string][]byte</code> where the bytes is the secret value and key is simple ASCII alphanumeric.</li>
</ul>
<pre><code class="language-go">type Secret struct {
	metav1.TypeMeta 
	metav1.ObjectMeta 
	Data map[string][]byte 
	Type SecretType 
	//... omitted for brevity
}
</code></pre>
<p><code>SecretType</code> can be of many types: <code>SecretTypeOpaque</code> which represents user-defined secreets, <code>SecretTypeServiceAccountToken</code> whichcontains a token that identifies a service account to the API, etc.</p>
<h2 id="client-go"><a class="header" href="#client-go">client-go</a></h2>
<p>k8s clients have the type  <a href="https://pkg.go.dev/k8s.io/client-go/kubernetes#Clientset">k8s.io/client-go/kubernetes.ClientSet</a> which is actually a high-level client set facade encapsulating clients for the  <code>core</code>, <code>appsv1</code>, <code>discoveryv1</code>, <code>eventsv1</code>, <code>networkingv1</code>, <code>nodev1</code>, <code>policyv1</code>, <code>storagev1</code> api groups. These individual clients are available via accessor methods. ie use <a href="https://pkg.go.dev/k8s.io/client-go/kubernetes#Clientset.AppsV1">clientset.AppsV1()</a> to get the the <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/apps/v1#AppsV1Interface">AppsV1</a> client.</p>
<pre><code class="language-go">// Clientset contains the clients for groups. Each group has exactly one
// version included in a Clientset.
type Clientset struct {
    appsV1                       *appsv1.AppsV1Client
    coreV1                       *corev1.CoreV1Client
    discoveryV1                  *discoveryv1.DiscoveryV1Client
    eventsV1                     *eventsv1.EventsV1Client
    // ...

  // AppsV1 retrieves the AppsV1Client
  func (c *Clientset) AppsV1() appsv1.AppsV1Interface {
    return c.appsV1
  }
    // ...
}
</code></pre>
<p>As can be noticed from the above snippet, each of these clients associated with api groups expose an interface named <em>GroupVersionInterface</em> that in-turn provides further access to a <em>generic</em> REST Interface as well as access to a <em>typed</em> interface containing getter/setter methods for objects within that API group.</p>
<p>For example <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.0/kubernetes/typed/events/v1#EventsV1Client">EventsV1Client</a> which is used to interact with features provided by the <code>events.k8s.io</code> group implements <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.0/kubernetes/typed/events/v1#EventsV1Interface">EventsV1Interface</a></p>
<pre><code class="language-go">type EventsV1Interface interface {
	RESTClient() rest.Interface // generic REST API access
	EventsGetter // typed interface access
}
// EventsGetter has a method to return a EventInterface.
type EventsGetter interface {
	Events(namespace string) EventInterface
}
</code></pre>
<p>The <code>ClientSet</code> struct implements the <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes#Interface">kubernetes.Interface</a> facade which is a high-level facade containing all the methods to access the individual <em>GroupVersionInterface</em> facade clients.</p>
<pre><code class="language-go">type Interface interface {
	Discovery() discovery.DiscoveryInterface
	AppsV1() appsv1.AppsV1Interface
	CoreV1() corev1.CoreV1Interface
	EventsV1() eventsv1.EventsV1Interface
	NodeV1() nodev1.NodeV1Interface
 // ... other facade accessor
}
</code></pre>
<p>One can <em>generate</em> k8s clients for custom k8s objects that follow the same pattern for core k8s objects. The details are not covered here. Please refer to <a href="https://itnext.io/how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd-b4b9907769ba">How to generate client codes for Kubernetes Custom Resource Definitions</a>
, <a href="https://github.com/kubernetes/gengo">gengo</a>, <a href="https://github.com/kubernetes/code-generator">k8s.io codegenrator</a> and the slightly-old article:  <a href="https://cloud.redhat.com/blog/kubernetes-deep-dive-code-generation-customresources">Kubernetes Deep Dive: Code Generation for CustomResources</a></p>
<h3 id="client-go-shared-informers"><a class="header" href="#client-go-shared-informers">client-go Shared Informers.</a></h3>
<p>The vital role of a Kubernetes controller is to watch objects for the desired state and the actual state, then send instructions to make the actual state be more like the desired state. The controller thus first needs to retrieve the object's information. Instead of making direct API calls using k8s listers/watchers, client-go controllers should use <code>SharedInformer</code>s.</p>
<p><a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#SharedInformer">cache.SharedInformer</a> is a primitive exposed by <code>client-go</code> lib that maintains a local cache of k8s objects of a particular API group and kind/resource. (restricable by namespace/label/field selectors) which is linked to the authoritative state of the corresponding objects in the API server. </p>
<p>Informers are used to reduced the load-pressure on the API Server and etcd.</p>
<p>All that is needed to be known at this point is that Informers internally watch for k8s object changes, update an internal indexed store and invoke registered event handlers. Client code must construct event handlers to inject the logic that one would like to execute when an object is Added/Updated/Deleted. </p>
<pre><code class="language-go">type SharedInformer interface {
	// AddEventHandler adds an event handler to the shared informer using the shared informer's resync period.  Events to a single handler are delivered sequentially, but there is no coordination between different handlers.
	AddEventHandler(handler cache.ResourceEventHandler)

	// HasSynced returns true if the shared informer's store has been
	// informed by at least one full LIST of the authoritative state
	// of the informer's object collection.  This is unrelated to &quot;resync&quot;.
	HasSynced() bool

	// Run starts and runs the shared informer, returning after it stops.
	// The informer will be stopped when stopCh is closed.
	Run(stopCh &lt;-chan struct{})
	//..
}
</code></pre>
<p>Note: resync period tells the informer to rebuild its cache every every time the period expires.</p>
<p><a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#ResourceEventHandler">cache.ResourceEventHandler</a> handle notifications for events that happen to a resource.</p>
<pre><code class="language-go">type ResourceEventHandler interface {
	OnAdd(obj interface{})
	OnUpdate(oldObj, newObj interface{})
	OnDelete(obj interface{})
}
</code></pre>
<p><a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#ResourceEventHandlerFuncs">cache.ResourceEventHandlerFuncs</a> is an adapter to let you easily specify as many or as few of the notification functions as you want while still implementing <code>ResourceEventHandler</code>. Nearly all controllers code use instance of this adapter struct to create event handlers to register on shared informers.</p>
<pre><code class="language-go">type ResourceEventHandlerFuncs struct {
	AddFunc    func(obj interface{})
	UpdateFunc func(oldObj, newObj interface{})
	DeleteFunc func(obj interface{})
}
</code></pre>
<p>Shared informers for standard k8s objects can be obtained using the <a href="https://pkg.go.dev/k8s.io/client-go/informers#NewSharedInformerFactory">k8s.io/client-go/informers.NewSharedInformerFactory</a> or one of the variant factory methods in the same package.</p>
<p>Informers and their factory functions for custom k8s objects are usually found in the generated factory code usually in a <code>factory.go</code> file. </p>
<p>See <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/client/informers/externalversions/factory.go#L79">github.com/machine-controller-manager/pkg/client/informers/externalversions/externalversions.NewSharedInformerFactory</a></p>
<h3 id="client-go-workqueues"><a class="header" href="#client-go-workqueues">client-go workqueues</a></h3>
<p>The basic <a href="https://pkg.go.dev/k8s.io/client-go/util/workqueue#Interface">workqueue.Interface</a> has the following methods:</p>
<pre><code class="language-go">type Interface interface {
	Add(item interface{})
	Len() int
	Get() (item interface{}, shutdown bool)
	Done(item interface{})
	ShutDown()
	ShutDownWithDrain()
	ShuttingDown() bool
}
</code></pre>
<p>This is extended with ability to Add Item as a later time using the <a href="https://pkg.go.dev/k8s.io/client-go/util/workqueue#DelayingInterface">workqueue.DelayingInterface</a>. </p>
<pre><code class="language-go">type DelayingInterface interface {
	Interface
	// AddAfter adds an item to the workqueue after the indicated duration has passed
	// Used to requeue items after failueres to avoid ending in hot-loop
	AddAfter(item interface{}, duration time.Duration)
}
</code></pre>
<p>This is further extended with rate limiting using <a href="https://pkg.go.dev/k8s.io/client-go/util/workqueue#RateLimiter">workqueue.RateLimiter</a></p>
<pre><code class="language-go">type RateLimiter interface {
 	// When gets an item and gets to decide how long that item should wait
	When(item interface{}) time.Duration
	// Forget indicates that an item is finished being retried.  Doesn't matter whether its for perm failing
	// or for success, we'll stop tracking it
	Forget(item interface{})
	// NumRequeues returns back how many failures the item has had
	NumRequeues(item interface{}) int
}

</code></pre>
<h3 id="client-go-controller-steps"><a class="header" href="#client-go-controller-steps">client-go controller steps</a></h3>
<p>The basic high-level contract for a k8s-client controller leveraging work-queues goes like the below:</p>
<ol>
<li>Create rate-limited work queue(s) created using <a href="https://pkg.go.dev/k8s.io/client-go/util/workqueue#NewNamedRateLimitingQueue">workqueue.NewNamedRateLimitingQueue</a></li>
<li>Define lifecycle callback functions (Add/Update/Delete) which accept k8s objects and enqueue k8s object keys (namespace/name) on these rate-limited work queue(s).</li>
<li>Create informers using the shared informer factory functions.</li>
<li>Add event handlers to the informers specifying these callback functions.
<ol>
<li>When informers are started, they will invoke the appropriate registered callbacks when k8s objects are added/updated/deleted.</li>
</ol>
</li>
<li>The controller <code>Run</code> loop then picks up objects from the work queue using <code>Get</code> and reconciles them by invoking the appropriate reconcile function, ensuring that <code>Done</code> is called after reconcile to mark it as done processing.</li>
</ol>
<p>Example:</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

CreateWorkQueue[&quot;machineQueue:=workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), 'machine')&quot;]
--&gt;
CreateInformer[&quot;machineInformerFactory := externalversions.NewSharedInformerFactory(...)
machineInformer := machineInformerFactory.Machine().V1alpha1().Machines()&quot;]
--&gt;
AddEventHandler[&quot;machineInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    controller.machineToMachineClassAdd,
		UpdateFunc: controller.machineToMachineClassUpdate,
		DeleteFunc: controller.machineToMachineClassDelete,
	})&quot;]
--&gt;Run[&quot;while (!shutdown) {
  key, shutdown := queue.Get()
  defer queue.Done(key)
  reconcile(key.(string))
}
&quot;]
--&gt;Z((&quot;End&quot;))
</pre>
<p>A more elaborate example of basic client-go controller flow is demonstrated in the <a href="https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go">clien-go workqueue example</a></p>
<h3 id="client-go-utilities"><a class="header" href="#client-go-utilities">client-go utilities</a></h3>
<h4 id="k8sioclient-goutilretryretryonconflict"><a class="header" href="#k8sioclient-goutilretryretryonconflict">k8s.io/client-go/util/retry.RetryOnConflict</a></h4>
<pre><code class="language-go">func RetryOnConflict(backoff wait.Backoff, fn func() error) error
</code></pre>
<ul>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/util/retry#RetryOnConflict">retry.RetryOnConflict</a> is used to make an update to a resource when you have to worry about conflicts caused by other code making unrelated updates to the resource at the same time. </li>
<li><code>fn</code> should fetch the resource to be modified, make appropriate changes to it, try to update it, and return (unmodified) the error from the update function.</li>
<li>On a successful update, <code>RetryOnConflict</code> will return <code>nil</code>. If the update <code>fn</code> returns a <em>Conflict</em> error, <code>RetryOnConflict</code> will wait some amount of time as described by <code>backoff</code>, and then try again. </li>
<li>On a <em>non-Conflict</em> error, or if it retries too many times (<code>backoff.Steps</code> has reached zero) and gives up, <code>RetryOnConflict</code> will return an error to the caller.</li>
</ul>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">K8s API Conventions</a></li>
<li><a href="https://iximiuz.com/en/posts/kubernetes-api-go-types-and-common-machinery/">How To Call Kubernetes API using Go - Types and Common Machinery</a></li>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Node Taints and Tolerances</a></li>
<li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-status">Node status</a></li>
<li><a href="https://medium.com/kubernetes-tutorials/making-sense-of-taints-and-tolerations-in-kubernetes-446e75010f4e">Making Sense of Taints and Tolerations</a></li>
<li><a href="https://www.ionos.com/digitalguide/server/know-how/cidr-classless-inter-domain-routing/">CIDR</a></li>
<li><a href="https://mxtoolbox.com/subnetcalculator.aspx">CIDR Calculator</a></li>
<li><a href="https://github.com/kubernetes/code-generator">k8s.io codegenrator</a></li>
<li><a href="https://itnext.io/how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd-b4b9907769ba">How to generate client codes for Kubernetes Custom Resource Definitions</a></li>
<li><a href="https://cloud.redhat.com/blog/kubernetes-deep-dive-code-generation-customresources">Kubernetes Deep Dive: Code Generation for CustomResources</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="mcm_facilities.html#mcm-facilities">MCM Facilities</a>
<ul>
<li><a href="mcm_facilities.html#machine-controller-core-types">Machine Controller Core Types</a>
<ul>
<li><a href="mcm_facilities.html#machine">Machine</a>
<ul>
<li><a href="mcm_facilities.html#machinespec">MachineSpec</a></li>
<li><a href="mcm_facilities.html#machinestatus">MachineStatus</a></li>
<li><a href="mcm_facilities.html#extended-nodeconditiontypes">Extended NodeConditionTypes</a></li>
<li><a href="mcm_facilities.html#lastoperation">LastOperation</a>
<ul>
<li><a href="mcm_facilities.html#machinestate-should-be-called-machineoperationstate">MachineState (should be called MachineOperationState)</a></li>
<li><a href="mcm_facilities.html#machineoperationtype">MachineOperationType</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#currentstatus">CurrentStatus</a>
<ul>
<li><a href="mcm_facilities.html#machinephase">MachinePhase</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#finalizers">Finalizers</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#machineclass">MachineClass</a>
<ul>
<li><a href="mcm_facilities.html#secretref-and-credentialssecretref">SecretRef and CredentialsSecretRef</a></li>
<li><a href="mcm_facilities.html#nodetemplate">NodeTemplate</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#machineset">MachineSet</a>
<ul>
<li><a href="mcm_facilities.html#machinesetspec">MachineSetSpec</a></li>
<li><a href="mcm_facilities.html#example-machineset-yaml">Example MachineSet YAML</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#machinedeployment">MachineDeployment</a>
<ul>
<li><a href="mcm_facilities.html#machinedeploymentspec">MachineDeploymentSpec</a></li>
<li><a href="mcm_facilities.html#machinedeploymentstrategy">MachineDeploymentStrategy</a></li>
<li><a href="mcm_facilities.html#example-machinedeployment-yaml">Example MachineDeployment YAML</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#volumeattachment">VolumeAttachment</a></li>
<li><a href="mcm_facilities.html#volumeattachmentspec">VolumeAttachmentSpec</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#utilities">Utilities</a>
<ul>
<li><a href="mcm_facilities.html#nodeops">NodeOps</a>
<ul>
<li><a href="mcm_facilities.html#nodeopsgetnodecondition">nodeops.GetNodeCondition</a></li>
<li><a href="mcm_facilities.html#nodeopscloneandaddcondition">nodeops.CloneAndAddCondition</a></li>
<li><a href="mcm_facilities.html#nodeopsaddorupdateconditionsonnode">nodeops.AddOrUpdateConditionsOnNode</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#machineutils">MachineUtils</a>
<ul>
<li><a href="mcm_facilities.html#operation-descriptions">Operation Descriptions</a></li>
<li><a href="mcm_facilities.html#retry-periods">Retry Periods</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#misc">Misc</a>
<ul>
<li><a href="mcm_facilities.html#permitspermitgiver">permits.PermitGiver</a>
<ul>
<li><a href="mcm_facilities.html#permitsnewpermitgiver">permits.NewPermitGiver</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="mcm_facilities.html#main-server-structs">Main Server Structs</a>
<ul>
<li><a href="mcm_facilities.html#mcserver">MCServer</a>
<ul>
<li><a href="mcm_facilities.html#mcserver-usage">MCServer Usage</a></li>
<li><a href="mcm_facilities.html#machinecontrollerconfiguration-struct">MachineControllerConfiguration struct</a>
<ul>
<li><a href="mcm_facilities.html#safetyoptions">SafetyOptions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="mcm_facilities.html#controller-structs">Controller Structs</a>
<ul>
<li><a href="mcm_facilities.html#machine-controller-core-struct">Machine Controller Core Struct</a></li>
</ul>
</li>
<li><a href="mcm_facilities.html#driver">Driver</a></li>
<li><a href="mcm_facilities.html#codes-and-error-status">Codes and (error) Status</a>
<ul>
<li><a href="mcm_facilities.html#code">Code</a></li>
<li><a href="mcm_facilities.html#status">Status</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="mcm-facilities"><a class="header" href="#mcm-facilities">MCM Facilities</a></h1>
<p>This chapter describes the core types and utilities present in the MCM module and used by the controllers and drivers.</p>
<h2 id="machine-controller-core-types"><a class="header" href="#machine-controller-core-types">Machine Controller Core Types</a></h2>
<p>The relevant machine types managed by the MCM controlled reside in <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1">github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1</a>. This follows the standard location for client gen types <code>&lt;module&gt;/pkg/apis/&lt;group&gt;/&lt;version&gt;</code>.</p>
<p>Example: <code>Machine</code> type is <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#Machine">pkg/apis/machine/v1alpha1.Machine</a></p>
<h3 id="machine"><a class="header" href="#machine">Machine</a></h3>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#Machine">Machine</a> is the representation of a physical or virtual machine that corresponds to a front-end k8s node object. An example YAML looks like the below</p>
<pre><code class="language-yaml">apiVersion: machine.sapcloud.io/v1alpha1
kind: Machine
metadata:
  name: test-machine
  namespace: default
spec:
  class:
    kind: MachineClass
    name: test-class
</code></pre>
<p>A <code>Machine</code> has a <code>Spec</code> field represented by <a href="mcm_facilities.html#machinespec">MachineSpec</a></p>
<pre><code class="language-go">type Machine struct {
	// ObjectMeta for machine object
	metav1.ObjectMeta 

	// TypeMeta for machine object
	metav1.TypeMeta 

	// Spec contains the specification of the machine
	Spec MachineSpec 

	// Status contains fields depicting the status
	Status MachineStatus 
}
</code></pre>
<pre class="mermaid">graph TB
    subgraph Machine
    ObjectMeta
    TypeMeta
    MachineSpec
    MachineStatus
    end
</pre>
<h4 id="machinespec"><a class="header" href="#machinespec">MachineSpec</a></h4>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/apis/machine/v1alpha1/machine_types.go#L54">MachineSpec</a> represents the specification of a Machine.</p>
<pre><code class="language-go">type MachineSpec struct {

	// Class is the referrent to the MachineClass. 
	Class ClassSpec 

    // Unique identification of the VM at the cloud provider
	ProviderID string 

	// NodeTemplateSpec describes the data a node should have when created from a template
	NodeTemplateSpec NodeTemplateSpec 

	// Configuration for the machine-controller.  
	*MachineConfiguration 
}
type NodeTemplateSpec struct {  // wraps a NodeSpec with ObjectMeta.
	metav1.ObjectMeta

	// NodeSpec describes the attributes that a node is created with.
	Spec corev1.NodeSpec
}
</code></pre>
<ul>
<li><code>ProviderID</code> is the unique identification of the VM at the cloud provider. <code>ProviderID</code> typically matches with the <code>node.Spec.ProviderID</code> on the node object.</li>
<li><code>Class</code> field is of type <code>ClassSpec</code> which is just the (<code>Kind</code> and the <code>Name</code>) referring to the <code>MachineClass</code>. (Ideally the field, type should have been called <code>ClassReference</code>) like <code>OwnerReference</code></li>
<li><code>NodeTemplateSpec</code> describes the data a node should have when created from a template, embeds <code>ObjectMeta</code> and holds a <a href="https://pkg.go.dev/k8s.io/api/core/v1#NodeSpec">corev1.NodeSpec</a> in its <code>Spec</code> field.
<ul>
<li>The <code>Machine.Spec.NodeTemplateSpec.Spec</code> mirrors k8s <code>Node.Spec</code></li>
</ul>
</li>
<li><code>MachineSpec</code> embeds a <code>MachineConfiguration</code> which is just a configuration object that is a connection of timeouts, maxEvictRetries and NodeConditions</li>
</ul>
<pre class="mermaid">graph TB
    subgraph MachineSpec
	Class:ClassSpec
	ProviderID:string
	NodeTemplateSpec:NodeTemplateSpec
	MachineConfiguration
    end
</pre>
<pre><code class="language-go">type MachineConfiguration struct {
	// MachineDraintimeout is the timeout after which machine is forcefully deleted.
	MachineDrainTimeout *Duration

	// MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.
	MachineHealthTimeout *Duration 

	// MachineCreationTimeout is the timeout after which machinie creation is declared failed.
	MachineCreationTimeout *Duration 

	// MaxEvictRetries is the number of retries that will be attempted while draining the node.
	MaxEvictRetries *int32 

	// NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.
	NodeConditions *string 
}
</code></pre>
<h4 id="machinestatus"><a class="header" href="#machinestatus">MachineStatus</a></h4>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/apis/machine/v1alpha1/machine_types.go#L97">pkg/apis/machine/v1alpha1.MachineStatus</a> represents the most recently observed status of Machine.</p>
<pre><code class="language-go">type MachineStatus struct {

	// Conditions of this machine, same as NodeStatus.Conditions
	Conditions []NodeCondition 

	// Last operation refers to the status of the last operation performed. NOTE: this is usually the NextOperation for reconcile!! Discuss!
	LastOperation LastOperation 

	// Current status of the machine object
	CurrentStatus CurrentStatus

	// LastKnownState can store details of the last known state of the VM by the plugins.
	// It can be used by future operation calls to determine current infrastucture state
	LastKnownState string 
}
</code></pre>
<h4 id="extended-nodeconditiontypes"><a class="header" href="#extended-nodeconditiontypes">Extended NodeConditionTypes</a></h4>
<p>The MCM extends standard k8s <a href="./k8s_facilities.html#nodeconditiontype">NodeConditionType</a> with several custom conditions. (TODO: isn't this hacky/error prone?)</p>
<ul>
<li><a href="">NodeTerminationCondition</a> defined as <code>	NodeTerminationCondition v1.NodeConditionType = &quot;Terminating&quot;</code></li>
</ul>
<p>NodeTerminationCondition</p>
<h4 id="lastoperation"><a class="header" href="#lastoperation">LastOperation</a></h4>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1#LastOperation">github.com/machine-controller-manager/pkg/apis/machine/v1alpha1.LastOperation</a> represents the last operation performed on the object. Can sometimes mean the <em>next</em> operation to be performed on the machine if the machine operation state is <code>Processing</code>! </p>
<pre><code class="language-go">type LastOperation struct {
	// Description of the operation
	Description string 

	// Last update time of operation
	LastUpdateTime Time 

	// State of operation (bad naming)
	State MachineState 

	// Type of operation
	Type MachineOperationType 
}

</code></pre>
<h5 id="machinestate-should-be-called-machineoperationstate"><a class="header" href="#machinestate-should-be-called-machineoperationstate">MachineState (should be called MachineOperationState)</a></h5>
<p>NOTE: BADLY NAMED: Should be called <code>MachineOperationState</code></p>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1#MachineState">machine-controller-manager/pkg/apis/machine/v1alpha1.MachineState</a> represents the  current state of a machine operation and is one of <code>Processing</code>, <code>Failed</code> or <code>Successful</code>.</p>
<pre><code class="language-go">// MachineState is  current state of the machine.
// BAD Name: Should be MachineOperationState
type MachineState string

// These are the valid (operation) states of machines.
const (
	// MachineStatePending means there are operations pending on this machine state
	MachineStateProcessing MachineState = &quot;Processing&quot;

	// MachineStateFailed means operation failed leading to machine status failure
	MachineStateFailed MachineState = &quot;Failed&quot;

	// MachineStateSuccessful indicates that the node is not ready at the moment
	MachineStateSuccessful MachineState = &quot;Successful&quot;
)
</code></pre>
<h5 id="machineoperationtype"><a class="header" href="#machineoperationtype">MachineOperationType</a></h5>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1#MachineOperationType">github.com/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineOperationType</a> is a label for the operation performed on a machine object: <code>Create</code>/<code>Update</code>/<code>HealthCheck</code>/<code>Delete</code>.</p>
<pre><code class="language-go">type MachineOperationType string
const (
	// MachineOperationCreate indicates that the operation is a create
	MachineOperationCreate MachineOperationType = &quot;Create&quot;

	// MachineOperationUpdate indicates that the operation is an update
	MachineOperationUpdate MachineOperationType = &quot;Update&quot;

	// MachineOperationHealthCheck indicates that the operation is a create
	MachineOperationHealthCheck MachineOperationType = &quot;HealthCheck&quot;

	// MachineOperationDelete indicates that the operation is a delete
	MachineOperationDelete MachineOperationType = &quot;Delete&quot;
)

</code></pre>
<h4 id="currentstatus"><a class="header" href="#currentstatus">CurrentStatus</a></h4>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1#CurrentStatus">github.com/machine-controller-manager/pkg/apis/machine/v1alpha1.CurrentStatus</a> encapsulates information about the current status of Machine.</p>
<pre><code class="language-go">type CurrentStatus struct {
	// Phase refers to the Machien (Lifecycle) Phase
	Phase MachinePhase 
	// TimeoutActive when set to true drives the machine controller
	// to check whether	machine failed the configured creation timeout or health check timeout and change machine phase to Failed.
	TimeoutActive bool 
	// Last update time of current status
	LastUpdateTime Time
}
</code></pre>
<h5 id="machinephase"><a class="header" href="#machinephase">MachinePhase</a></h5>
<p><code>MachinePhase</code> is a label for the life-cycle phase of a machine at a given time: <code>Unknown</code>, <code>Pending</code>, <code>Available</code>, <code>Running</code>, <code>Terminating</code>, <code>Failed</code>, <code>CrashLoopBackOff</code>,.</p>
<pre><code class="language-go">type MachinePhase string
const (
	// MachinePending means that the machine is being created
	MachinePending MachinePhase = &quot;Pending&quot;

	// MachineAvailable means that machine is present on provider but hasn't joined cluster yet
	MachineAvailable MachinePhase = &quot;Available&quot;

	// MachineRunning means node is ready and running successfully
	MachineRunning MachinePhase = &quot;Running&quot;

	// MachineRunning means node is terminating
	MachineTerminating MachinePhase = &quot;Terminating&quot;

	// MachineUnknown indicates that the node is not ready at the movement
	MachineUnknown MachinePhase = &quot;Unknown&quot;

	// MachineFailed means operation failed leading to machine status failure
	MachineFailed MachinePhase = &quot;Failed&quot;

	// MachineCrashLoopBackOff means creation or deletion of the machine is failing.
	MachineCrashLoopBackOff MachinePhase = &quot;CrashLoopBackOff&quot;
)
</code></pre>
<h4 id="finalizers"><a class="header" href="#finalizers">Finalizers</a></h4>
<p>See <a href="./k8s_facilities.html#finalizers-and-deletion">K8s Finalizers</a>. The MC defines the following finalizer keys</p>
<pre><code class="language-go">const (
	MCMFinalizerName = &quot;machine.sapcloud.io/machine-controller-manager&quot;
	MCFinalizerName = &quot;machine.sapcloud.io/machine-controller&quot;
)
</code></pre>
<ol>
<li><code>MCMFinalizerName</code> is the finalizer used to tag dependecies before deletion</li>
<li><code>MCFinalizerName</code> is the finalizer added on <code>Secret</code> objects.</li>
</ol>
<h3 id="machineclass"><a class="header" href="#machineclass">MachineClass</a></h3>
<p>A <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineClass">MachineClass</a> is used to used to templatize and re-use provider configuration across multiple Machines or MachineSets or MachineDeployments</p>
<details>
<summary>Example MachineClass YAML for AWS provider</summary>
<pre><code class="language-yaml">apiVersion: machine.sapcloud.io/v1alpha1
credentialsSecretRef:
  name: cloudprovider
  namespace: shoot--i544024--hana-test
kind: MachineClass
metadata:
  creationTimestamp: &quot;2022-10-20T13:08:07Z&quot;
  finalizers:
  - machine.sapcloud.io/machine-controller-manager
  generation: 1
  labels:
    failure-domain.beta.kubernetes.io/zone: eu-west-1c
  name: shoot--i544024--hana-test-whana-z1-b0f23
  namespace: shoot--i544024--hana-test
  resourceVersion: &quot;38424578&quot;
  uid: 656f863e-5061-420f-8710-96dcc9777be4
nodeTemplate:
  capacity:
    cpu: &quot;4&quot;
    gpu: &quot;1&quot;
    memory: 61Gi
  instanceType: p2.xlarge
  region: eu-west-1
  zone: eu-west-1c
provider: AWS
providerSpec:
  ami: ami-0c3484dbcde4c4d0c
  blockDevices:
  - ebs:
      deleteOnTermination: true
      encrypted: true
      volumeSize: 50
      volumeType: gp3
  iam:
    name: shoot--i544024--hana-test-nodes
  keyName: shoot--i544024--hana-test-ssh-publickey
  machineType: p2.xlarge
  networkInterfaces:
  - securityGroupIDs:
    - sg-0445497aa49ddecb2
    subnetID: subnet-04a338730d20ea601
  region: eu-west-1
  srcAndDstChecksEnabled: false
  tags:
    kubernetes.io/arch: amd64
    kubernetes.io/cluster/shoot--i544024--hana-test: &quot;1&quot;
    kubernetes.io/role/node: &quot;1&quot;
    networking.gardener.cloud/node-local-dns-enabled: &quot;true&quot;
    node.kubernetes.io/role: node
    worker.garden.sapcloud.io/group: whana
    worker.gardener.cloud/cri-name: containerd
    worker.gardener.cloud/pool: whana
    worker.gardener.cloud/system-components: &quot;true&quot;
secretRef:
  name: shoot--i544024--hana-test-whana-z1-b0f23
  namespace: shoot--i544024--hana-test
</code></pre>
</details>
<br />
<p>Type definition for <code>MachineClass</code> shown below</p>
<pre><code class="language-go">type MachineClass struct {
	metav1.TypeMeta 
	metav1.ObjectMeta 

	ProviderSpec runtime.RawExtension `json:&quot;providerSpec&quot;`
	SecretRef *corev1.SecretReference `json:&quot;secretRef,omitempty&quot;`
	CredentialsSecretRef *corev1.SecretReference
	Provider string 


	NodeTemplate *NodeTemplate 
}
</code></pre>
<p>Notes</p>
<ul>
<li>As can be seen above , the provider specific configuration to create a node is specified in <code>MachineClass.ProviderSpec</code> and this is of extensible custom type <a href="./k8s_facilities.html#rawextension">runtime.RawExtension</a>. This permits instances of different structure types like AWS <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager-provider-aws@v0.14.0/pkg/aws/apis#AWSProviderSpec">aws/api/AWSProviderSpec</a> or <a href="https://github.com/gardener/machine-controller-manager-provider-azure/blob/v0.9.0/pkg/azure/apis/azure_provider_spec.go#L38">azure/apis.AzureProviderSpec</a> to be held within a single type.</li>
</ul>
<h4 id="secretref-and-credentialssecretref"><a class="header" href="#secretref-and-credentialssecretref">SecretRef and CredentialsSecretRef</a></h4>
<p>Both these fields in the MachineClass are of type <a href="https://pkg.go.dev/k8s.io/api/core/v1#SecretReference">k8s.io/api/core/v1.SecretReference</a>.</p>
<p>Generally, When there is some operation to be performed on the machine, the K8s secret objects are obtained using the <a href="https://pkg.go.dev/k8s.io/client-go/listers/core/v1#SecretLister">k8s.io/client-go/listers/core/v1.SecretLister</a>. The corresponding <a href="https://pkg.go.dev/k8s.io/api/core/v1#Secret.Data">Secret.Data</a> map fields are retrieved and then merged into a single map. (Design TODO: Why do we do this and why can't use just one secret ?)</p>
<p>Snippet of AWS MachineClass containing <code>CredentialsSecretRef</code> and <code>SecretRef</code></p>
<pre><code class="language-yaml">apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineClass
credentialsSecretRef:
  name: cloudprovider
  namespace: shoot--i034796--tre
secretRef:
¬† name: shoot--i034796--tre-worker-q3rb4-z1-30c4a
¬† namespace: shoot--i034796--tre
//...
</code></pre>
<ul>
<li><code>MachineClass.CredentialsSecretRef</code> is a reference to a secret that generally holds the cloud provider credentials. For the example above the corresponding <code>Secret</code> holds the <code>accessKeyID</code> and <code>secretAccessKey</code>. 
<code>k get secret cloudprovider -n shoot--i034796--tre -oyaml</code></li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
data:
accessKeyID: QUtJQTZ...
secretAccessKey: dm52MX...
</code></pre>
<ul>
<li><code>MachineClass.SecretRef</code> points to a <code>Secret</code> whose <code>Secret.Data</code> contains a <code>userData</code> entry that has a <em>lot</em> of info.
<ul>
<li>The <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">Bootstrap Token</a> which is generated by the MCM. See <a href="https://github.com/gardener/gardener/issues/3898">Bootstrap Tokens Per Shoot Worker Machine</a>.</li>
<li>shoot api server address.</li>
<li>kubeconfig to contact shoot api server.</li>
<li>TODO: Discuss this. Why so much info in one secret field?</li>
</ul>
</li>
<li><code>MachineClass.Provider</code> is specified as the combination of name and location of cloud-specific drivers. (Unsure if this is being actually being followed as for AWS&amp;Azure this is set to simply <code>AWS</code> and <code>Azure</code> respectively)</li>
</ul>
<h4 id="nodetemplate"><a class="header" href="#nodetemplate">NodeTemplate</a></h4>
<p>A <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#NodeTemplate">NodeTemplate</a> as shown below</p>
<ol>
<li><code>Capacity</code> is of type <a href="https://pkg.go.dev/k8s.io/api/core/v1#ResourceList">k8s.io/api/core/v1.ResourceList</a> which is effectively a map of (resource name, quantity) pairs. Similar to <a href="./k8s_facilities.html#capacity">Node Capacity</a>, it has keys like <code>cpu</code>, <code>gpu</code>, <code>memory</code>, <code>storage</code>, etc and string values that are represented by <a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity">resource.Quantity</a></li>
<li><code>InstanceType</code> provider specific Instance type of the node belonging to nodeGroup. For AWS this would be the EC2 instance type like <code>p2.xlarge</code> for AWS.</li>
<li><code>Region</code>: provider specific region name like <code>eu-west-1</code> for AWS.</li>
<li><code>Zone</code>: provider specified Availability Zone like <code>eu-west-1c</code> for AWS.</li>
</ol>
<pre><code class="language-go">type NodeTemplate struct {
	Capacity corev1.ResourceList 
	InstanceType string 
	Region string 
	Zone string 
}
</code></pre>
<p>Example</p>
<pre><code class="language-yaml">nodeTemplate:
  capacity:
    cpu: &quot;4&quot;
    gpu: &quot;1&quot;
    memory: 61Gi
  instanceType: p2.xlarge
  region: eu-west-1
  zone: eu-west-1c

</code></pre>
<h3 id="machineset"><a class="header" href="#machineset">MachineSet</a></h3>
<p>A <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineSet">MachineSet</a> is to a <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#Machine">Machine</a> in an analogue of what a <a href="https://pkg.go.dev/k8s.io/api/apps/v1#ReplicaSet">ReplicaSet</a> is to a <a href="https://pkg.go.dev/k8s.io/api/core/v1#Pod">Pod</a>. A <code>MachineSet</code> ensures that the specified number of <code>Machines</code> are running at any given time. A <code>MachineSet</code> is rarely rarely created directly. It is generally owned by its parent <a href="mcm_facilities.html#machinedeployment">MachineDeployment</a> and its <code>ObjectMetadata.OwnerReferenes</code> slice has a reference to the parent deployment.</p>
<p><code>MachineSet</code> struct is defined as follows:</p>
<pre><code class="language-go">type MachineSet struct {
	metav1.ObjectMeta 
	metav1.TypeMeta

	Spec MachineSetSpec 
	Status MachineSetStatus 
}
</code></pre>
<h4 id="machinesetspec"><a class="header" href="#machinesetspec">MachineSetSpec</a></h4>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineSetSpec">MachineSetSpec</a> is the specification of a MachineSet.</p>
<pre><code class="language-go">type MachineSetSpec struct {
	Replicas int32 
	Selector *metav1.LabelSelector 
	MinReadySeconds int32 
	MachineClass ClassSpec 
	Template MachineTemplateSpec // Am I used ?
}
type ClassSpec struct {
	APIGroup string 
	Kind string 
	Name string 
}
type MachineTemplateSpec struct { 
	metav1.ObjectMeta 
	Spec MachineSpec
}
</code></pre>
<ul>
<li><code>MachineSetSpec.Replicas</code> is the number of desired replicas.</li>
<li><code>MachineSetSpec.Selector</code> is a label query over machines that should match the replica count. See <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">Label Selectors</a></li>
<li><code>MachineSetSpec.MinReadySeconds</code> - TODO: unsure ? Mininum number of seconds for which a newly created <code>Machine</code> should be ready for it to be considered as available ? (guessing here - can't find the code using this). Usually specified as <code>500</code> (ie in the YAML)</li>
<li><code>MachineSetSpec.MachineClass</code> is an instance of type <code>ClassSpec</code> which is a reference type to the <a href="mcm_facilities.html#machineclass">MachineClass</a>.</li>
<li>TODO: Discuss whether needed. <code>MachineSetSpec.Template</code> is an instance of <code>MachineTemplateSpec</code> which is an encapsulation over <a href="mcm_facilities.html#machinespec">MachineSpec</a>. I don't see this used since <code>MachineSetSpec.MachineClass</code> is already a reference to a <code>MachineClass</code> which by definition is a template.</li>
</ul>
<h4 id="example-machineset-yaml"><a class="header" href="#example-machineset-yaml">Example MachineSet YAML</a></h4>
<details>
<summary>AWs MachineSet YAML</summary>
<pre><code class="language-yaml">apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineSet
metadata:
  annotations:
    deployment.kubernetes.io/desired-replicas: &quot;1&quot;
    deployment.kubernetes.io/max-replicas: &quot;2&quot;
    deployment.kubernetes.io/revision: &quot;1&quot;
  creationTimestamp: &quot;2022-10-20T13:53:01Z&quot;
  finalizers:
  - machine.sapcloud.io/machine-controller-manager
  generation: 1
  labels:
    machine-template-hash: &quot;2415498538&quot;
    name: shoot--i034796--tre-worker-q3rb4-z1
  name: shoot--i034796--tre-worker-q3rb4-z1-68598
  namespace: shoot--i034796--tre
  ownerReferences:
  - apiVersion: machine.sapcloud.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: MachineDeployment
    name: shoot--i034796--tre-worker-q3rb4-z1
    uid: f20cba25-2fdb-4315-9e38-d301f5f08459
  resourceVersion: &quot;38510891&quot;
  uid: b0f12abc-2d19-4a6e-a69d-4bf4aa12d02b
spec:
  minReadySeconds: 500
  replicas: 1
  selector:
    matchLabels:
      machine-template-hash: &quot;2415498538&quot;
      name: shoot--i034796--tre-worker-q3rb4-z1
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine-template-hash: &quot;2415498538&quot;
        name: shoot--i034796--tre-worker-q3rb4-z1
    spec:
      class:
        kind: MachineClass
        name: shoot--i034796--tre-worker-q3rb4-z1-30c4a
      nodeTemplate:
        metadata:
          creationTimestamp: null
          labels:
            kubernetes.io/arch: amd64
            networking.gardener.cloud/node-local-dns-enabled: &quot;true&quot;
            node.kubernetes.io/role: node
            topology.ebs.csi.aws.com/zone: eu-west-1b
            worker.garden.sapcloud.io/group: worker-q3rb4
            worker.gardener.cloud/cri-name: containerd
            worker.gardener.cloud/pool: worker-q3rb4
            worker.gardener.cloud/system-components: &quot;true&quot;
        spec: {}
status:
  availableReplicas: 1
  fullyLabeledReplicas: 1
  lastOperation:
    lastUpdateTime: &quot;2022-10-20T13:55:23Z&quot;
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
</code></pre>
</details>
<h3 id="machinedeployment"><a class="header" href="#machinedeployment">MachineDeployment</a></h3>
<p>A <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineDeployment">MachineDeployment</a> is to a <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineSet">MachineSet</a> in an analogue of what a <a href="https://pkg.go.dev/k8s.io/api/apps/v1#Deployment">Deployment</a> is to a <a href="https://pkg.go.dev/k8s.io/api/apps/v1#ReplicaSet">ReplicaSet</a>. </p>
<p>A <code>MachineDeployment</code> manages MachineSets and enables declarative updates for the machines in MachineSets. </p>
<p><code>MachineDeployment</code> struct is defined as follows</p>
<pre><code class="language-go">type MachineDeployment struct {
	metav1.TypeMeta 
	metav1.ObjectMeta 
	Spec MachineDeploymentSpec 
	// Most recently observed status of the MachineDeployment.
	// +optional
	Status MachineDeploymentStatus 
</code></pre>
<h4 id="machinedeploymentspec"><a class="header" href="#machinedeploymentspec">MachineDeploymentSpec</a></h4>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineDeploymentSpec">MachineDeploymentSpec</a> is the is the specification of the desired behavior of the MachineDeployment.</p>
<pre><code class="language-go">type MachineDeploymentSpec struct {
	Replicas int32 
	Selector *metav1.LabelSelector
	Template MachineTemplateSpec 
	MinReadySeconds int32 
	Paused bool 
	ProgressDeadlineSeconds *int32 
	Strategy MachineDeploymentStrategy 
}
</code></pre>
<ul>
<li><code>Replicas</code> is number of desired machines.</li>
<li><code>Selector</code>is label selector for machines. Usually a <code>name</code> label to select machines with a given name matching the deployment name. </li>
<li><code>Template</code> of type <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineTemplateSpec">MachineTemplateSpec</a> which is an encapsulation over <a href="mcm_facilities.html#machinespec">MachineSpec</a>.</li>
<li><code>MinReadySeconds</code> is the Minimum number of seconds for which a newly created machine should be ready without any of its container crashing, for it to be considered available.</li>
<li><code>Paused</code> indicates that the <code>MachineDeployment</code> is paused and will not be processed by the MCM controller.</li>
<li><code>Strategy</code> is the MachineDeploymentStrategy strategy to use to replace existing machines with new ones.</li>
<li><code>ProgressDeadlineSeconds</code> maximum time in seconds for a MachineDeployment to make progress before it is considered to be failed. The MachineDeployment controller will continue to  process failed MachineDeployments and a condition with a ProgressDeadlineExceeded  reason will be surfaced in the MachineDeployment status.</li>
</ul>
<h4 id="machinedeploymentstrategy"><a class="header" href="#machinedeploymentstrategy">MachineDeploymentStrategy</a></h4>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineDeploymentStrategy">MachineDeploymentStrategy</a> describes how to replace existing machines with new ones.</p>
<pre><code class="language-go">type MachineDeploymentStrategy struct {
	Type MachineDeploymentStrategyType
	RollingUpdate *RollingUpdateMachineDeployment
}

type MachineDeploymentStrategyType string
const (
	RecreateMachineDeploymentStrategyType MachineDeploymentStrategyType = &quot;Recreate&quot;
	RollingUpdateMachineDeploymentStrategyType MachineDeploymentStrategyType = &quot;RollingUpdate&quot;
)

type RollingUpdateMachineDeployment struct {
	MaxUnavailable *intstr.IntOrString
	MaxSurge *intstr.IntOrString
}

</code></pre>
<ol>
<li><code>Type</code>is one of the <code>MachineDeploymentStrategyType</code> constants
<ol>
<li><code>RecreateMachineDeploymentStrategyType</code> is strategy to Kill all existing machines before creating new ones. </li>
<li><code>RollingUpdateMachineDeploymentStrategyType</code> is strategy to replace the old machines by new one using rolling update i.e gradually scale down the old machines and scale up the new one.</li>
</ol>
</li>
<li><code>RollingUpdate</code> is the rolling update config params represented by <code>RollingUpdateMachineDeployment</code>. This is analogous to <a href="https://pkg.go.dev/k8s.io/api/apps/v1#RollingUpdateDeployment">k8s.io/api/apps/v1.RollingUpdateDeployment</a> which also has <code>MaxUnavailable</code> and <code>MaxSurge</code>.
<ol>
<li><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#RollingUpdateMachineDeployment.MaxUnavailable">RollingUpdateMachineDeployment.MaxUnavailable</a> is the maximum number of machines that can be unavailable during the update. </li>
<li><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#RollingUpdateMachineDeployment.MaxSurge">RollingUpdateMachineDeployment.MaxSurge</a> is the maximum number of machines that can be scheduled above the desired number of
machines.</li>
</ol>
</li>
</ol>
<h4 id="example-machinedeployment-yaml"><a class="header" href="#example-machinedeployment-yaml">Example MachineDeployment YAML</a></h4>
<details>
<summary>AWS MchineDeployment YAML</summary>
<pre><code class="language-yaml">apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineDeployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;1&quot;
  creationTimestamp: &quot;2022-10-20T13:53:01Z&quot;
  finalizers:
  - machine.sapcloud.io/machine-controller-manager
  generation: 1
  name: shoot--i034796--tre-worker-q3rb4-z1
  namespace: shoot--i034796--tre
  resourceVersion: &quot;38510892&quot;
  uid: f20cba25-2fdb-4315-9e38-d301f5f08459
spec:
  minReadySeconds: 500
  replicas: 1
  selector:
    matchLabels:
      name: shoot--i034796--tre-worker-q3rb4-z1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: shoot--i034796--tre-worker-q3rb4-z1
    spec:
      class:
        kind: MachineClass
        name: shoot--i034796--tre-worker-q3rb4-z1-30c4a
      nodeTemplate:
        metadata:
          creationTimestamp: null
          labels:
            kubernetes.io/arch: amd64
            networking.gardener.cloud/node-local-dns-enabled: &quot;true&quot;
            node.kubernetes.io/role: node
            topology.ebs.csi.aws.com/zone: eu-west-1b
            worker.garden.sapcloud.io/group: worker-q3rb4
            worker.gardener.cloud/cri-name: containerd
            worker.gardener.cloud/pool: worker-q3rb4
            worker.gardener.cloud/system-components: &quot;true&quot;
        spec: {}
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: &quot;2022-10-20T13:55:23Z&quot;
    lastUpdateTime: &quot;2022-10-20T13:55:23Z&quot;
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &quot;True&quot;
    type: Available
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
</code></pre>
</details>
<h3 id="volumeattachment"><a class="header" href="#volumeattachment">VolumeAttachment</a></h3>
<p>A <a href="https://pkg.go.dev/k8s.io/api/storage/v1#VolumeAttachment">k8s.io/api/storage/v1.VolumeAttachment</a> is a non-namespaced object that captures the intent to attach or detach the specified volume to/from the specified node (specified in <code>VolumeAttachmentSpec.NodeName</code>).</p>
<pre><code class="language-go">type VolumeAttachment struct {
	metav1.TypeMeta 
	metav1.ObjectMeta 

	// Specification of the desired attach/detach volume behavior.
	// Populated by the Kubernetes system.
	Spec VolumeAttachmentSpec 

	// Status of the VolumeAttachment request.
	// Populated by the entity completing the attach or detach
	// operation, i.e. the external-attacher.
	Status VolumeAttachmentStatus 
}
</code></pre>
<h3 id="volumeattachmentspec"><a class="header" href="#volumeattachmentspec">VolumeAttachmentSpec</a></h3>
<p>A <a href="https://pkg.go.dev/k8s.io/api/storage/v1#VolumeAttachmentSpec">k8s.ip/api/storage/v1.VolumeAttachmentSpec</a>is the specification of a VolumeAttachment request.</p>
<pre><code class="language-go">type VolumeAttachmentSpec struct {
	// Attacher indicates the name of the volume driver that MUST handle this
	// request. Same as CSI Plugin name
	Attacher string 

	// Source represents the volume that should be attached.
	Source VolumeAttachmentSource 

	// The node that the volume should be attached to.
	NodeName string
}
</code></pre>
<p>See <a href="https://pkg.go.dev/k8s.io/api@v0.25.2/storage/v1">Storage V1 Docs</a> for further elaboration. </p>
<h2 id="utilities"><a class="header" href="#utilities">Utilities</a></h2>
<h3 id="nodeops"><a class="header" href="#nodeops">NodeOps</a></h3>
<h4 id="nodeopsgetnodecondition"><a class="header" href="#nodeopsgetnodecondition">nodeops.GetNodeCondition</a></h4>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/nodeops/conditions.go#L72">machine-controller-manager/pkg/util/nodeops.GetNodeCondition</a> get the nodes condition matching the specified type </p>
<pre><code class="language-go">func GetNodeCondition(ctx context.Context, c clientset.Interface, nodeName string, conditionType v1.NodeConditionType) (*v1.NodeCondition, error) {
	node, err := c.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
	if err != nil {
		return nil, err
	}
	return getNodeCondition(node, conditionType), nil
}
func getNodeCondition(node *v1.Node, conditionType v1.NodeConditionType) *v1.NodeCondition {
	for _, cond := range node.Status.Conditions {
		if cond.Type == conditionType {
			return &amp;cond
		}
	}
	return nil
}
</code></pre>
<h4 id="nodeopscloneandaddcondition"><a class="header" href="#nodeopscloneandaddcondition">nodeops.CloneAndAddCondition</a></h4>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/nodeops/conditions.go#L31">machine-controller-manager/pkg/util/nodeops.CloneAndAddCondition</a> adds a condition to the node condition slice. If condition with this type already exists, it updates the <code>LastTransitionTime</code></p>
<pre><code class="language-go">func CloneAndAddCondition(conditions []v1.NodeCondition, condition v1.NodeCondition) []v1.NodeCondition {
	if condition.Type == &quot;&quot; || condition.Status == &quot;&quot; {
		return conditions
	}
	var newConditions []v1.NodeCondition

	for _, existingCondition := range conditions {
		if existingCondition.Type != condition.Type { 
			newConditions = append(newConditions, existingCondition)
		} else { 
			// condition with this type already exists
			if existingCondition.Status == condition.Status 
			&amp;&amp; existingCondition.Reason == condition.Reason {
				// condition status and reason are  the same, keep existing transition time
				condition.LastTransitionTime = existingCondition.LastTransitionTime
			}
		}
	}
	newConditions = append(newConditions, condition)
	return newConditions
}
</code></pre>
<p>TODO: Bug ? Logic above will end up adding duplicate node condition if status and reason phrase are different</p>
<h4 id="nodeopsaddorupdateconditionsonnode"><a class="header" href="#nodeopsaddorupdateconditionsonnode">nodeops.AddOrUpdateConditionsOnNode</a></h4>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/nodeops/conditions.go#L81">machine-controller-manager/pkg/util/nodeops.AddOrUpdateConditionsOnNode</a> adds a condition to the node's status, retrying on conflict with backoff.</p>
<pre><code class="language-go">func AddOrUpdateConditionsOnNode(ctx context.Context, c clientset.Interface, nodeName string, condition v1.NodeCondition) error 
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Init[&quot;backoff = wait.Backoff{
	Steps:    5,
	Duration: 100 * time.Millisecond,
	Jitter:   1.0,
}&quot;]
--&gt;retry.RetryOnConflict[&quot;retry.RetryOnConflict(backoff, fn)&quot;]
--&gt;GetNode[&quot;oldNode, err = c.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})&quot;]
subgraph &quot;fn&quot;
    GetNode--&gt;ChkIfErr{&quot;err != nil&quot;}
	ChkIfErr--Yes--&gt;ReturnErr((&quot;return err&quot;))
	ChkIfErr--No--&gt;InitNewNode[&quot;newNode := oldNode.DeepCopy()&quot;]
	InitNewNode--&gt;InitConditions[&quot;newNode.Status.Conditions:= CloneAndAddCondition(newNode.Status.Conditions, condition)&quot;]
	--&gt;UpdateNewNode[&quot;_, err := c.CoreV1().Nodes().UpdateStatus(ctx, newNodeClone, metav1.UpdateOptions{}&quot;]
	--&gt;ReturnErr
end
</pre>
<h3 id="machineutils"><a class="header" href="#machineutils">MachineUtils</a></h3>
<h4 id="operation-descriptions"><a class="header" href="#operation-descriptions">Operation Descriptions</a></h4>
<p><code>machineutils</code> has a bunch of constants that are descriptions of machine operations that are set into <code>machine.Status.LastOperation.Description</code> by the machine controller while performing reconciliation. It also has some <a href="k8s_facilities.html#conditions">reason phrase</a></p>
<pre><code class="language-go">const (
	// GetVMStatus sets machine status to terminating and specifies next step as getting VMs
	GetVMStatus = &quot;Set machine status to termination. Now, getting VM Status&quot;

	// InitiateDrain specifies next step as initiate node drain
	InitiateDrain = &quot;Initiate node drain&quot;

	// InitiateVMDeletion specifies next step as initiate VM deletion
	InitiateVMDeletion = &quot;Initiate VM deletion&quot;

	// InitiateNodeDeletion specifies next step as node object deletion
	InitiateNodeDeletion = &quot;Initiate node object deletion&quot;

	// InitiateFinalizerRemoval specifies next step as machine finalizer removal
	InitiateFinalizerRemoval = &quot;Initiate machine object finalizer removal&quot;

	// LastAppliedALTAnnotation contains the last configuration of annotations, 
	// labels &amp; taints applied on the node object
	LastAppliedALTAnnotation = &quot;node.machine.sapcloud.io/last-applied-anno-labels-taints&quot;

	// MachinePriority is the annotation used to specify priority
	// associated with a machine while deleting it. The less its
	// priority the more likely it is to be deleted first
	// Default priority for a machine is set to 3
	MachinePriority = &quot;machinepriority.machine.sapcloud.io&quot;

	// MachineClassKind is used to identify the machineClassKind for generic machineClasses
	MachineClassKind = &quot;MachineClass&quot;

	// MigratedMachineClass annotation helps in identifying machineClasses who have been migrated by migration controller
	MigratedMachineClass = &quot;machine.sapcloud.io/migrated&quot;

	// NotManagedByMCM annotation helps in identifying the nodes which are not handled by MCM
	NotManagedByMCM = &quot;node.machine.sapcloud.io/not-managed-by-mcm&quot;

	// TriggerDeletionByMCM annotation on the node would trigger the deletion of the corresponding machine object in the control cluster
	TriggerDeletionByMCM = &quot;node.machine.sapcloud.io/trigger-deletion-by-mcm&quot;

	// NodeUnhealthy is a node termination reason for failed machines
	NodeUnhealthy = &quot;Unhealthy&quot;

	// NodeScaledDown is a node termination reason for healthy deleted machines
	NodeScaledDown = &quot;ScaleDown&quot;

	// NodeTerminationCondition describes nodes that are terminating
	NodeTerminationCondition v1.NodeConditionType = &quot;Terminating&quot;
)

</code></pre>
<h4 id="retry-periods"><a class="header" href="#retry-periods">Retry Periods</a></h4>
<p>These are standard retry periods that are internally used by the machine controllers to enqueue keys into the work queue after the specified duration so that reconciliation can be retried afer elapsed duration.</p>
<pre><code class="language-go">// RetryPeriod is an alias for specifying the retry period
type RetryPeriod time.Duration

// These are the valid values for RetryPeriod
const (
	// ShortRetry tells the controller to retry after a short duration - 15 seconds
	ShortRetry RetryPeriod = RetryPeriod(15 * time.Second)
	// MediumRetry tells the controller to retry after a medium duration - 2 minutes
	MediumRetry RetryPeriod = RetryPeriod(3 * time.Minute)
	// LongRetry tells the controller to retry after a long duration - 10 minutes
	LongRetry RetryPeriod = RetryPeriod(10 * time.Minute)
)

</code></pre>
<h3 id="misc"><a class="header" href="#misc">Misc</a></h3>
<h4 id="permitspermitgiver"><a class="header" href="#permitspermitgiver">permits.PermitGiver</a></h4>
<p><code>permits.PermitGiver</code> provides the ability to register, obtain, release and delete a number of permits for a given key. All operations are concurrent safe.</p>
<pre><code class="language-go">type PermitGiver interface {
	RegisterPermits(key string, numPermits int) //numPermits should be  maxNumPermits
	TryPermit(key string, timeout time.Duration) bool
	ReleasePermit(key string)
	DeletePermits(key string)
	Close()
}
</code></pre>
<p>The implementation of <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/permits/permits.go#L29">github.com/gardener/machine-controller-manager/pkg/util/permits.PermitGiver</a> maintains:</p>
<ul>
<li>a sync map of permit keys mapped to permits, where each permit is a structure comprising a buffered empty struct <code>struct{}</code> channel  with buffer size equalling the (max) number of permits. 
<ul>
<li><code>RegisterPermits(key, numPermits)</code> registers <code>numPermits</code> for the given <code>key</code>. A <code>permit</code> struct is initialized with <code>permit.c</code> buffer size as <code>numPermits</code>.</li>
</ul>
</li>
<li>entries are deleted if not accessed for a configured time represented by <code>stalePermitKeyTimeout</code>. This is done by 'janitor' go-routine associated with the permit giver instance.</li>
<li>When attempting to get a permit using <code>TryPermit</code>, one writes an empty struct to the permit channel within a given timeout. If one can do so within the timeout one has acquired the permit, else not. 
<ul>
<li><code>TryPermit(key, timeout)</code> attempts to get a permit for the given key by sending a <code>struct{}{}</code> instance to the buffered <code>permit.c</code> channel. </li>
</ul>
</li>
</ul>
<pre><code class="language-go">type permit struct {
	// lastAcquiredPermitTime is time since last successful TryPermit or new RegisterPermits
	lastAcquiredPermitTime time.Time
	// c is initialized with buffer size N representing num permits
	c                      chan struct{} 
}
type permitGiver struct {
	keyPermitsMap sync.Map // map of string keys to permit struct values
	stopC         chan struct{}
}

</code></pre>
<h5 id="permitsnewpermitgiver"><a class="header" href="#permitsnewpermitgiver">permits.NewPermitGiver</a></h5>
<p><code>permits.NewPermitGiver</code> returns a new <code>PermitGiver</code></p>
<pre><code class="language-go">func NewPermitGiver(stalePermitKeyTimeout time.Duration, janitorFrequency time.Duration) PermitGiver
</code></pre>
<pre class="mermaid">
%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
	Begin((&quot; &quot;))
	--&gt;InitStopCh[&quot;stopC := make(chan struct{})&quot;]
	--&gt;InitPG[&quot;
		pg := permitGiver{
		keyPermitsMap: sync.Map{},
		stopC:         stopC,
	}&quot;]
	--&gt;LaunchCleanUpGoroutine[&quot;go cleanup()&quot;]
	LaunchCleanUpGoroutine--&gt;Return((&quot;return &amp;pg&quot;))
	LaunchCleanUpGoroutine---&gt;InitTicker
	subgraph cleanup
	InitTicker[&quot;ticker := time.NewTicker(janitorFrequency)&quot;]
	--&gt;caseReadStopCh{&quot;&lt;-stopC ?&quot;}
	caseReadStopCh--Yes--&gt;End((&quot;End&quot;))
	caseReadStopCh--No
		--&gt;ReadTickerCh{&quot;&lt;-ticker.C ?&quot;}
	ReadTickerCh--Yes--&gt;CleanupStale[&quot;
	pg.cleanupStalePermitEntries(stalePermitKeyTimeout)
	(Iterates keyPermitsMap, remove entries whose 
	lastAcquiredPermitTime exceeds stalePermitKeyTimeout)
	&quot;]
	ReadTickerCh--No--&gt;caseReadStopCh
	end
</pre>
<h2 id="main-server-structs"><a class="header" href="#main-server-structs">Main Server Structs</a></h2>
<h3 id="mcserver"><a class="header" href="#mcserver">MCServer</a></h3>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/app/options/options.go#L40">machine-controller-manager/pkg/util/provider/app/options.MCServer</a>
is the main server context object for the machine controller. It embeds <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/options/types.go#L45">options.MachineControllerConfiguration</a> and has a <code>ControlKubeConfig</code> and <code>TargetKubeConfig</code> string fields.</p>
<pre><code class="language-go">type MCServer struct {
	options.MachineControllerConfiguration

	ControlKubeconfig string
	TargetKubeconfig  string

</code></pre>
<p>The <code>MCServer</code> is constructed and initialized using the  <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/app/options/options.go#L48">pkg/util/provider/app/options.NewMCServer</a> function which sets most of the default values for fields for the embedded struct.</p>
<h4 id="mcserver-usage"><a class="header" href="#mcserver-usage">MCServer Usage</a></h4>
<p>Individual providers leverage the MCServer as follows:</p>
<pre><code class="language-go">  s := options.NewMCServer()
  driver := &lt;providerSpecificDriverInitialization&gt;
  if err := app.Run(s, driver); err != nil {
      fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err)
      os.Exit(1)
  }
</code></pre>
<p>The MCServer is thus re-used across different providers.</p>
<h4 id="machinecontrollerconfiguration-struct"><a class="header" href="#machinecontrollerconfiguration-struct">MachineControllerConfiguration struct</a></h4>
<p>An imnportant struct that represents machine configuration that supports deep-coopying and is embedded within the <code>MCServer</code></p>
<p><code>machine-controller-manager/pkg/util/provider/options.MachineControllerConfiguration</code></p>
<pre><code class="language-go">type MachineControllerConfiguration struct {
	metav1.TypeMeta

	// namespace in seed cluster in which controller would look for the resources.
	Namespace string

	// port is the port that the controller-manager's http service runs on.
	Port int32
	// address is the IP address to serve on (set to 0.0.0.0 for all interfaces).
	Address string
	// CloudProvider is the provider for cloud services.
	CloudProvider string
	// ConcurrentNodeSyncs is the number of node objects that are
	// allowed to sync concurrently. Larger number = more responsive nodes,
	// but more CPU (and network) load.
	ConcurrentNodeSyncs int32

	// enableProfiling enables profiling via web interface host:port/debug/pprof/
	EnableProfiling bool
	// enableContentionProfiling enables lock contention profiling, if enableProfiling is true.
	EnableContentionProfiling bool
	// contentType is contentType of requests sent to apiserver.
	ContentType string
	// kubeAPIQPS is the QPS to use while talking with kubernetes apiserver.
	KubeAPIQPS float32
	// kubeAPIBurst is the burst to use while talking with kubernetes apiserver.
	KubeAPIBurst int32
	// leaderElection defines the configuration of leader election client.
	LeaderElection mcmoptions.LeaderElectionConfiguration
	// How long to wait between starting controller managers
	ControllerStartInterval metav1.Duration
	// minResyncPeriod is the resync period in reflectors; will be random between
	// minResyncPeriod and 2*minResyncPeriod.
	MinResyncPeriod metav1.Duration

	// SafetyOptions is the set of options to set to ensure safety of controller
	SafetyOptions SafetyOptions

	//NodeCondition is the string of known NodeConditions. If any of these NodeCondition is set for a timeout period, the machine  will be declared failed and will replaced. Default is &quot;KernelDeadlock,ReadonlyFilesystem,DiskPressure,NetworkUnavailable&quot;
	NodeConditions string

	//BootstrapTokenAuthExtraGroups is a comma-separated string of groups to set bootstrap token's &quot;auth-extra-groups&quot; field to.
	BootstrapTokenAuthExtraGroups string
}

</code></pre>
<h5 id="safetyoptions"><a class="header" href="#safetyoptions">SafetyOptions</a></h5>
<p>An important struct availablea as the <code>SafetyOptions</code> field in <code>MachineControllerConfiguration</code> containing several timeouts, retry-limits, etc. Most of these fields are set via <a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/app/options/options.go#L48">pkg/util/provider/app/options.NewMCServer</a> function</p>
<p><code>pkg/util/provider/option.SafetyOptions</code></p>
<pre><code class="language-go">// SafetyOptions are used to configure the upper-limit and lower-limit
// while configuring freezing of machineSet objects
type SafetyOptions struct {
	// Timeout (in durartion) used while creation of
	// a machine before it is declared as failed
	MachineCreationTimeout metav1.Duration
	// Timeout (in durartion) used while health-check of
	// a machine before it is declared as failed
	MachineHealthTimeout metav1.Duration
	// Maximum number of times evicts would be attempted on a pod for it is forcibly deleted
	// during draining of a machine.
	MaxEvictRetries int32
	// Timeout (in duration) used while waiting for PV to detach
	PvDetachTimeout metav1.Duration
	// Timeout (in duration) used while waiting for PV to reattach on new node
	PvReattachTimeout metav1.Duration

	// Timeout (in duration) for which the APIServer can be down before
	// declare the machine controller frozen by safety controller
	MachineSafetyAPIServerStatusCheckTimeout metav1.Duration
	// Period (in durartion) used to poll for orphan VMs
	// by safety controller
	MachineSafetyOrphanVMsPeriod metav1.Duration
	// Period (in duration) used to poll for APIServer's health
	// by safety controller
	MachineSafetyAPIServerStatusCheckPeriod metav1.Duration

	// APIserverInactiveStartTime to keep track of the
	// start time of when the APIServers were not reachable
	APIserverInactiveStartTime time.Time
	// MachineControllerFrozen indicates if the machine controller
	// is frozen due to Unreachable APIServers
	MachineControllerFrozen bool
}
</code></pre>
<h2 id="controller-structs"><a class="header" href="#controller-structs">Controller Structs</a></h2>
<h3 id="machine-controller-core-struct"><a class="header" href="#machine-controller-core-struct">Machine Controller Core Struct</a></h3>
<p><code>controller</code> struct in package <code>controller</code> inside go file: <code>machine-controller-manager/pkg/util/provider/machinecontroller.go</code> (Bad convention) is the concrete Machine Controller struct that holds state data for the MC and implements the classifical controller <code>Run(workers int, stopCh &lt;-chan struct{})</code> method.</p>
<p>The top level <code>MCServer.Run</code> method initializes this controller struct and calls its <code>Run</code> method</p>
<pre><code class="language-go">package controller
type controller struct {
	namespace                     string // control clustern namespace
	nodeConditions                string // Default: &quot;KernelDeadlock,ReadonlyFilesystem,DiskPressure,NetworkUnavailable&quot;

	controlMachineClient    machineapi.MachineV1alpha1Interface
	controlCoreClient       kubernetes.Interface
	targetCoreClient        kubernetes.Interface
	targetKubernetesVersion *semver.Version

	recorder                record.EventRecorder
	safetyOptions           options.SafetyOptions
	internalExternalScheme  *runtime.Scheme
	driver                  driver.Driver
	volumeAttachmentHandler *drain.VolumeAttachmentHandler
	// permitGiver store two things:
	// - mutex per machinedeployment
	// - lastAcquire time
	// it is used to limit removal of `health timed out` machines
	permitGiver permits.PermitGiver

	// listers
	pvcLister               corelisters.PersistentVolumeClaimLister
	pvLister                corelisters.PersistentVolumeLister
	secretLister            corelisters.SecretLister
	nodeLister              corelisters.NodeLister
	pdbV1beta1Lister        policyv1beta1listers.PodDisruptionBudgetLister
	pdbV1Lister             policyv1listers.PodDisruptionBudgetLister
	volumeAttachementLister storagelisters.VolumeAttachmentLister
	machineClassLister      machinelisters.MachineClassLister
	machineLister           machinelisters.MachineLister
	// queues
    // secretQueue = workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;secret&quot;),
	secretQueue                 workqueue.RateLimitingInterface
	nodeQueue                   workqueue.RateLimitingInterface
	machineClassQueue           workqueue.RateLimitingInterface
	machineQueue                workqueue.RateLimitingInterface
	machineSafetyOrphanVMsQueue workqueue.RateLimitingInterface
	machineSafetyAPIServerQueue workqueue.RateLimitingInterface
	// syncs
	pvcSynced               cache.InformerSynced
	pvSynced                cache.InformerSynced
	secretSynced            cache.InformerSynced
	pdbV1Synced             cache.InformerSynced
	volumeAttachementSynced cache.InformerSynced
	nodeSynced              cache.InformerSynced
	machineClassSynced      cache.InformerSynced
	machineSynced           cache.InformerSynced
}

</code></pre>
<h2 id="driver"><a class="header" href="#driver">Driver</a></h2>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/driver#Driver">github.com/gardener/machine-controller-manager/pkg/util/provider/driver.Driver</a>  is the abstraction facade that decouplesthe machine controller from the cloud-provider specific machine lifecycle details. The MC invokes driver methods while performing reconciliation.</p>
<pre><code class="language-go">type Driver interface {
	CreateMachine(context.Context, *CreateMachineRequest) (*CreateMachineResponse, error)
	DeleteMachine(context.Context, *DeleteMachineRequest) (*DeleteMachineResponse, error)
	GetMachineStatus(context.Context, *GetMachineStatusRequest) (*GetMachineStatusResponse, error)
	ListMachines(context.Context, *ListMachinesRequest) (*ListMachinesResponse, error)
	GetVolumeIDs(context.Context, *GetVolumeIDsRequest) (*GetVolumeIDsResponse, error)
	GenerateMachineClassForMigration(context.Context, *GenerateMachineClassForMigrationRequest) (*GenerateMachineClassForMigrationResponse, error)
}
</code></pre>
<ul>
<li><code>GetVolumeIDs</code> returns a list volumeIDs for the given list of <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolumeSpec">PVSpecs</a>
<ul>
<li>Example: the AWS driver checks if <code>spec.AWSElasticBlockStore.VolumeID</code> is not nil and coverts the k8s <code>spec.AWSElasticBlockStore.VolumeID</code> to the EBS volume ID. Or if storage is provided by CSI <code>spec.CSI.Driver=&quot;ebs.csi.aws.com&quot;</code> just gets <code>spec.CSI.VolumeHandle</code></li>
</ul>
</li>
<li><code>GetMachineStatus</code> gets the status of the VM backing the machine object on the provider</li>
</ul>
<h2 id="codes-and-error-status"><a class="header" href="#codes-and-error-status">Codes and (error) Status</a></h2>
<h3 id="code"><a class="header" href="#code">Code</a></h3>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/machinecodes/codes#Code">github.com/gardener/machine-controller-manager/pkg/util/provider/machinecodes/codes.Code</a> is a <code>uint32</code> with following error codes. These error codes are contained in errors returned from Driver methods.</p>
<p>Note: Un-happy with current design. It is clear that some error codes overlap each other in the sense that they are supersets of other codes. The right thing to do would have been to make an ErrorCategory.</p>
<div class="table-wrapper"><table><thead><tr><th>Value</th><th>Code</th><th>Description</th></tr></thead><tbody>
<tr><td>0</td><td>Ok</td><td>Success</td></tr>
<tr><td>1</td><td>Canceled</td><td>the operation was canceled (by caller)</td></tr>
<tr><td>2</td><td>Unknown</td><td>Unknown error (unrecognized code)</td></tr>
<tr><td>3</td><td>InvalidArgument</td><td>InvalidArgument indicates client specified an invalid argument.</td></tr>
<tr><td>4</td><td>DeadlineExceeded</td><td>DeadlineExceeded means operation expired before completion.  For operations that change the state of the system, this error may be  returned even if the operation has completed successfully. For example, a successful response from a server could have been delayed long enough for the deadline to expire.</td></tr>
<tr><td>5</td><td>NotFound</td><td>requested entity not found.</td></tr>
<tr><td>6</td><td>AlreadyExists</td><td>an attempt to create an entity failed because one already exists.</td></tr>
<tr><td>7</td><td>PermissionDenied</td><td>caller does not have permission to execute the specified operation.</td></tr>
<tr><td>8</td><td>ResourceExhausted</td><td>indicates some resource has been exhausted, perhaps a per-user quota, or perhaps the entire file system is out of space.</td></tr>
<tr><td>9</td><td>FailedPrecondition</td><td>operation was rejected because the	 system is not in a state required for the operation's execution.</td></tr>
<tr><td>10</td><td>Aborted</td><td>operation was aborted and client should retry the full process</td></tr>
<tr><td>11</td><td>OutOfRange</td><td>operation was attempted past the valid range. Unlike InvalidArgument, this error indicates a problem that may be fixed if the system state changes.</td></tr>
<tr><td>12</td><td>UnImplemented</td><td>operation is not implemented or not supported</td></tr>
<tr><td>13</td><td>Internal</td><td>BAD. Some internal invariant broken.</td></tr>
<tr><td>14</td><td>Unavailable</td><td>Service is currently unavailable (transient and op may be tried with backoff)</td></tr>
<tr><td>15</td><td>DataLoss</td><td>unrecoverable data loss or corruption.</td></tr>
<tr><td>16</td><td>Unauthenticated</td><td>request does not have valid creds for operation. Note: It would have been nice if 7 was called Unauthorized.</td></tr>
</tbody></table>
</div>
<h3 id="status"><a class="header" href="#status">Status</a></h3>
<p>(OPINION: I beleive this is a minor NIH design defect. Ideally one should have re-levaraged the k8s API machinery Status <a href="https://pkg.go.dev/k8s.io/apimachinery@v0.25.2/pkg/apis/meta/v1#Status">k8s.io/apimachinery/pkg/apis/meta/v1.Status</a>
instead of making custom status object.)</p>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/machinecodes/status">status</a> implements errors returned by MachineAPIs. MachineAPIs service handlers should return an error created by this package, and machineAPIs clients should expect a corresponding error to be returned from the RPC call.</p>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/machinecodes/status#Status">status.Status</a> implements <code>error</code> and encapsulates a <code>code</code> which should be onf the codes in <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/machinecodes/codes#Code">codes.Code</a> and a develoer-facing error message in English</p>
<pre><code class="language-go">type Status struct {
	code int32
	message string
}
// New returns a Status encapsulating code and msg.
func New(code codes.Code, msg string) *Status {
	return &amp;Status{code: int32(code), message: msg}
}
</code></pre>
<p>NOTE: No ideally why we are doing such hard work as shown in <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecodes/status/status.go#L84">status.FromError</a> which involves time-consuming regex parsing of an error string into a status. This is actually being used to parse error strings of errors returned by Driver methods. Not good - should be better designed.</p>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller/index.html#machine-controller">Machine Controller</a>
<ul>
<li><a href="machine-controller/index.html#mc-launch">MC Launch</a>
<ul>
<li><a href="machine-controller/index.html#dev">Dev</a>
<ul>
<li><a href="machine-controller/index.html#build">Build</a></li>
<li><a href="machine-controller/index.html#launch">Launch</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#prod">Prod</a>
<ul>
<li><a href="machine-controller/index.html#build-1">Build</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#launch-flow">Launch Flow</a>
<ul>
<li><a href="machine-controller/index.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="machine-controller/index.html#machine-controller-loop">Machine Controller Loop</a>
<ul>
<li><a href="machine-controller/index.html#apprun">app.Run</a>
<ul>
<li><a href="machine-controller/index.html#summary-1">Summary</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#appstartcontrollers">app.StartControllers</a></li>
<li><a href="machine-controller/index.html#machine-controller-initialization">Machine Controller Initialization</a>
<ul>
<li><a href="machine-controller/index.html#1-newcontroller-factory-func">1. NewController factory func</a></li>
<li><a href="machine-controller/index.html#11-init-controller-struct">1.1 Init Controller Struct</a></li>
<li><a href="machine-controller/index.html#12-assign-listers-and-hassynced-funcs-to-controller-struct">1.2 Assign Listers and HasSynced funcs to controller struct</a></li>
<li><a href="machine-controller/index.html#13-register-controller-event-handlers-on-informers">1.3 Register Controller Event Handlers on Informers.</a>
<ul>
<li><a href="machine-controller/index.html#131-secret-informer-callback">1.3.1 Secret Informer Callback</a></li>
<li><a href="machine-controller/index.html#132-machine-class-informer-callbacks">1.3.2 Machine Class Informer Callbacks</a>
<ul>
<li><a href="machine-controller/index.html#machineclass-adddelete-callback-1">MachineClass Add/Delete Callback 1</a></li>
<li><a href="machine-controller/index.html#machineclass-update-callback-1">MachineClass Update Callback 1</a></li>
<li><a href="machine-controller/index.html#machineclass-adddeleteupdate-callback-2">MachineClass Add/Delete/Update Callback 2</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#132-machine-informer-callbacks">1.3.2 Machine Informer Callbacks</a>
<ul>
<li><a href="machine-controller/index.html#machine-addupdatedelete-callbacks-1">Machine Add/Update/Delete Callbacks 1</a></li>
<li><a href="machine-controller/index.html#machine-updatedelete-callbacks-2">Machine Update/Delete Callbacks 2</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#133-node-informer-callbacks">1.3.3 Node Informer Callbacks</a>
<ul>
<li><a href="machine-controller/index.html#node-add-callback">Node Add Callback</a></li>
<li><a href="machine-controller/index.html#node-delete-callback">Node Delete Callback</a></li>
<li><a href="machine-controller/index.html#node-update-callback">Node Update Callback</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="machine-controller/index.html#machine-controller-run">Machine Controller Run</a>
<ul>
<li><a href="machine-controller/index.html#1-wait-for-informer-caches-to-sync">1. Wait for Informer Caches to Sync</a></li>
<li><a href="machine-controller/index.html#2-register-on-prometheus">2. Register On Prometheus</a>
<ul>
<li><a href="machine-controller/index.html#21-describe-metrics-controllerdescribe">2.1 Describe Metrics (controller.Describe)</a></li>
<li><a href="machine-controller/index.html#21-collect-metrics-controllercollect">2.1 Collect Metrics (controller.Collect)</a>
<ul>
<li><a href="machine-controller/index.html#211-collect-machine-metrics">2.1.1 Collect Machine Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="machine-controller/index.html#3-create-controller-worker-go-routines-specifying-reconcile-functions">3. Create controller worker go-routines specifying reconcile functions</a>
<ul>
<li><a href="machine-controller/index.html#31-workerworker">3.1 worker.worker</a></li>
</ul>
</li>
<li><a href="machine-controller/index.html#4-reconciliation-functions-executed-by-worker">4. Reconciliation functions executed by worker</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="machine-controller"><a class="header" href="#machine-controller">Machine Controller</a></h1>
<p>The <a href="machine-controller/">Machine Controller</a> handles reconciliation of <a href="machine-controller/./../mcm_facilities.html#machine">Machine</a> and <a href="machine-controller/./../mcm_facilities.html#machineclass">MachineClass</a> objects. </p>
<p>The Machine Controller Entry Point for any provider is at 
<code>machine-controller-manager-provider-&lt;name&gt;/cmd/machine-controller/main.go</code></p>
<h2 id="mc-launch"><a class="header" href="#mc-launch">MC Launch</a></h2>
<h3 id="dev"><a class="header" href="#dev">Dev</a></h3>
<h4 id="build"><a class="header" href="#build">Build</a></h4>
<p>A <code>Makefile</code> in the root of <code>machine-controller-manager-provider-&lt;name&gt;</code> builds the provider specific machine controller  for linux with CGO enabled. The <code>make build</code> target invokes the shell script <a href="https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/.ci/build">.ci/build</a> to do this.</p>
<pre><code>CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
  -a \
  -v \
  -o ${BINARY_PATH}/rel/machine-controller \
  cmd/machine-controller/main.goo
</code></pre>
<h4 id="launch"><a class="header" href="#launch">Launch</a></h4>
<p>Assuming one has initialized the variables using <code>make download-kubeconfigs</code>, one can then use <code>make start</code> target which launches the MC with flags as shown below.
Most of these timeout flags are redundant since exact same values are 
given in <a href="machine-controller/">machine-controller-manager/pkg/util/provider/app/options.NewMCServer</a></p>
<pre><code>go run -mod=vendor 
    cmd/machine-controller/main.go 
    --control-kubeconfig=$(CONTROL_KUBECONFIG) 
    --target-kubeconfig=$(TARGET_KUBECONFIG) 
    --namespace=$(CONTROL_NAMESPACE) 
    --machine-creation-timeout=20m 
    --machine-drain-timeout=5m 
    --machine-health-timeout=10m 
    --machine-pv-detach-timeout=2m 
    --machine-safety-apiserver-statuscheck-timeout=30s 
    --machine-safety-apiserver-statuscheck-period=1m 
    --machine-safety-orphan-vms-period=30m 
    --leader-elect=$(LEADER_ELECT) 
    --v=3
</code></pre>
<h3 id="prod"><a class="header" href="#prod">Prod</a></h3>
<h4 id="build-1"><a class="header" href="#build-1">Build</a></h4>
<p>A <code>Dockerfile</code> builds the provider specific machine controller and launches it directly with no CLI arguments. Hence uses coded defaults</p>
<pre><code class="language-Dockerfile">RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH \
      go build \
      -mod=vendor \
      -o /usr/local/bin/machine-controller \
      cmd/machine-controller/main.go
COPY --from=builder /usr/local/bin/machine-controller /machine-controller
ENTRYPOINT [&quot;/machine-controller&quot;]
</code></pre>
<p>The <code>machine-controller-manager</code> deployment usually launches both the MC in a Pod with following arguments</p>
<pre><code>./machine-controller
         --control-kubeconfig=inClusterConfig
         --machine-creation-timeout=20m
         --machine-drain-timeout=2h
         --machine-health-timeout=10m
         --namespace=shoot--i034796--tre
         --port=10259
         --target-kubeconfig=/var/run/secrets/gardener.cloud/shoot/generic-kubeconfig/kubeconfig
         --v=3
</code></pre>
<h3 id="launch-flow"><a class="header" href="#launch-flow">Launch Flow</a></h3>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TB

Begin((&quot;cmd/
machine-controller/
main.go&quot;))
--&gt;NewMCServer[&quot;mc=options.NewMCServer&quot;]
--&gt;AddFlaogs[&quot;mc.AddFlags(pflag.CommandLine)&quot;]
--&gt;LogOptions[&quot;options := k8s.io/component/base/logs.NewOptions()
	options.AddFlags(pflag.CommandLine)&quot;]
--&gt;InitFlags[&quot;flag.InitFlags&quot;]
InitFlags--local--&gt;NewLocalDriver[&quot;
	driver, err := local.NewDriver(s.ControlKubeconfig)
	if err exit
&quot;]
InitFlags--aws--&gt;NewPlatformDriver[&quot;
	driver := aws.NewAWSDriver(&amp;spi.PluginSPIImpl{}))
	OR
	driver := cp.NewAzureDriver(&amp;spi.PluginSPIImpl{})
	//etc
&quot;]

NewLocalDriver--&gt;AppRun[&quot;
	err := app.Run(mc, driver)
&quot;]
NewPlatformDriver--&gt;AppRun
AppRun--&gt;End((&quot;if err != nil 
os.Exit(1)&quot;))
</pre>
<h4 id="summary"><a class="header" href="#summary">Summary</a></h4>
<ol>
<li>
<p>Creates <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/app/options#MCServer">machine-controller-manager/pkg/util/provider/app/options.MCServer</a> using <code>options.NewMCServer</code> which is the main context object for the machinecontroller that embeds a
<a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/options#MachineControllerManagerConfiguration">options.MachineControllerConfiguration</a>.</p>
<p><code>options.NewMCServer</code> initializes <code>options.MCServer</code> struct with default values for </p>
<ul>
<li><code>Port: 10258</code>, </li>
<li><code>Namespace: default</code>, </li>
<li><code>ConcurrentNodeSyncs: 50</code>: number of worker go-routines that are used to process items from a work queue. See <a href="machine-controller/index.html#31-createworker">Worker</a> below</li>
<li><code>NodeConditions: &quot;KernelDeadLock,ReadonlyFilesystem,DiskPressure,NetworkUnavailable&quot;</code> (failure node conditions that indicate that a machine is un-healthy)</li>
<li><code>MinResyncPeriod: 12 hours</code>, <code>KubeAPIQPS: 20</code>, <code>KubeAPIBurst:30</code>: config params for k8s clients. See <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.3/rest#Config">rest.Config</a></li>
</ul>
</li>
<li>
<p>calls <code>MCServer.AddFlags</code> which defines all parsing flags for the machine controller into fields of <code>MCServer</code> instance created in the last step.</p>
</li>
<li>
<p>calls <code>k8s.io/component-base/logs.NewOptions</code> and then <code>options.AddFlags</code> for logging options. 
TODO: Should get rid of this when moving to <code>logr</code>.) </p>
<ul>
<li>See <a href="https://github.com/gardener/gardener/blob/master/docs/development/logging.md">Logging In Gardener Components</a>. </li>
<li>Then use the <a href="https://github.com/gardener/gardener/tree/master/hack/tools/logcheck">logcheck</a>tool.</li>
</ul>
</li>
<li>
<p>Driver initialization code varies according to the provider type.</p>
<ul>
<li>Local Driver
<ul>
<li>calls <code>NewDriver</code> with control kube config that creates a controller runtime client (<code>sigs.k8s.io/controller-runtime/pkg/client</code>) which then calls <code>pkg/local/driver.NewDriver</code> passing the controlloer-runtime client which constructs a <code>localdriver</code> encapsulating the passed in client.</li>
<li><code>driver := local.NewDriver(c)</code></li>
<li>the <code>localdriver</code> implements <a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go#l28">Driver</a> is the facade for creation/deletion of vm's</li>
</ul>
</li>
<li>Provider Specific Driver Example
<ul>
<li><code>driver := aws.NewAWSDriver(&amp;spi.PluginSPIImpl{})</code></li>
<li><code>driver := cp.NewAzureDriver(&amp;spi.PluginSPIImpl{})</code></li>
<li><code>spi.PluginSPIImpl</code> is a struct that implements a provider specific interface that initializes a provider session.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>calls <a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/app/app.go#l77">app.Run</a> passing in the previously created <code>MCServer</code> and <code>Driver</code> instances.</p>
</li>
</ol>
<h2 id="machine-controller-loop"><a class="header" href="#machine-controller-loop">Machine Controller Loop</a></h2>
<h3 id="apprun"><a class="header" href="#apprun">app.Run</a></h3>
<p><code>app.Run</code> is the function that setups the main control loop of the machine controller server. </p>
<h4 id="summary-1"><a class="header" href="#summary-1">Summary</a></h4>
<ol>
<li><a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/app/app.go#L77">app.Run(options *options.MCServer, driver driver.Driver)</a> is the common run loop for all provider Machine Controllers.</li>
<li>Creates <code>targetkubeconfig</code> and <code>controlkubeconfig</code> of type <code>k8s.io/client-go/rest.Config</code> from the target kube config path using <code>clientcmd.BuildConfigFromFlags</code></li>
<li>Set fields such as <code>config.QPS</code> and <code>config.Burst</code>  in both <code>targetkubeconfig</code> and <code>controlkubeconfig</code> from the passed in <code>options.MCServer</code></li>
<li>Create <code>kubeClientControl</code> from the <code>controlkubeconfig</code> using the standard client-go client factory metohd: <code>kubernetes.NewForConfig</code> that returns a <code>client-go/kubernetes.Clientset</code></li>
<li>Similarly create another <code>Clientset</code> named <code>leaderElectionClient</code> using <code>controlkubeconfig</code></li>
<li>Start a go routine using the function <code>startHTTP</code> that registers a bunch of http handlers for the go profiler, prometheus metrics and the health check.</li>
<li>Call <code>createRecorder</code> passing the <code>kubeClientControl</code> client set instance that returns a <a href="https://github.com/kubernetes/client-go/blob/master/tools/record/event.go#L91">client-go/tools/record.EventRecorder</a>
<ol>
<li>Creates a new <code>eventBroadcaster</code> of type <a href="https://github.com/kubernetes/client-go/blob/master/tools/record/event.go#l113">event.EventBroadcaster</a></li>
<li>Set the logging function of the broadcaster to <code>klog.Infof</code>.</li>
<li>Sets the event sink using <code>eventBroadcaster.StartRecordingToSink</code> passing the event interface as <code>kubeClient.CoreV1().RESTClient()).Events(&quot;&quot;)</code>. Effectively events will be published remotely.</li>
<li>Returns the <code>record.EventRecorder</code> associated with the <code>eventBroadcaster</code> using <code>eventBroadcaster.NewRecorder</code> </li>
</ol>
</li>
<li>Constructs an anonymous function assigned to <code>run</code> variable which does the following:
<ol>
<li>Initializes a <code>stop</code> receive channel.</li>
<li>Creates a <code>controlMachineClientBuilder</code> using <code>machineclientbuilder.SimpleClientBuilder</code> using the <code>controlkubeconfig</code>.</li>
<li>Creates a <code>controlCoreClientBuidler</code> using <code>coreclientbuilder.SimpleControllerClientBuilder</code> wrapping <code>controlkubeconfig</code>.</li>
<li>Creates <code>targetCoreClientBuilder</code> using <code>coreclientbuilder.SimpleControllerClientBuilder</code> wrapping <code>controlkubeconfig</code>.</li>
<li>Call the <code>app.StartControllers</code> function passing the <code>options</code>, <code>driver</code>, <code>controlkubeconfig</code>, <code>targetkubeconfig</code>, <code>controlMachineClientBuilder</code>, <code>controlCoreClientBuilder</code>, <code>targetCoreClientBuilder</code>, <code>recorder</code> and <code>stop</code> channel.
<ul>
<li>// Q: if you are going to pass the controlkubeconfig and targetkubeconfig - why not create the client builders inside the startcontrollers ?</li>
</ul>
</li>
<li>if <code>app.StartcOntrollers</code> return an error panic and exit <code>run</code>.</li>
</ol>
</li>
<li>use <a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/app/app.go#L186">leaderelection.RunOrDie</a> to start a leader election and pass the previously created <code>run</code> function to as the callback for <code>OnStartedLeading</code>. <code>OnStartedLeading</code> callback is invoked when a leaderelector client starts leading.</li>
</ol>
<h3 id="appstartcontrollers"><a class="header" href="#appstartcontrollers">app.StartControllers</a></h3>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/app/app.go#L202">app.StartControllers</a> starts all controller loops which are part of the machine controller. </p>
<pre><code class="language-go">func StartControllers(options *options.MCServer,
	controlCoreKubeconfig *rest.Config,
	targetCoreKubeconfig *rest.Config,
	controlMachineClientBuilder machineclientbuilder.ClientBuilder,
	controlCoreClientBuilder coreclientbuilder.ClientBuilder,
	targetCoreClientBuilder coreclientbuilder.ClientBuilder,
	driver driver.Driver,
	recorder record.EventRecorder,
	stop &lt;-chan struct{}) error
</code></pre>
<ol>
<li>Calls <code>getAvailableResources</code> using the <code>controlCoreClientBuilder</code> that returns a <code>map[schema.GroupVersionResource]bool</code> assigned to <code>availableresources</code>
<ul>
<li><code>getAvailableResources</code> waits till the api server is running by checking its <code>/healthz</code> using <code>wait.PollImmediate</code>. keeps re-creating the client using <code>clientbuilder.Client</code> method. </li>
<li>then uses <code>client.Discovery().ServerResources</code> which returns returns the supported resources for all groups and versions as a slice of <a href="https://github.com/kubernetes/apimachinery/blob/373a5f752d44989b9829888460844849878e1b6e/pkg/apis/meta/v1/types.go#L1131">*metav1.APIResourceList</a> (which encapsulates a <a href="https://github.com/kubernetes/apimachinery/blob/v0.26.1/pkg/apis/meta/v1/types.go#L1081">[]APIResource</a>) and then converts that to a <code>map[schema.GroupVersionResource]bool</code> `</li>
</ul>
</li>
<li>Creates a <code>controlMachineClient</code> using <code>controlMachineClientBuilder.ClientOrDie(&quot;machine-controller&quot;).MachineV1alpha1()</code> which is a client of type <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/clientset/versioned/typed/machine/v1alpha1#MachineV1alpha1Interface">MachineV1alpha1Interface</a>. This interface is a composition of <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/clientset/versioned/typed/machine/v1alpha1#MachinesGetter">MachineGetter</a>,<a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/clientset/versioned/typed/machine/v1alpha1#MachineClassesGetter">MachineClassesGetter</a>, <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/clientset/versioned/typed/machine/v1alpha1#MachineDeploymentsGetter">MachineDeploymentsGetter</a> and <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/clientset/versioned/typed/machine/v1alpha1#MachineSetsGetter">MachineSetsGetter</a> allowing access to CRUD interface for machines, machine classes, machine deployments and machine sets. This client targets the control cluster - ie the cluster holding the machine crd's.</li>
<li>creates a <code>controlCoreClient</code> (of type: <a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/kubernetes#Clientset">kubernetes.Clientset</a> which is the standard k8s client-go client for accessing the k8s control cluster.</li>
<li>creates a <code>targetCoreClient</code> (of type: <a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/kubernetes#Clientset">kubernetes.Clientset</a>) which is the standard k8s client-go client for accessing the target cluster - in which machines will be spawned.</li>
<li>obtain the target cluster k8s version using the discovery interface and preserve it in <code>targetKubernetesVersion</code></li>
<li>if the <code>availableResources</code> does not contain the machine GVR,  exit <code>app.StartControllers</code> with error.</li>
<li>creates the following informer factories:</li>
</ol>
<ul>
<li><code>controlMachineInformerfactory</code> using the generated <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/informers/externalversions.NewFilteredSharedInformerFactory">pkg/client/informers/externalversions#NewFilteredSharedInformerFactory</a> passing the conrol machine client, the configured min resync period and control namespace.</li>
<li>Create <code>controlCoreInformerfactory</code> using the client-go core <a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers#NewFilteredSharedInformerFactory">informers#NewFilteredSharedInformerFactory</a> passing in the control core client, min resync period and control namespace.</li>
<li>Similarly create <code>targetCoreInformerFactory</code></li>
<li>Get the controller's <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/informers/externalversions/machine/v1alpha1#Interface">Machine Informers Facade</a> using <code>controlMachineInformerfactory.Machine().V1alpha1()</code> and assign to <code>machinesharedinformers</code></li>
</ul>
<ol start="8">
<li>Now create the <code>machinecontroller</code> using <a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/machinecontroller/controller.go#L77">machinecontroller.NewController</a> factory function, passing the below:
<ul>
<li>control namespace from <code>options.MCServer.Namespace</code></li>
<li><code>SafetyOptions</code> from <code>options.MCServer.SafetyOptions</code></li>
<li><code>NodeConditions</code> from <code>options.MCserver.NodeConditions</code>. (by default these would be : &quot;KernelDeadlock,ReadonlyFilesystem,DiskPressure,NetworkUnavailable&quot;)</li>
<li>clients: <code>controlMachineClient</code>, <code>controlCoreClient</code>, <code>targetCoreClient</code></li>
<li>the <code>driver</code> </li>
<li>Target Cluster Informers obtained from <code>targetCoreInformerfactory</code>:
<ul>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers/core/v1#PersistentVolumeClaimInformer">PersistentVolumeClaimInformer</a></li>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers/core/v1#PersistentVolumeInformer">PersistentVolumeInformer</a>, </li>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers/storage/v1#VolumeAttachmentInformer">VolumeAttachmentsInformer</a> </li>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers/policy/v1#PodDisruptionBudgetInformer">PodDisruptionBudgetInformer</a></li>
</ul>
</li>
<li>Control Cluster Informers obtained from <code>controlCoreInformerFactory</code> 
<ul>
<li><a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers/core/v1#SecretInformer">SecretInformer</a></li>
</ul>
</li>
<li><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/informers/externalversions/machine/v1alpha1#MachineClassInformer">MachineClassInformer</a>, <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/client/informers/externalversions/machine/v1alpha1#MachineInformer">MachineInformer</a> using <code>machinesharedinformers.MachineClasses()</code> and <code>machinesharedinformers.Machines()</code></li>
<li>The event recorder created earlier</li>
<li><code>targetKubernetesVersion</code></li>
</ul>
</li>
<li>Start <code>controlMachineInformerFactory</code>, <code>controlCoreInformerFactory</code> and <code>targetCoreInformerFactory</code> by calling <a href="https://pkg.go.dev/k8s.io/client-go@v0.26.1/informers#SharedInformerFactory">SharedInformerfactory.Start</a> passing the <code>stop</code> channel. </li>
<li>Launches the <a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/machinecontroller/controller.go#L302">machinecontroller.Run</a> in new go-routine passing the stop channel.</li>
<li>Block forever using a <code>select{}</code></li>
</ol>
<h3 id="machine-controller-initialization"><a class="header" href="#machine-controller-initialization">Machine Controller Initialization</a></h3>
<p>the machine controller is constructed using <a href="https://github.com/gardener/machine-controller-manager/blob/v0.48.0/pkg/util/provider/machinecontroller/controller.go#L77">controller.NewController</a>
factory function which initializes the <code>controller</code> struct.</p>
<h4 id="1-newcontroller-factory-func"><a class="header" href="#1-newcontroller-factory-func">1. NewController factory func</a></h4>
<p>mc is constructed using the factory function below:</p>
<pre><code class="language-go">func NewController(
	namespace string,
	controlMachineClient machineapi.MachineV1alpha1Interface,
	controlCoreClient kubernetes.Interface,
	targetCoreClient kubernetes.Interface,
	driver driver.Driver,
	pvcInformer coreinformers.PersistentVolumeClaimInformer,
	pvInformer coreinformers.PersistentVolumeInformer,
	secretInformer coreinformers.SecretInformer,
	nodeInformer coreinformers.NodeInformer,
	pdbV1beta1Informer policyv1beta1informers.PodDisruptionBudgetInformer,
	pdbV1Informer policyv1informers.PodDisruptionBudgetInformer,
	volumeAttachmentInformer storageinformers.VolumeAttachmentInformer,
	machineClassInformer machineinformers.MachineClassInformer,
	machineInformer machineinformers.MachineInformer,
	recorder record.EventRecorder,
	safetyOptions options.SafetyOptions,
	nodeConditions string,
	bootstrapTokenAuthExtraGroups string,
	targetKubernetesVersion *semver.Version,
) (Controller, error) 

</code></pre>
<h4 id="11-init-controller-struct"><a class="header" href="#11-init-controller-struct">1.1 Init Controller Struct</a></h4>
<p>Create and Initialize the Controller struct initializing rate-limiting work queues for secrets: <code>controller.secretQueue</code>,  nodes: <code>controller.nodeQueue</code>, machines: <code>controller.machineQueue</code>, machineclass: <code>controller.machineClassQueue</code>. Along with 2 work queues used by safety controllers: <code>controller.machineSafetyOrphanVMsQueue</code> and <code>controller.machineSafetyAPIServerQueue</code></p>
<p>Example: </p>
<pre><code class="language-go">controller := &amp;controller {
	//...
 secretQueue:                   workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;secret&quot;),
 machineQueue=workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;machine&quot;),
	//...
}
</code></pre>
<h4 id="12-assign-listers-and-hassynced-funcs-to-controller-struct"><a class="header" href="#12-assign-listers-and-hassynced-funcs-to-controller-struct">1.2 Assign Listers and HasSynced funcs to controller struct</a></h4>
<pre><code class="language-go">	// initialize controller listers from the passed-in shared informers (8 listers)
	controller.pvcLister = pvcInformer
	controller.pvLister = pvinformer.Lister()
    controller.machineLister = machineinformer.lister()

	controller.pdbV1Lister = pdbV1Informer.Lister()
	controller.pdbV1Synced = pdbV1Informer.Informer().HasSynced

	// ...

	// assign the HasSynced function from the passed-in shared informers
	controller.pvcSynced = pvcInformer.Informer().HasSynced
	controller.pvSynced = pvInformer.Informer().HasSynced
    controller.machineSynced = machineInformer.Informer().HasSynced
</code></pre>
<h4 id="13-register-controller-event-handlers-on-informers"><a class="header" href="#13-register-controller-event-handlers-on-informers">1.3 Register Controller Event Handlers on Informers.</a></h4>
<p>An informer invokes registered event handler when a k8s object changes. </p>
<p>Event handlers are registered using <code>&lt;ResourceType&gt;Informer().AddEventhandler</code> function. </p>
<p>The controller initialization registers add//delete event handlers for secrets. add/update/delete event handlers for MachineClass, Machine and Node informers.</p>
<p>The event handlers generally add the object keys to the appropriate work queues which are later picked up and reconciled in processing in <code>controller.Run</code>.</p>
<p>The work queue is used to separate the delivery of the object from its processing. resource event handler functions extract the key of the delivered object and add it to the relevant work queue for future processing. (in <code>controller.Run</code>) </p>
<p>Example</p>
<pre><code class="language-go">secretInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
	AddFunc:    controller.secretAdd,
	DeleteFunc: controller.secretDelete,
})
</code></pre>
<h5 id="131-secret-informer-callback"><a class="header" href="#131-secret-informer-callback">1.3.1 Secret Informer Callback</a></h5>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB

SecretInformerAddCallback
--&gt;SecretAdd[&quot;controller.secretAdd(obj)&quot;]
--&gt;GetSecretKey[&quot;key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)&quot;]
--&gt;AddSecretQ[&quot;if err != nil c.secretQueue.Add(key)&quot;]

SecretInformeDeleteCallback
--&gt;SecretAdd
</pre>
<p>We must check for the <a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#DeletedFinalStateUnknown">DeletedFinalStateUnknown</a> state of that secret in the cache before enqueuing its key. The <code>DeletedFinalStateUnknown</code> state means that the object has been deleted but that the watch deletion event was missed while disconnected from apiserver and the controller didn't react accordingly. Hence if there is no error, we can add the key to the queue.</p>
<h5 id="132-machine-class-informer-callbacks"><a class="header" href="#132-machine-class-informer-callbacks">1.3.2 Machine Class Informer Callbacks</a></h5>
<h6 id="machineclass-adddelete-callback-1"><a class="header" href="#machineclass-adddelete-callback-1">MachineClass Add/Delete Callback 1</a></h6>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
MachineClassInformerAddCallback1
--&gt;
MachineAdd[&quot;controller.machineClassToSecretAdd(obj)&quot;]
--&gt;CastMC[&quot;
	mc, ok := obj.(*v1alpha1.MachineClass)
&quot;]
--&gt;EnqueueSecret[&quot;
	c.secretQueue.Add(mc.SecretRef.Namespace + '/' + 
	mc.SecretRef.Name)
	c.secretQueue.Add(mc.CredentialSecretRef.Namespace + '/' + mc.CredentialSecretRef.Namespace.Name)
&quot;]

MachineClassToSecretDeleteCallback1
--&gt;MachineAdd
</pre>
<h6 id="machineclass-update-callback-1"><a class="header" href="#machineclass-update-callback-1">MachineClass Update Callback 1</a></h6>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
MachineClassInformerUpdateCallback1
--&gt;
MachineAdd[&quot;controller.machineClassToSecretUpdate(oldObj, newObj)&quot;]
--&gt;CastMC[&quot;
	old, ok := oldObj.(*v1alpha1.MachineClass)
	new, ok := newObj.(*v1alpha1.MachineClass)
&quot;]
--&gt;RefNotEqual{&quot;old.SecretRef != 
	new.SecretRef?&quot;}
--Yes--&gt;EnqueueSecret[&quot;
	c.secretQueue.Add(old.SecretRef.Namespace + '/' + old.SecretRef.Name)
	c.secretQueue.Add(new.SecretRef.Namespace + '/' + new.SecretRef.Name)
&quot;]
--&gt;CredRefNotEqual{&quot;old.CredentialsSecretRef!=
new.CredentialsSecretRef?&quot;}
--Yes--&gt;EnqueueCredSecretRef[&quot;
c.secretQueue.Add(old.CredentialsSecretRef.Namespace + '/' + old.CredentialsSecretRef.Name)
c.secretQueue.Add(new.CredentialsSecretRef.Namespace + '/' + new.CredentialsSecretRef.Name)
&quot;]
</pre>
<h6 id="machineclass-adddeleteupdate-callback-2"><a class="header" href="#machineclass-adddeleteupdate-callback-2">MachineClass Add/Delete/Update Callback 2</a></h6>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
MachineClassInformerAddCallback2
--&gt;
MachineAdd[&quot;controller.machineClassAdd(obj)&quot;]
--&gt;CastMC[&quot;
	key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)
	if err != nil c.machineClassQueue.Add(key)
&quot;]
MachineClassInformerDeleteCallback2
--&gt;MachineAdd

MachineClassInformeUpdateCallback2
--&gt;MachineUpdate[&quot;controller.machineClassUpdate(oldObj,obj)&quot;]
--&gt;MachineAdd
</pre>
<h5 id="132-machine-informer-callbacks"><a class="header" href="#132-machine-informer-callbacks">1.3.2 Machine Informer Callbacks</a></h5>
<h6 id="machine-addupdatedelete-callbacks-1"><a class="header" href="#machine-addupdatedelete-callbacks-1">Machine Add/Update/Delete Callbacks 1</a></h6>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
MachineAddCallback1
--&gt;AddMachine[&quot;controller.addMachine(obj)&quot;]
--&gt;EnqueueMachine[&quot;
	key, err := cache.MetaNamespaceKeyFunc(obj)
	//Q: why don't we use DeletionHandlingMetaNamespaceKeyFunc here ?
	if err!=nil c.machineQueue.Add(key)
&quot;]
MachineUpdateCallback1--&gt;AddMachine
MachineDeleteCallback1--&gt;AddMachine
</pre>
<h6 id="machine-updatedelete-callbacks-2"><a class="header" href="#machine-updatedelete-callbacks-2">Machine Update/Delete Callbacks 2</a></h6>
<p>DISCUSS THIS.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
MachineUpdateCallback2
--&gt;UpdateMachineToSafety[&quot;controller.updateMachineToSafety(oldObj, newObj)&quot;]
--&gt;EnqueueSafetyQ[&quot;
	newM := newObj.(*v1alpha1.Machine)
	if multipleVMsBackingMachineFound(newM) {
		c.machineSafetyOrphanVMsQueue.Add('')
	}
&quot;]
MachineDeleteCallback2
--&gt;DeleteMachineToSafety[&quot;deleteMachineToSafety(obj)&quot;]
--&gt;EnqueueSafetyQ1[&quot;
	c.machineSafetyOrphanVMsQueue.Add('')
&quot;]
</pre>
<h5 id="133-node-informer-callbacks"><a class="header" href="#133-node-informer-callbacks">1.3.3 Node Informer Callbacks</a></h5>
<h6 id="node-add-callback"><a class="header" href="#node-add-callback">Node Add Callback</a></h6>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
NodeaAddCallback
--&gt;InvokeAddNodeToMachine[&quot;controller.addNodeToMachine(obj)&quot;]
--&gt;AddNodeToMachine[&quot;
	node := obj.(*corev1.Node)
	if node.ObjectMeta.Annotations has NotManagedByMCM return;
	key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)
	if err != nil return
&quot;]
--&gt;GetMachineFromNode[&quot;
	machine := (use machineLister to get first machine whose 'node' label equals key)
&quot;]
--&gt;ChkMachine{&quot;
machine.Status.CurrentStatus.Phase != 'CrashLoopBackOff'
&amp;&amp;
nodeConditionsHaveChanged(
  machine.Status.Conditions, 
  node.Status.Conditions) ?
&quot;}
--Yes--&gt;EnqueueMachine[&quot;
	mKey, err := cache.MetaNamespaceKeyFunc(obj)
	if err != nil return
	controller.machineQueue.Add(mKey)
&quot;]

</pre>
<h6 id="node-delete-callback"><a class="header" href="#node-delete-callback">Node Delete Callback</a></h6>
<p>This is straightforward - it checks that the node has an associated machine and if so, enqueues the machine on the <code>machineQueue</code></p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TB
NodeDeleteCallback
--&gt;InvokeDeleteNodeToMachine[&quot;controller.deleteNodeToMachine(obj)&quot;]
--&gt;DeleteNodeToMachine[&quot;
	key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)
	if err != nil return
&quot;]
--&gt;GetMachineFromNode[&quot;
	machine := (use machineLister to get first machine whose 'node' label equals key)
	if err != nil return
&quot;]
--&gt;EnqueueMachine[&quot;
	mKey, err := cache.MetaNamespaceKeyFunc(obj)
	if err != nil return
	controller.machineQueue.Add(mKey)
&quot;]
</pre>
<h6 id="node-update-callback"><a class="header" href="#node-update-callback">Node Update Callback</a></h6>
<p><code>controller.updateNodeTomachine</code> is specified as <code>UpdateFunc</code> registered for the <code>nodeInformer</code>. </p>
<p>In a nutshell, it simply delegates to <code>AddNodeTomachine(newobj)</code> described earlier, <em>except</em> if the node has the annotation <code>machineutils.TriggerDeletionByMCM</code> (value: <code>node.machine.sapcloud.io/trigger-deletion-by-mcm</code>).  In this case it gets the <code>machine</code> obj corresponding to the node and then leverages <code>controller.controlMachineClient</code> to delete the machine object.</p>
<p>NOTE:  This annotation was introduced for the user to add on the node. This gives them an indirect way to delete the machine object because they don‚Äôt have access to control plane.</p>
<p>Snippet shown below with error handling+logging omitted.</p>
<pre><code class="language-go">func (c *controller) updateNodeToMachine(oldobj, newobj interface{}) {
	node := newobj.(*corev1.node)
	// check for the triggerdeletionbymcm annotation on the node object
	// if it is present then mark the machine object for deletion
	if value, ok := node.annotations[machineutils.TriggerDeletionByMCM]; ok &amp;&amp; value == &quot;true&quot; {
		machine, err := c.getMachineFromnOde(node.name)
		if machine.deletiontimestamp == nil {
			c.controlmachineclient
			.Machines(c.namespace)
			.Delete(context.Background(), machine.Name, metav1.Deleteoptions{});		
		} 
	}  else {
		c.addnodeToMachine(newobj)
	}
}
</code></pre>
<h3 id="machine-controller-run"><a class="header" href="#machine-controller-run">Machine Controller Run</a></h3>
<pre><code class="language-go">func (c *controller) Run(workers int, stopch &lt;-chan struct{}) {
	// ...
}

</code></pre>
<h4 id="1-wait-for-informer-caches-to-sync"><a class="header" href="#1-wait-for-informer-caches-to-sync">1. Wait for Informer Caches to Sync</a></h4>
<p>When an informer starts, it will build a cache of all resources it currently watches which is lost when the application
restarts. This means that on startup, each of your handler functions will be invoked as the initial state is built. If this
is not desirable, one should wait until the caches are synced before performing any updates. This can be done using the
<a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#WaitForCacheSync">cache.WaitForCacheSync</a> function.</p>
<pre><code class="language-go">if !cache.WaitForCacheSync(stopCh, c.secretSynced, c.pvcSynced, c.pvSynced, c.volumeAttachementSynced, c.nodeSynced, c.machineClassSynced, c.machineSynced) {
	runtimeutil.HandleError(fmt.Errorf(&quot;Timed out waiting for caches to sync&quot;))
	return
}

</code></pre>
<h4 id="2-register-on-prometheus"><a class="header" href="#2-register-on-prometheus">2. Register On Prometheus</a></h4>
<p>The Machine controller struct implements the <a href="https://pkg.go.dev/github.com/prometheus/client_golang@v1.13.0/prometheus#Collector">prometheus.Collector</a> interface and can therefore
be then be registered on prometheus metrics registry. </p>
<pre><code class="language-go">prometheus.MustRegister(controller)
</code></pre>
<p>Collectors which are added to the registry will collect metrics to expose them via the metrics endpoint of the MCM every time when the endpoint is called.</p>
<h5 id="21-describe-metrics-controllerdescribe"><a class="header" href="#21-describe-metrics-controllerdescribe">2.1 Describe Metrics (controller.Describe)</a></h5>
<p>All <a href="https://pkg.go.dev/github.com/prometheus/client_golang@v1.13.0/prometheus#Metric">promethueus.Metric</a> that are collected must first be described using a <a href="https://pkg.go.dev/github.com/prometheus/client_golang@v1.13.0/prometheus#Desc">prometheus.Desc</a> which is the immutable  <em>meta-data</em> about a metric.</p>
<p>As can be seen below the machine controller sends a description of <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/util/provider/metrics#pkg-variables">metrics.MachineCountDesc</a> to prometheus. this is <code>mcm_machine_items_total</code> which is the count of machines managed by controller. </p>
<p>This <code>Describe</code> callback is called by <code>prometheus.MustRegister</code></p>
<p>Doubt: we currently appear to have  only have one metric for the mc ?</p>
<pre><code class="language-go">var	MachineCountDesc = prometheus.NewDesc(&quot;mcm_machine_items_total&quot;, &quot;Count of machines currently managed by the mcm.&quot;, nil, nil)

func (c *controller) Describe(ch chan&lt;- *prometheus.desc) {
	ch &lt;- metrics.MachineCountDesc
}
</code></pre>
<h5 id="21-collect-metrics-controllercollect"><a class="header" href="#21-collect-metrics-controllercollect">2.1 Collect Metrics (controller.Collect)</a></h5>
<p><code>Collect</code> is called by the prometheus registry when collecting
metrics. The implementation sends each collected metric via the
provided channel and returns once the last metric has been sent. the
descriptor of each sent metric is one of those returned by <code>Describe</code></p>
<pre><code class="language-go">// Collect is method required to implement the prometheus.Collect interface.
func (c *controller) Collect(ch chan&lt;- prometheus.Metric) {
	c.CollectMachineMetrics(ch)
	c.CollectMachineControllerFrozenStatus(ch)
}
</code></pre>
<h6 id="211-collect-machine-metrics"><a class="header" href="#211-collect-machine-metrics">2.1.1 Collect Machine Metrics</a></h6>
<pre><code class="language-go">func (c *controller) CollectMachineMetrics(ch chan&lt;- prometheus.Metric) 
</code></pre>
<p>A [prometheus.Metric])(https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#Metric) models a sample linking data points together over time. Custom labels (with their own values) can be added to each data point</p>
<p>A <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#Gauge">prometheus.Gauge</a> is a Metric that represents a single numerical value that can arbitrarily go up and down. We use a Gauge for the machine count.</p>
<p>A <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#GaugeVec">prometheus.GaugeVec</a> is a factory for creating a set of gauges all with the same description but which have different data values for the metric labels.</p>
<p>Machine information about a machine managed by MCM is described on Prometheus using a <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#GaugeVec">prometheus.GaugeVec</a> constructed using the factory function <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#NewGaugeVec">prometheus.NewGaugeVec</a>. </p>
<p>A <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#Desc">prometheus.Desc</a> is the descriptor used by every Prometheus Metric. It is essentially the immutable meta-data of a Metric that includes fully qualified name of the metric, the help string and the metric label names.</p>
<p>We have 3 such gauge vecs for machine metrics  and 1 gauge metric for the machine count as seen below. </p>
<p>Q: Discuss Why do we need the 3 gauge vecs ?</p>
<p>Example:</p>
<pre><code class="language-go">
var	MachineCountDesc = prometheus.NewDesc(&quot;mcm_machine_items_total&quot;, &quot;Count of machines currently managed by the mcm.&quot;, nil, nil)

//MachineInfo Information of the Machines currently managed by the mcm.
var MachineInfo prometheus.GaugeVec = prometheus.NewGaugeVec(prometheus.GaugeOpts{
		Namespace: &quot;mcm&quot;,
		Subsystem: &quot;machine&quot;,
		Name:      &quot;info&quot;,
		Help:      &quot;Information of the Machines currently managed by the mcm.&quot;,
	}, []string{&quot;name&quot;, &quot;namespace&quot;, &quot;createdAt&quot;,
		&quot;spec_provider_id&quot;, &quot;spec_class_api_group&quot;, &quot;spec_class_kind&quot;, &quot;spec_class_name&quot;})

// MachineStatusCondition Information of the mcm managed Machines' status conditions
var	MachineStatusCondition = prometheus.NewGaugeVec(prometheus.GaugeOpts{
		Namespace: namespace,
		Subsystem: machineSubsystem,
		Name:      &quot;status_condition&quot;,
		Help:      &quot;Information of the mcm managed Machines' status conditions.&quot;,
	}, []string{&quot;name&quot;, &quot;namespace&quot;, &quot;condition&quot;})

//MachineCSPhase Current status phase of the Machines currently managed by the mcm.
	MachineCSPhase = prometheus.NewGaugeVec(prometheus.GaugeOpts{
		Namespace: namespace,
		Subsystem: machineSubsystem,
		Name:      &quot;current_status_phase&quot;,
		Help:      &quot;Current status phase of the Machines currently managed by the mcm.&quot;,
	}, []string{&quot;name&quot;, &quot;namespace&quot;})
</code></pre>
<p>One invokes the <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#GaugeVec.With">GaugeVec.With</a> method passing a <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#Labels">prometheus.Labels</a> which is a <code>map[string]string</code> to obtain a <a href="https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#Gauge">prometheus.Gauge</a>. </p>
<ol>
<li>Gets the list of machines using the <code>machineLister</code></li>
<li>Iterate through list of machines. Use the <code>MachineInfo.With</code> method to initialize the labels for each metric and obtain the <code>Gauge</code>. Use <code>Gauge.Set</code> to set value for metric.</li>
<li>Create the machine count metric</li>
</ol>
<pre><code class="language-go">	metric, err := prometheus.NewConstMetric(metrics.MachineCountDesc, prometheus.GaugeValue, float64(len(machineList)))
</code></pre>
<ol start="4">
<li>Set the metric value and send the metric to the prometheus metric channel:</li>
</ol>
<pre><code class="language-go">metric, err := prometheus.NewConstMetric(metrics.MachineCountDesc, prometheus.GaugeValue, float64(len(machineList)))
</code></pre>
<h4 id="3-create-controller-worker-go-routines-specifying-reconcile-functions"><a class="header" href="#3-create-controller-worker-go-routines-specifying-reconcile-functions">3. Create controller worker go-routines specifying reconcile functions</a></h4>
<p>Finally use <code>worker.Run</code> to create and runs a worker routine that just processes items in the specified queue. The worker will run until <code>stopCh</code> is closed. The worker go-routine will be added to the wait group when started and marked done when finished.</p>
<p>Q: <code>reconcileClusterNodeKey</code> seems useless ?</p>
<pre><code class="language-go">func (c *controller) Run(workers int, stopch &lt;-chan struct{}) {
	//...
waitGroup sync.WaitGroup
for i := 0; i &lt; workers; i++ {
	worker.Run(c.secretQueue, &quot;ClusterSecret&quot;, worker.DefaultMaxRetries, true, c.reconcileClusterSecretKey, stopCh, &amp;waitGroup)
	worker.Run(c.machineClassQueue, &quot;ClusterMachineClass&quot;, worker.DefaultMaxRetries, true, c.reconcileClusterMachineClassKey, stopCh, &amp;waitGroup)
	worker.Run(c.machineQueue, &quot;ClusterMachine&quot;, worker.DefaultMaxRetries, true, c.reconcileClusterMachineKey, stopCh, &amp;waitGroup)
	worker.Run(c.machineSafetyOrphanVMsQueue, &quot;ClusterMachineSafetyOrphanVMs&quot;, worker.DefaultMaxRetries, true, c.reconcileClusterMachineSafetyOrphanVMs, stopCh, &amp;waitGroup)
	worker.Run(c.machineSafetyAPIServerQueue, &quot;ClusterMachineAPIServer&quot;, worker.DefaultMaxRetries, true, c.reconcileClusterMachineSafetyAPIServer, stopCh, &amp;waitGroup)
}
&lt;-stopch
waitGroup.wait()
}

</code></pre>
<pre><code class="language-go">func Run(queue workqueue.ratelimitinginterface, resourcetype string, maxretries int, forgetaftersuccess bool, reconciler func(key string) error, stopch &lt;-chan struct{}, waitgroup *sync.waitgroup) {
	waitgroup.add(1)
	go func() {
		wait.until(worker(queue, resourcetype, maxretries, forgetaftersuccess, reconciler), time.second, stopch)
		waitgroup.done()
	}()
}
</code></pre>
<h5 id="31-workerworker"><a class="header" href="#31-workerworker">3.1 worker.worker</a></h5>
<p>NOTE: Puzzled that basic routine like this is NOT part of client-go lib. Its likely repeated across thosands of controllers (prob with bugs). Thankfully controller-runtime obviates the need for soemthing like this.</p>
<p><a href="machine-controller/">worker</a> returns a function that </p>
<ol>
<li>de-queues items (keys) from the work <code>queue</code>. the <code>key</code>s that are obtained using work <code>queue.get</code> to be strings of the form <code>namespace/name</code> of the resource. </li>
<li>processes them by invoking the <code>reconciler(key)</code> function 
<ol>
<li>the purpose of the <code>reconciler</code> is to compares the actual state with the desired state, and attempts to converge the two. it should then update the <code>status</code> block of the resource.</li>
<li>if <code>reconciler</code> returns an error, requeue the item up to <code>maxretries</code> before giving up.</li>
</ol>
</li>
<li>marks items as done.</li>
</ol>
<pre><code class="language-go">func worker(queue workqueue.RateLimitingInterface, resourceType string, maxRetries int, forgetAfterSuccess bool, reconciler func(key string) error) func() {
	return func() {
		exit := false
		for !exit {
			exit = func() bool {
				key, quit := queue.Get()
				if quit {
					return true
				}
				defer queue.Done(key)

				err := reconciler(key.(string))
				if err == nil {
					if forgetAfterSuccess {
						queue.Forget(key)
					}
					return false
				}

				if queue.NumRequeues(key) &lt; maxRetries {
					queue.AddRateLimited(key)
					return false
				}

				queue.Forget(key)
				return false
			}()
		}
	}
}

</code></pre>
<h4 id="4-reconciliation-functions-executed-by-worker"><a class="header" href="#4-reconciliation-functions-executed-by-worker">4. Reconciliation functions executed by worker</a></h4>
<p>The controller starts worker go-routines that pop out keys from the relevant workqueue and execute the reconcile function.</p>
<p>See reconcile chapters</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reconcile-cluster-machine-class"><a class="header" href="#reconcile-cluster-machine-class">Reconcile Cluster Machine Class</a></h1>
<h2 id="reconcile-cluster-machine-class-key"><a class="header" href="#reconcile-cluster-machine-class-key">Reconcile Cluster Machine Class Key</a></h2>
<p><code>reconcileClusterMachineClassKey</code> just picks up the machine class key from the machine class queue  and then delegates further. </p>
<pre><code class="language-go">func (c *controller) reconcileClusterMachineClassKey(key string) error
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

GetMCName[&quot;ns,name=cache.SplitMetanamespacekey(mkey)&quot;]
--&gt;GetMC[&quot;
class, err := c.machineClassLister.MachineClasses(c.namespace).Get(name)
if err != nil return err  // basically adds back to the queue after rate limiting
&quot;]
--&gt;RMC[&quot;
ctx := context.Background()
reconcileClusterMachineClass(ctx, class)&quot;]
--&gt;CheckErr{&quot;err !=nil&quot;}
--Yes--&gt;ShortR[&quot;machineClassQueue.AddAfter(key, machineutils.ShortRetry)&quot;]
CheckErr--No--&gt;LongR[&quot;machineClassQueue.AddAfter(key, machineutils.LongRetry)&quot;]

</pre>
<h2 id="reconcile-cluster-machine-class-1"><a class="header" href="#reconcile-cluster-machine-class-1">Reconcile Cluster Machine Class</a></h2>
<pre><code class="language-go">func (c *controller) reconcileClusterMachineClass(ctx context.Context,
 class *v1alpha1.MachineClass) error 
</code></pre>
<p>Bad design: should ideally return the retry period like other reconcile functions.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

FindMachineForClass[&quot;
machines := Use machineLister and 
match on Machine.Spec.Class.Name == class to 
find machines with matching class&quot;]
--&gt;CheckDelTimeStamp{&quot;
// machines are ref
class.DeletionTimestamp == nil
&amp;&amp; len(machines) &gt; 0
&quot;}

CheckDelTimeStamp--Yes--&gt;AddMCFinalizers[&quot;
Add/Update MCM Finalizers to MC 
and use controlMachineClient to update
(why mcm finalizer not mc finalizer?)
'machine.sapcloud.io/machine-controller-manager'
retryPeriod=LongRetry
&quot;]
--&gt;ChkMachineCount{{&quot;len(machines)&gt;0?&quot;}}
--Yes--&gt;EnQMachines[&quot;
iterate machines and invoke:
c.machineQueue.Add(machine)
&quot;]
--&gt;End((&quot;End&quot;))

CheckDelTimeStamp--No--&gt;Shortr[&quot;
// Seems like over-work here.
retryPeriod=ShortRetry
&quot;]--&gt;ChkMachineCount

ChkMachineCount--No--&gt;DelMCFinalizers[&quot;
controller.deleteMachineClassFinalizers(ctx, class)
&quot;]--&gt;End
</pre>
<p>NOTE: Scratch work below. IGNORE.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

a[&quot;ns,name=cache.SplitMetanamespacekey(mkey)&quot;]
getm[&quot;machine=machinelister.machines(ns).get(name)&quot;]
valm[&quot;validation.validatemachine(machine)&quot;]
valmc[&quot;machineclz,secretdata,err=validation.validatemachineclass(machine)&quot;]
longr[&quot;retryperiod=machineutils.longretry&quot;]
shortr[&quot;retryperiod=machineutils.shortretry&quot;]
enqm[&quot;machinequeue.addafter(mkey, retryperiod)&quot;]
checkmdel{&quot;is\nmachine.deletiontimestamp\nset?&quot;}
newdelreq[&quot;req=&amp;driver.deletemachinerequest{machine,machineclz,secretdata}&quot;]
delflow[&quot;retryperiod=controller.triggerdeletionflow(req)&quot;]
createflow[&quot;retryperiod=controller.triggercreationflow(req)&quot;]
hasfin{&quot;hasfinalizer(machine)&quot;}
addfin[&quot;addmachinefinalizers(machine)&quot;]
checkmachinenodeexists{&quot;machine.status.node\nexists?&quot;}
reconcilemachinehealth[&quot;controller.reconcilemachinehealth(machine)&quot;]
syncnodetemplates[&quot;controller.syncnodetemplates(machine)&quot;]
newcreatereq[&quot;req=&amp;driver.createmachinerequest{machine,machineclz,secretdata}&quot;]
z((&quot;end&quot;))

a--&gt;getm
enqm--&gt;z
longr--&gt;enqm
shortr--&gt;enqm
getm--&gt;valm
valm--&gt;ok--&gt;valmc
valm--err--&gt;longr
valmc--err--&gt;longr
valmc--ok--&gt;checkmdel
checkmdel--yes--&gt;newdelreq
checkmdel--no--&gt;hasfin
newdelreq--&gt;delflow
hasfin--no--&gt;addfin
hasfin--yes--&gt;shortr
addfin--&gt;checkmachinenodeexists
checkmachinenodeexists--yes--&gt;reconcilemachinehealth
checkmachinenodeexists--no--&gt;newcreatereq
reconcilemachinehealth--ok--&gt;syncnodetemplates
syncnodetemplates--ok--&gt;longr
syncnodetemplates--err--&gt;shortr
delflow--&gt;enqm
newcreatereq--&gt;createflow
createflow--&gt;enqm

</pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reconcile-cluster-secret"><a class="header" href="#reconcile-cluster-secret">Reconcile Cluster Secret</a></h1>
<p><code>reconcileClusterSecretKey</code> reconciles an secret due to controller resync
or an event on the secret</p>
<pre><code class="language-go">func (c *controller) reconcileClusterSecretKey(key string) error 
// which looks up secret and delegates to
func (c *controller) reconcileClusterSecret(ctx context.Context, secret *corev1.Secret) error 
</code></pre>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>Worker go-routines are created for this as below</p>
<pre><code class="language-go">worker.Run(c.secretQueue, 
    &quot;ClusterSecret&quot;, 
    worker.DefaultMaxRetries, 
    true, 
    c.reconcileClusterSecretKey,
    stopCh,
    &amp;waitGroup)
</code></pre>
<h2 id="flow"><a class="header" href="#flow">Flow</a></h2>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/secret.go#L37">controller.reconcileClusterSecretkey</a>
basically adds the <a href="machine-controller/../mcm_facilities.html#finalizers">MCFinalizerName</a>  (value: <code>machine.sapcloud.io/machine-controller</code>) to the list of finalizers for all secrets that are referenced by machine classes within the same namespace.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%

flowchart TD

a[ns, name = cache.SplitMetaNamespaceKey]
b[&quot;sec=secretLister.Secrets(ns).Get(name)&quot;]
c[&quot;machineClasses=findMachineClassForSecret(name)
// Gets the slice of MachineClasses referring to the passed secret
//iterates through machine classes and 
// checks whether mc.SecretRef.Name or mcCredentialSecretRef.Name 
// matches secret name
&quot;]
d{machineClasses empty?}
e[&quot;controller.addSecretFinalizers(sec)&quot;] 
z((&quot;return err&quot;))
a--&gt;b
b--&gt;c
c--&gt;d
d--Yes--&gt;DeleteFinalizers[&quot;controller.deleteSecretFinalizers&quot;]--&gt;z
e--success--&gt;z
d--No--&gt;e
</pre>
<h3 id="controlleraddsecretfinalizers"><a class="header" href="#controlleraddsecretfinalizers">controller.addSecretFinalizers</a></h3>
<pre><code class="language-go">func (c *controller) addSecretFinalizers(ctx context.Context, secret *corev1.Secret) error {
</code></pre>
<p>Basicaly adds <code>machine.sapcloud.io/machine-controller</code> to the secret and uses <code>controlCoreClient</code> to update the secret.</p>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller/cluster_machine_reconcile.html#cluster-machine-reconciliation">Cluster Machine Reconciliation</a>
<ul>
<li><a href="machine-controller/cluster_machine_reconcile.html#controllertriggercreationflow">controller.triggerCreationFlow</a></li>
<li><a href="machine-controller/cluster_machine_reconcile.html#controllertriggerdeletionflow">controller.triggerDeletionFlow</a></li>
<li><a href="machine-controller/cluster_machine_reconcile.html#controllerreconcilemachinehealth">controller.reconcileMachineHealth</a>
<ul>
<li><a href="machine-controller/cluster_machine_reconcile.html#health-check-flow-diagram">Health Check Flow Diagram</a></li>
<li><a href="machine-controller/cluster_machine_reconcile.html#health-check-summary">Health Check Summary</a></li>
<li><a href="machine-controller/cluster_machine_reconcile.html#health-check-doubts">Health Check Doubts</a></li>
</ul>
</li>
<li><a href="machine-controller/cluster_machine_reconcile.html#controllertriggerupdationflow">controller.triggerUpdationFlow</a></li>
</ul>
</li>
</ul>
<p>While perusing the below, you might need to reference <a href="machine-controller/./mc_helper_funcs.html">Machine Controller Helper Functions</a>  as several reconcile functions delegate to helper methods defined on the machine controller struct.</p>
<h1 id="cluster-machine-reconciliation"><a class="header" href="#cluster-machine-reconciliation">Cluster Machine Reconciliation</a></h1>
<pre><code class="language-go">func (c *controller) reconcileClusterMachineKey(key string) error
</code></pre>
<p>The top-level reconcile function for the machine that analyzes machine status and delegates to the individual reconcile functions for machine-creation, machine-deletion and machine-health-check flows. </p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

A[&quot;ns,name=cache.SplitMetaNamespaceKey(mKey)&quot;]
GetM[&quot;machine=machineLister.Machines(ns).Get(name)&quot;]
ValM[&quot;validation.ValidateMachine(machine)&quot;]
ValMC[&quot;machineClz,secretData,err=validation.ValidateMachineClass(machine)&quot;]
LongR[&quot;retryPeriod=machineutils.LongRetry&quot;]
ShortR[&quot;retryPeriod=machineutils.ShortRetry&quot;]
EnqM[&quot;machineQueue.AddAfter(mKey, retryPeriod)&quot;]
CheckMDel{&quot;Is\nmachine.DeletionTimestamp\nSet?&quot;}
NewDelReq[&quot;req=&amp;driver.DeleteMachineRequest{machine,machineClz,secretData}&quot;]
DelFlow[&quot;retryPeriod=controller.triggerDeletionFlow(req)&quot;]
CreateFlow[&quot;retryPeriod=controller.triggerCreationFlow(req)&quot;]
HasFin{&quot;HasFinalizer(machine)&quot;}
AddFin[&quot;addMachineFinalizers(machine)&quot;]
CheckMachineNodeExists{&quot;machine.Status.Node\nExists?&quot;}
ReconcileMachineHealth[&quot;controller.reconcileMachineHealth(machine)&quot;]
SyncNodeTemplates[&quot;controller.syncNodeTemplates(machine)&quot;]
NewCreateReq[&quot;req=&amp;driver.CreateMachineRequest{machine,machineClz,secretData}&quot;]
Z((&quot;End&quot;))

Begin((&quot; &quot;))--&gt;A
A--&gt;GetM
EnqM--&gt;Z
LongR--&gt;EnqM
ShortR--&gt;EnqM
GetM--&gt;ValM
ValM--Ok--&gt;ValMC
ValM--Err--&gt;LongR
ValMC--Err--&gt;LongR
ValMC--Ok--&gt;CheckMDel
CheckMDel--Yes--&gt;NewDelReq
CheckMDel--No--&gt;HasFin
NewDelReq--&gt;DelFlow
HasFin--No--&gt;AddFin
HasFin--Yes--&gt;ShortR
AddFin--&gt;CheckMachineNodeExists
CheckMachineNodeExists--Yes--&gt;ReconcileMachineHealth
CheckMachineNodeExists--No--&gt;NewCreateReq
ReconcileMachineHealth--Ok--&gt;SyncNodeTemplates
SyncNodeTemplates--Ok--&gt;LongR
SyncNodeTemplates--Err--&gt;ShortR
DelFlow--&gt;EnqM
NewCreateReq--&gt;CreateFlow
CreateFlow--&gt;EnqM

</pre>
<h2 id="controllertriggercreationflow"><a class="header" href="#controllertriggercreationflow">controller.triggerCreationFlow</a></h2>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/machine.go#L326">Controller Method</a> that orchestraes the call to the <a href="machine-controller/../mcm_facilities.html#driver">Driver.CreateMachine</a></p>
<p>This method badly requires to be split into several functions. It is too long. </p>
<pre><code class="language-go">func (c *controller) triggerCreationFlow(ctx context.Context, 
cmr *driver.CreateMachineRequest) 
  (machineutils.RetryPeriod, error) 
</code></pre>
<p>Apologies for HUMONGOUS flow diagram - ideally code should have been split here into small functions.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

ShortP[&quot;retryPeriod=machineutils.ShortRetry&quot;]
MediumP[&quot;retryPeriod=machineutils.MediumRetry&quot;]
Return((&quot;return retryPeriod, err&quot;))


Begin((&quot; &quot;))--&gt;Init[&quot;
  machine     = cmr.Machine
	machineName = cmr.Machine.Name
  secretCopy := cmr.Secret.DeepCopy() //NOTE: seems Un-necessary?
&quot;]
--&gt;AddBootStrapToken[&quot;
 err = c.addBootstrapTokenToUserData(ctx, machine.Name, secretCopy)
//  get/create bootstrap token and populate inside secretCopy['userData']
&quot;]
--&gt;ChkErr{err != nil?}

ChkErr--Yes--&gt;ShortP--&gt;Return

ChkErr--No--&gt;CreateMachineStatusReq[&quot;
  statusReq = &amp; driver.GetMachineStatusRequest{
			Machine:      machine,
			MachineClass: cmr.MachineClass,
			Secret:       cmr.Secret,
		},
&quot;]--&gt;GetMachineStatus[&quot;
  statusResp, err := c.driver.GetMachineStatus(ctx, statusReq)
  //check if VM already exists
&quot;]--&gt;ChkStatusErr{err!=nil}

ChkStatusErr--No--&gt;InitNodeNameFromStatusResp[&quot;
   nodeName = statusResp.NodeName
  providerID = statusResp.ProviderID
&quot;]

ChkStatusErr--Yes--&gt;DecodeErrorStatus[&quot;
  errStatus,decodeOk= status.FromError(err)
&quot;]
DecodeErrorStatus--&gt;CheckDecodeOk{&quot;decodeOk ?&quot;}

CheckDecodeOk--No--&gt;MediumP--&gt;Return
CheckDecodeOk--Yes--&gt;AnalyzeCode{status.Code?}


AnalyzeCode--NotFound,Unimplemented--&gt;ChkNodeLabel{&quot;machine.Labels['node']?&quot;}

ChkNodeLabel--No--&gt;CreateMachine[&quot;
// node label is not present -&gt; no machine
 resp, err := c.driver.CreateMachine(ctx, cmr)
&quot;]--&gt;ChkCreateError{err!=nil?}

ChkNodeLabel--Yes--&gt;InitNodeNameFromMachine[&quot;
  nodeName = machine.Labels['node']
&quot;]


AnalyzeCode--Unknown,DeadlineExceeded,Aborted,Unavailable--&gt;ShortRetry[&quot;
retryPeriod=machineutils.ShortRetry
&quot;]--&gt;GetLastKnownState[&quot;
  lastKnownState := machine.Status.LastKnownState
&quot;]--&gt;InitFailedOp[&quot;
 lastOp := LastOperation{
    Description: err.Error(),
    State: MachineStateFailed,
    Type: MachineOperationCreatea,
    LastUpdateTime: Now(),
 };
 currStatus := CurrentStatus {
    Phase: MachineCrashLoopBackOff || MachineFailed (on create timeout)
    LastUpdateTime: Now()
 }
&quot;]--&gt;UpdateMachineStatus[&quot;
c.machineStatusUpdate(ctx,machine,lastOp,currStatus,lastKnownState)
&quot;]--&gt;Return


ChkCreateError--Yes--&gt;SetLastKnownState[&quot;
  	lastKnownState = resp.LastKnownState
&quot;]--&gt;InitFailedOp

ChkCreateError--No--&gt;InitNodeNameFromCreateResponse[&quot;
  nodeName = resp.NodeName
  providerID = resp.ProviderID
&quot;]--&gt;ChkStaleNode{&quot;
// check stale node
nodeName != machineName 
&amp;&amp; nodeLister.Get(nodeName) exists&quot;}


InitNodeNameFromStatusResp--&gt;ChkNodeLabelAnnotPresent{&quot;
cmr.Machine.Labels['node']
&amp;&amp; cmr.Machine.Annotations[MachinePriority] ?
&quot;}
InitNodeNameFromMachine--&gt;ChkNodeLabelAnnotPresent

ChkNodeLabelAnnotPresent--No--&gt;CloneMachine[&quot;
  clone := machine.DeepCopy;
  clone.Labels['node'] = nodeName
  clone.Annotations[machineutils.MachinePriority] = '3'
  clone.Spec.ProviderID = providerID
&quot;]--&gt;UpdateMachine[&quot;
  _, err := c.controlMachineClient.Machines(clone.Namespace).Update(ctx, clone, UpdateOptions{})
&quot;]--&gt;ShortP




ChkStaleNode--No--&gt;CloneMachine
ChkStaleNode--Yes--&gt;CreateDMR[&quot;
  dmr := &amp;driver.DeleteMachineRequest{
						Machine: &amp;Machine{
							ObjectMeta: machine.ObjectMeta,
							Spec: MachineSpec{
								ProviderID: providerID,
							},
						},
						MachineClass: createMachineRequest.MachineClass,
						Secret:       secretCopy,
					}
&quot;]--&gt;DeleteMachine[&quot;
  _, err := c.driver.DeleteMachine(ctx, deleteMachineRequest)
  // discuss stale node case
  retryPeriod=machineutils.ShortRetry
&quot;]--&gt;InitFailedOp1[&quot;
 lastOp := LastOperation{
    Description: 'VM using old node obj',
    State: MachineStateFailed,
    Type: MachineOperationCreate, //seems wrong
    LastUpdateTime: Now(),
 };
 currStatus := CurrentStatus {
    Phase: MachineFailed (on create timeout)
    LastUpdateTime: Now()
 }
&quot;]--&gt;UpdateMachineStatus

ChkNodeLabelAnnotPresent--Yes--&gt;ChkMachineStatus{&quot;machine.Status.Node != nodeName
  || machine.Status.CurrentStatus.Phase == ''&quot;}

ChkMachineStatus--No--&gt;LongP[&quot;retryPeriod = machineutils.LongRetry&quot;]--&gt;Return

ChkMachineStatus--Yes--&gt;CloneMachine1[&quot;
  clone := machine.DeepCopy()
  clone.Status.Node = nodeName
&quot;]--&gt;SetLastOp[&quot;
 lastOp := LastOperation{
    Description: 'Creating Machine on Provider',
    State: MachineStateProcessing,
    Type: MachineOperationCreate,
    LastUpdateTime: Now(),
 };
 currStatus := CurrentStatus {
    Phase: MachinePending,
    TimeoutActive:  true,
    LastUpdateTime: Now()
 }
 lastKnownState = clone.Status.LastKnownState
&quot;]--&gt;UpdateMachineStatus

style InitFailedOp text-align:left

</pre>
<h2 id="controllertriggerdeletionflow"><a class="header" href="#controllertriggerdeletionflow">controller.triggerDeletionFlow</a></h2>
<pre><code class="language-go">func (c *controller) triggerDeletionFlow(ctx context.Context, dmr *driver.DeleteMachineRequest) (machineutils.RetryPeriod, error) 

</code></pre>
<p>Please note that there is sad use of <code>machine.Status.LastOperation</code>  as semantically the <em>next</em> requested operation. This is confusing. TODO: DIscuss This.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

GM[&quot;machine=dmr.Machine\n
machineClass=dmr.MachineClass\n
secret=dmr.Secret&quot;]
HasFin{&quot;HasFinalizer(machine)&quot;}
LongR[&quot;retryPeriod=machineUtils.LongRetry&quot;]
ShortR[&quot;retryPeriod=machineUtils.ShortRetry&quot;]
ChkMachineTerm{&quot;machine.Status.CurrentStatus.Phase\n==MachineTerminating ?&quot;}
CheckMachineOperation{&quot;Check\nmachine.Status.LastOperation.Description&quot;}
DrainNode[&quot;retryPeriod=c.drainNode(dmr)&quot;]
DeleteVM[&quot;retryPeriod=c.deleteVM(dmr)&quot;]
DeleteNode[&quot;retryPeriod=c.deleteNodeObject(dmr)&quot;]
DeleteMachineFin[&quot;retryPeriod=c.deleteMachineFinalizers(machine)&quot;]
SetMachineTermStatus[&quot;c.setMachineTerminationStatus(dmr)&quot;]

CreateMachineStatusRequest[&quot;statusReq=&amp;driver.GetMachineStatusRequest{machine, machineClass,secret}&quot;]
GetVMStatus[&quot;retryPeriod=c.getVMStatus(statusReq)&quot;]



Z((&quot;End&quot;))

Begin((&quot; &quot;))--&gt;HasFin
HasFin--Yes--&gt;GM
HasFin--No--&gt;LongR
LongR--&gt;Z
GM--&gt;ChkMachineTerm
ChkMachineTerm--No--&gt;SetMachineTermStatus
ChkMachineTerm--Yes--&gt;CheckMachineOperation
SetMachineTermStatus--&gt;ShortR
CheckMachineOperation--GetVMStatus--&gt;CreateMachineStatusRequest
CheckMachineOperation--InitiateDrain--&gt;DrainNode
CheckMachineOperation--InitiateVMDeletion--&gt;DeleteVM
CheckMachineOperation--InitiateNodeDeletion--&gt;DeleteNode
CheckMachineOperation--InitiateFinalizerRemoval--&gt;DeleteMachineFin
CreateMachineStatusRequest--&gt;GetVMStatus
GetVMStatus--&gt;Z


DrainNode--&gt;Z
DeleteVM--&gt;Z
DeleteNode--&gt;Z
DeleteMachineFin--&gt;Z
ShortR--&gt;Z

</pre>
<h2 id="controllerreconcilemachinehealth"><a class="header" href="#controllerreconcilemachinehealth">controller.reconcileMachineHealth</a></h2>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/machine_util.go#L584">controller.reconcileMachineHealth</a> reconciles the machine object with any change in node conditions or VM health.</p>
<pre><code class="language-go">func (c *controller) reconcileMachineHealth(ctx context.Context, machine *Machine) 
  (machineutils.RetryPeriod, error)
</code></pre>
<p>NOTES:</p>
<ol>
<li>Reference <a href="machine-controller/./mc_helper_methods.html#controllerishealthy">controller.isHealth</a> which checks the machine status conditions.</li>
</ol>
<h3 id="health-check-flow-diagram"><a class="header" href="#health-check-flow-diagram">Health Check Flow Diagram</a></h3>
<p>See <a href="https://github.com/gardener/machine-controller-manager/blob/master/docs/FAQ.md#what-are-the-different-phases-of-a-machine">What are the different phases of a Machine</a></p>
<h3 id="health-check-summary"><a class="header" href="#health-check-summary">Health Check Summary</a></h3>
<ol>
<li>Gets the <code>Node</code> obj associated with the machine. If it IS NOT found, yet the current machine phase is <code>Running</code>, change the machine phase to <code>Unknown</code>, the last operation state to <code>Processing</code>, the last operation type to <code>HealthCheck</code>, update the machine status and return with a short retry.</li>
<li>If the <code>Node</code> object IS found, then it checks whether the <code>Machine.Status.Conditions</code> are different from <code>Node.Status.Conditions</code>. If so it sets the machine conditions to the node conditions.</li>
<li>If the machine IS NOT healthy (See <a href="machine-controller/./mc_helper_methods.html#controllerishealthy">isHealthy</a>) but the current machine phase is <code>Running</code>, change the machine phase to <code>Unknown</code>, the last operation state to <code>Processing</code>, the last operation type to <code>HealthCheck</code>, update the machine status and return with a short retry.</li>
<li>If the machine IS healthy but the current machine phase is NOT <code>Running</code> and the machine's node does not have the <code>node.gardener.cloud/critical-components-not-ready</code> taint,  check whether the last operation type was a <code>Create</code>.
<ol>
<li>If the last operation type was a <code>Create</code> and last operation state is NOT marked as <code>Successful</code>, then delete the bootstrap token associated with the machine. Change the last operation state to <code>Successful</code>. Let the last operation type continue to remain as <code>Create</code>.</li>
<li>If the last operation type was NOT a <code>Create</code>, change the last operation type to <code>HealthCheck</code></li>
<li>Change the machine phase to <code>Running</code> and update the machine status and return with a short retry.</li>
<li>(The above 2 cases take care of a newly created machine and a machine that became OK after ome temporary issue)</li>
</ol>
</li>
<li>If the current machine phase is <code>Pending</code> (ie machine being created: see <code>triggerCreationFlow</code>) get the configured machine creation timeout and check.
<ol>
<li>If the timoeut HAS NOT expired, enqueue the machine key on the machine work queue after 1m. </li>
<li>If the timeout HAS expired, then change the last operation state to <code>Failed</code> and the machine phase to <code>Failed</code>. Update the machine status and return with a short retry.</li>
</ol>
</li>
<li>If the current machine phase is <code>Unknown</code>, get the effective machine health timeout and check. 
<ol>
<li>If the timeout HAS NOT expired, enqueue the machine key on the machine work queue after 1m. </li>
<li>If the timeout HAS expired 
<ol>
<li>Get the machine deployment name <code>machineDeployName := machine.Labels['name']</code> corresponding to this machine</li>
<li>Register ONE permit with this with <code>machineDeployName</code>. See <a href="machine-controller/../mcm_facilities.html#permitspermitgiver">Permit Giver</a>. Q: Background of this change ? Couldn't we find a better way to throttle via work-queues instead of complicated <code>PermitGiver</code> and go-routines? Even simple lock would be OK here right ? </li>
<li>Attempt to get ONE permit for <code>machineDeployName</code> using a <code>lockAcquireTimeout</code> of 1s
<ol>
<li>Throttle to check whether machine CAN be marked as <code>Failed</code> using <code>markable, err := controller.canMarkMachineFailed</code>. </li>
<li>If machine can be marked, change the last operation state (ie the health check) to <code>Failed</code>, preserve the last operation type, change machine phase to <code>Failed</code>. Update the machine status. See <code>c.updateMachineToFailedState</code></li>
<li>Then use <code>wait.Poll</code> using 100ms as <code>pollInterval</code> and 1s as <code>cacheUpdateTimeout</code> using the following poll condition function:
<ol>
<li>Get the <code>machine</code> from the <code>machineLister</code> (which uses the cache of the shared informer)</li>
<li>Return true if <code>machine.Status.CurrentStatus.Phase</code> is <code>Failed</code> or <code>Terminating</code> or the <code>machine</code> is not found</li>
<li>Return false otherwise.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="health-check-doubts"><a class="header" href="#health-check-doubts">Health Check Doubts</a></h3>
<ol>
<li>TODO: Why don't we check the machine health using the <code>Driver.GetMachineStatus</code> in the reconcile Machine health ? (seems like something obvious to do and would have helped in those meltdown issues where machine was incorrectly marked as failed)</li>
<li>TODO: why doesn't this code make use of the helper method: <code>c.machineStatusUpdate</code> ?</li>
<li>TODO: Unclear why <code>LastOperation.Description</code> does not use/concatenate one of the predefined constants in <code>machineutils</code></li>
<li>TODO: code makes too much use of <code>cloneDirty</code> to check whether machine clone obj has changed, when it could easily return early in several branches.</li>
<li>TODO: Code directly makes calls to enqueue machine keys on the machine queue and still returns retry periods to caller leanding to un-necessary enqueue of machine keys. (spurious design)</li>
</ol>
<h2 id="controllertriggerupdationflow"><a class="header" href="#controllertriggerupdationflow">controller.triggerUpdationFlow</a></h2>
<p>Doesn't seem to be used ? Possibly dead code ?</p>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller/mc_helper_methods.html#machine-controller-helper-methods">Machine Controller Helper Methods</a>
<ul>
<li><a href="machine-controller/mc_helper_methods.html#controlleraddbootstraptokentouserdata">controller.addBootstrapTokenToUserData</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controlleraddmachinefinalizers">controller.addMachineFinalizers</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllersetmachineterminationstatus">controller.setMachineTerminationStatus</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllermachinestatusupdate">controller.machineStatusUpdate</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllerupdatenodeterminationcondition">controller.UpdateNodeTerminationCondition</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllerishealthy">controller.isHealthy</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllergetvmstatus">controller.getVMStatus</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllerdrainnode">controller.drainNode</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllerdeletevm">controller.deleteVM</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllerdeletenodeobject">controller.deleteNodeObject</a></li>
<li><a href="machine-controller/mc_helper_methods.html#controllersyncmachinenodetemplates">controller.syncMachineNodeTemplates</a></li>
</ul>
</li>
</ul>
<h1 id="machine-controller-helper-methods"><a class="header" href="#machine-controller-helper-methods">Machine Controller Helper Methods</a></h1>
<h2 id="controlleraddbootstraptokentouserdata"><a class="header" href="#controlleraddbootstraptokentouserdata">controller.addBootstrapTokenToUserData</a></h2>
<p>This method is responsible for adding the bootstrap token for the machine.</p>
<p>Bootstrap tokens are used when joining new nodes to a cluster. Bootstrap Tokens are defined with a specific <code>SecretType</code>: <code>bootstrap.kubernetes.io/token</code> and live in the <code>kube-system</code> namespace. These Secrets are then read by the Bootstrap Authenticator in the API Server</p>
<p>Reference</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">Bootstrap Tokens</a></li>
<li><a href="https://github.com/kubernetes/design-proposals-archive/blob/main/cluster-lifecycle/bootstrap-discovery.md#new-bootstrap-token-secrets">Bootstrap Token Secrets</a></li>
<li><a href="https://github.com/kubernetes/design-proposals-archive/blob/main/cluster-lifecycle/bootstrap-discovery.md#new-bootstrap-token-structure">Bootstrap Token Structure</a></li>
</ul>
<pre><code class="language-go">func (c *controller) addBootstrapTokenToUserData(ctx context.Context, machineName string, secret *corev1.Secret) error 

</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;InitTokenSecret[&quot;
tokenID := hex.EncodeToString([]byte(machineName)[len(machineName)-5:])[:6]
// 6 chars length
secretName :='bootstrap-token-' + tokenID
&quot;]
--&gt;GetSecret[&quot;
	secret, err = c.targetCoreClient.CoreV1().Secrets('kube-system').Get(ctx, secretName, GetOptions{}) 
&quot;]
--&gt;ChkErr{err!=nil?}

ChkErr--Yes--&gt;ChkNotFound{&quot;IsNotFound(err)&quot;}

ChkNotFound--Yes--&gt;GenToken[&quot;
  tokenSecretKey = generateRandomStrOf16Chars
&quot;]--&gt;InitSecretData[&quot;
  data := map[string][]byte{
    'token-id': []byte(tokenID),
    'token-secret': []byte(tokenSecretKey),
    'expiration': []byte(c.safetyOptions.MachineCreationTimeout.Duration)
    //..others
 }
&quot;]
--&gt;InitSecret[&quot;
  	secret = &amp;corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Name:      secretName,
					Namespace: metav1.NamespaceSystem,
				},
				Type: 'bootstrap.kubernetes.io/token'
				Data: data,
			}
&quot;]
--&gt;CreateSecret[&quot;
secret, err =c.targetCoreClient.CoreV1().Secrets('kube-system').Create(ctx, secret, CreateOptions{})
&quot;]
--&gt;ChkErr1{err!=nil?}

ChkErr1--Yes--&gt;ReturnErr((&quot;return err&quot;))
ChkNotFound--No--&gt;ReturnErr

ChkErr1--No--&gt;CreateToken[&quot;
token = tokenID + '.' + tokenSecretKey
&quot;]--&gt;InitUserData[&quot;
  userDataByes = secret.Data['userData']
  userDataStr = string(userDataBytes)
&quot;]--&gt;ReplaceUserData[&quot;
  	userDataS = strings.ReplaceAll(userDataS, 'BOOTSTRAP_TOKEN',placeholder, token)
   	secret.Data['userData'] = []byte(userDataS)
    //discuss this.
&quot;]--&gt;ReturnNil((&quot;return nil&quot;))

style InitSecretData text-align:left
style InitSecret text-align:left
</pre>
<h2 id="controlleraddmachinefinalizers"><a class="header" href="#controlleraddmachinefinalizers">controller.addMachineFinalizers</a></h2>
<p>This method checks for the <code>MCMFinalizer</code> Value: <code>machine.sapcloud.io/machine-controller-manager</code> and adds it if it is not present. It leverages <code>k8s.io/apimachinery/pkg/util/sets</code> package for its work.</p>
<p>This method is regularly called during machine reconciliation, if a machine does not have a deletion timestamp so that all non-deleted machines possess this finalizer.</p>
<pre><code class="language-go">func (c *controller) addMachineFinalizers(ctx context.Context, machine *v1alpha1.Machine) (machineutils.RetryPeriod, error)
	if finalizers := sets.NewString(machine.Finalizers...); !finalizers.Has(MCMFinalizerName) {
		finalizers.Insert(MCMFinalizerName)
		clone := machine.DeepCopy()
		clone.Finalizers = finalizers.List()
		_, err := c.controlMachineClient.Machines(clone.Namespace).Update(ctx, clone, metav1.UpdateOptions{})
		if err != nil {
			// Keep retrying until update goes through
			klog.Errorf(&quot;Failed to add finalizers for machine %q: %s&quot;, machine.Name, err)
		} else {
			// Return error even when machine object is updated
			klog.V(2).Infof(&quot;Added finalizer to machine %q with providerID %q and backing node %q&quot;, machine.Name, getProviderID(machine), getNodeName(machine))
			err = fmt.Errorf(&quot;Machine creation in process. Machine finalizers are UPDATED&quot;)
		}
	}
	return machineutils.ShortRetry, err

</code></pre>
<h2 id="controllersetmachineterminationstatus"><a class="header" href="#controllersetmachineterminationstatus">controller.setMachineTerminationStatus</a></h2>
<p><code>setMachineTerminationStatus</code> set's the machine status to terminating. This is illustrated below. Please note that <code>Machine.Status.LastOperation</code> is set an instance of the <code>LastOperation</code> struct. (which at times appears to be a command for the next action? Discuss this.) </p>
<pre><code class="language-go">func (c *controller) setMachineTerminationStatus(ctx context.Context, dmr *driver.DeleteMachineRequest) (machineutils.RetryPeriod, error)  
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

CreateClone[&quot;clone := dmr.Machine.DeepCopy()&quot;]
NewCurrStatus[&quot;currStatus := &amp;v1alpha1.CurrentStatus{Phase:\n MachineTerminating, LastUpdateTime: time.Now()}&quot;]
SetCurrentStatus[&quot;clone.Status.CurrentStatus = currStatus&quot;]
UpdateStatus[&quot;c.controlMachineClient.Machines(ns).UpdateStatus(clone)&quot;]
ShortR[&quot;retryPeriod=machineUtils.ShortRetry&quot;]
Z((&quot;Return&quot;))

CreateClone--&gt;NewCurrStatus
NewCurrStatus--&gt;SetCurrentStatus
SetCurrentStatus--&gt;UpdateStatus
UpdateStatus--&gt;ShortR
ShortR--&gt;Z
</pre>
<h2 id="controllermachinestatusupdate"><a class="header" href="#controllermachinestatusupdate">controller.machineStatusUpdate</a></h2>
<p>Updates <code>machine.Status.LastOperation</code>, <code>machine.Status.CurrentStatus</code> and <code>machine.Status.LastKnownState</code></p>
<pre><code class="language-go">func (c *controller) machineStatusUpdate(
	ctx context.Context,
	machine *v1alpha1.Machine,
	lastOperation v1alpha1.LastOperation,
	currentStatus v1alpha1.CurrentStatus,
	lastKnownState string) error 
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

CreateClone[&quot;clone := machine.DeepCopy()&quot;]
--&gt;InitClone[&quot;
	clone.Status.LastOperation = lastOperation
	clone.Status.CurrentStatus = currentStatus
	clone.Status.LastKnownState = lastKnownState
&quot;]
--&gt;ChkSimilarStatus{&quot;isMachineStatusSimilar(
	clone.Status,
	machine.Status)&quot;}

ChkSimilarStatus--No--&gt;UpdateStatus[&quot;
	err:=c.controlMachineClient
	.Machines(clone.Namespace)
	.UpdateStatus(ctx, clone, metav1.UpdateOptions{})
&quot;]
--&gt;Z1((&quot;return err&quot;))
ChkSimilarStatus--Yes--&gt;Z2((&quot;return nil&quot;))
</pre>
<p>NOTE: <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/machine_util.go#L544">isMachineStatusSimilar</a> implementation is quite sad. TODO: we should improve stuff like this when we move to controller-runtime.</p>
<h2 id="controllerupdatenodeterminationcondition"><a class="header" href="#controllerupdatenodeterminationcondition">controller.UpdateNodeTerminationCondition</a></h2>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machinecontroller/machine_util.go#L1316">controller.UpdateNodeTerminationCondition</a> adds or updates the termination condition to the <code>Node.Status.Conditions</code> of the node object corresponding to the machine.</p>
<pre><code class="language-go">func (c *controller) UpdateNodeTerminationCondition(ctx context.Context, machine *v1alpha1.Machine) error 
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Init[&quot;
	nodeName := machine.Labels['node']
	newTermCond := v1.NodeCondition{
		Type:               machineutils.NodeTerminationCondition,
		Status:             v1.ConditionTrue,
		LastHeartbeatTime:  Now(),
		LastTransitionTime: Now()}&quot;]
--&gt;GetCond[&quot;oldTermCond, err := nodeops.GetNodeCondition(ctx, c.targetCoreClient, nodeName, machineutils.NodeTerminationCondition)&quot;]
--&gt;ChkIfErr{&quot;err != nil ?&quot;}
ChkIfErr--Yes--&gt;ChkNotFound{&quot;apierrors.IsNotFound(err)&quot;}
ChkNotFound--Yes--&gt;ReturnNil((&quot;return nil&quot;))
ChkNotFound--No--&gt;ReturnErr((&quot;return err&quot;))
ChkIfErr--No--&gt;ChkOldTermCondNotNil{&quot;oldTermCond != nil
&amp;&amp; machine.Status.CurrentStatus.Phase 
== MachineTerminating ?&quot;}

ChkOldTermCondNotNil--No--&gt;ChkMachinePhase{&quot;Check\nmachine\n.Status.CurrentStatus\n.Phase?&quot;}
ChkMachinePhase--MachineFailed--&gt;NodeUnhealthy[&quot;newTermCond.Reason = machineutils.NodeUnhealthy&quot;]
ChkMachinePhase--&quot;else&quot;--&gt;NodeScaleDown[&quot;newTermCond.Reason=machineutils.NodeScaledDown
//assumes scaledown..why?&quot;]
NodeUnhealthy--&gt;UpdateCondOnNode[&quot;err=nodeops.AddOrUpdateConditionsOnNode(ctx, c.targetCoreClient, nodeName, newTermCond)&quot;]
NodeScaleDown--&gt;UpdateCondOnNode


ChkOldTermCondNotNil--Yes--&gt;CopyTermReasonAndMessage[&quot;
newTermCond.Reason=oldTermCond.Reason
newTermCond.Message=oldTermCond.Message
&quot;]
CopyTermReasonAndMessage--&gt;UpdateCondOnNode


UpdateCondOnNode--&gt;ChkNotFound
</pre>
<h2 id="controllerishealthy"><a class="header" href="#controllerishealthy">controller.isHealthy</a></h2>
<p>Checks if machine is healty by checking its conditions.</p>
<pre><code class="language-go">func (c *controller) isHealthy(machine *.Machine) bool 
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;Init[&quot;
	conditions = machine.Status.Conditions
	badTypes = strings.Split(
	 'KernelDeadlock,ReadonlyFilesystem,DiskPressure,NetworkUnavailable', 
		',')
&quot;]--&gt;ChkCondLen{&quot;len(conditions)==0?&quot;}

ChkCondLen--Yes--&gt;ReturnF((&quot;return false&quot;))
ChkCondLen--No--&gt;IterCond[&quot;c:= range conditions&quot;]
IterCond--&gt;ChkNodeReady{&quot;c.Type=='Ready'
&amp;&amp; c.Status != 'True' ?&quot;}--Yes--&gt;ReturnF
ChkNodeReady
--Yes--&gt;IterBadConditions[&quot;badType := range badTypes&quot;]
--&gt;ChkType{&quot;badType == c.Type
&amp;&amp;
c.Status != 'False' ?&quot;}
--Yes--&gt;ReturnF

IterBadConditions--loop--&gt;IterCond
ChkType--loop--&gt;IterBadConditions




style Init text-align:left
</pre>
<p>NOTE</p>
<ol>
<li>controller.NodeConditions should be called controller.BadConditionTypes</li>
<li>Iterate over <code>machine.Status.Conditions</code>
<ol>
<li>If <code>Ready</code> condition inis not <code>True</code>, node is determined as un-healty.</li>
<li>If any of the bad condition types are detected, then node is determine as un-healthy</li>
</ol>
</li>
</ol>
<h2 id="controllergetvmstatus"><a class="header" href="#controllergetvmstatus">controller.getVMStatus</a></h2>
<p>(BAD NAME FOR METHOD: should be called <code>checkMachineExistenceAndEnqueNextOperation</code>)</p>
<pre><code class="language-go">func (c *controller) getVMStatus(ctx context.Context, 
    statusReq *driver.GetMachineStatusRequest) (machineutils.RetryPeriod, error)
</code></pre>
<p>This method is only called for the delete flow. </p>
<ol>
<li>It attempts to get the machine status</li>
<li>If the machine exists, it updates the machine status operation to <code>InitiateDrain</code> and returns a <code>ShortRetry</code> for the machine work queue. </li>
<li>If attempt to get machine status failed, it will obtain the error code from the error.
<ol>
<li>For <code>Unimplemented</code>(ie <code>GetMachineStatus</code> op was is not implemented), it does the same as <code>2</code>. ie: it updates the machine status operation to <code>InitiateDrain</code> and returns a <code>ShortRetry</code> for the machine work queue. </li>
<li>If decoding the error code failed, it will update the  machine status operation to <code>machineutils.GetVMStatus</code> and returns a <code>LongRetry</code> for the machine key into the machine work queue. 
<ol>
<li>Unsure how we get out of this Loop. TODO: Discuss this.</li>
</ol>
</li>
<li>For <code>Unknown|DeadlineExceeded|Aborted|Unavailable</code> it updates the machine status operation to <code>machineutils.GetVMStatus</code> status and returns a <code>ShortRetry</code> for the machine work queue.  (So that reconcile will run this method again in future)</li>
<li>For <code>NotFound</code> code (ie machine is not found), it will enqueue node deletion by updating the machine stauts operation to <code>machineutils.InitiateNodeDeletion</code> and returning a <code>ShortRetry</code> for the machine work queue.</li>
</ol>
</li>
</ol>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

GetMachineStatus[&quot;_,err=driver.GetMachineStatus(statusReq)&quot;]
ChkMachineExists{&quot;err==nil ?\n (ie machine exists)&quot;}
DecodeErrorStatus[&quot;errStatus,decodeOk= status.FromError(err)&quot;]
CheckDecodeOk{&quot;decodeOk ?&quot;}

CheckErrStatusCode{&quot;Check errStatus.Code&quot;}

CreateDrainOp[&quot;op:=LastOperation{Description: machineutils.InitiateDrain
State: v1alpha1.MachineStateProcessing,
Type: v1alpha1.MachineOperationDelete,
Time: time.Now()}&quot;]

CreateNodeDelOp[&quot;op:=LastOperation{Description: machineutils.InitiateNodeDeletion
State: v1alpha1.MachineStateProcessing,
Type: v1alpha1.MachineOperationDelete,
Time: time.Now()}&quot;]

CreateDecodeFailedOp[&quot;op:=LastOperation{Description: machineutils.GetVMStatus,
State: v1alpha1.MachineStateFailed,
Type: v1alpha1.MachineOperationDelete,
Time: time.Now()}&quot;]

CreaterRetryVMStatusOp[&quot;op:=LastOperation{Description: ma1chineutils.GetVMStatus,
State: v1alpha1.MachineStateFailed,
Type:  v1alpha1.MachineOperationDelete,
Time: time.Now()}&quot;]

ShortR[&quot;retryPeriod=machineUtils.ShortRetry&quot;]
LongR[&quot;retryPeriod=machineUtils.LongRetry&quot;]
UpdateMachineStatus[&quot;c.machineStatusUpdate(machine,op,machine.Status.CurrentStatus, machine.Status.LastKnownState)&quot;]

Z((&quot;End&quot;))

GetMachineStatus--&gt;ChkMachineExists

ChkMachineExists--Yes--&gt;CreateDrainOp
ChkMachineExists--No--&gt;DecodeErrorStatus
DecodeErrorStatus--&gt;CheckDecodeOk
CheckDecodeOk--Yes--&gt;CheckErrStatusCode
CheckDecodeOk--No--&gt;CreateDecodeFailedOp
CreateDecodeFailedOp--&gt;LongR
CheckErrStatusCode--&quot;Unimplemented&quot;--&gt;CreateDrainOp
CheckErrStatusCode--&quot;Unknown|DeadlineExceeded|Aborted|Unavailable&quot;--&gt;CreaterRetryVMStatusOp
CheckErrStatusCode--&quot;NotFound&quot;--&gt;CreateNodeDelOp
CreaterRetryVMStatusOp--&gt;ShortR

CreateDrainOp--&gt;ShortR
CreateNodeDelOp--&gt;ShortR
ShortR--&gt;UpdateMachineStatus
LongR--&gt;UpdateMachineStatus
UpdateMachineStatus--&gt;Z
</pre>
<h2 id="controllerdrainnode"><a class="header" href="#controllerdrainnode">controller.drainNode</a></h2>
<p>Inside <code>pkg/util/provider/machinecontroller/machine_util.go</code></p>
<pre><code class="language-go">func (c *controller) drainNode(ctx context.Context, dmr *driver.DeleteMachineRequest) (machineutils.RetryPeriod, error)
</code></pre>
<pre class="mermaid">
%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD


Initialize[&quot;err = nil
machine = dmr.Machine
nodeName= machine.Labels['node']
drainTimeout=machine.Spec.MachineConfiguration.MachineDrainTimeout || c.safetyOptions.MachineDrainTimeout
maxEvictRetries=machine.Spec.MachineConfiguration.MaxEvictRetries || c.safetyOptions.MaxEvictRetries
skipDrain = false&quot;]
--&gt;GetNodeReadyCond[&quot;nodeReadyCond = machine.Status.Conditions contains k8s.io/api/core/v1/NodeReady
readOnlyFSCond=machine.Status.Conditions contains 'ReadonlyFilesystem' 
&quot;]
--&gt;ChkNodeNotReady[&quot;skipDrain = (nodeReadyCond.Status == ConditionFalse) &amp;&amp; nodeReadyCondition.LastTransitionTime.Time &gt; 5m
or (readOnlyFSCond.Status == ConditionTrue) &amp;&amp; readOnlyFSCond.LastTransitionTime.Time &gt; 5m
// discuss this
&quot;]
--&gt;ChkSkipDrain{&quot;skipDrain true?&quot;}
ChkSkipDrain--Yes--&gt;SetOpStateProcessing
ChkSkipDrain--No--&gt;SetHasDrainTimedOut[&quot;hasDrainTimedOut = time.Now() &gt; machine.DeletionTimestamp + drainTimeout&quot;]
SetHasDrainTimedOut--&gt;ChkForceDelOrTimedOut{&quot;machine.Labels['force-deletion']
  || hasDrainTimedOut&quot;}

ChkForceDelOrTimedOut--Yes--&gt;SetForceDelParams[&quot;
  forceDeletePods=true
  drainTimeout=1m
  maxEvictRetries=1
  &quot;]
SetForceDelParams--&gt;UpdateNodeTermCond[&quot;err=c.UpdateNodeTerminationCondition(ctx, machine)&quot;]
ChkForceDelOrTimedOut--No--&gt;UpdateNodeTermCond

UpdateNodeTermCond--&gt;ChkUpdateErr{&quot;err != nil ?&quot;}
ChkUpdateErr--No--&gt;InitDrainOpts[&quot;
  // params reduced for brevity
  drainOptions := drain.NewDrainOptions(
    c.targetCoreClient,
    drainTimeout,
    maxEvictRetries,
    c.safetyOptions.PvDetachTimeout.Duration,
    c.safetyOptions.PvReattachTimeout.Duration,
    nodeName,
    forceDeletePods,
    c.driver,
		c.pvcLister,
		c.pvLister,
    c.pdbV1Lister,
		c.nodeLister,
		c.volumeAttachmentHandler)
&quot;]
ChkUpdateErr--&quot;Yes&amp;&amp;forceDelPods&quot;--&gt;InitDrainOpts
ChkUpdateErr--Yes--&gt;SetOpStateFailed[&quot;opstate = v1alpha1.MachineStateFailed
  description=machineutils.InitiateDrain
  //drain failed. retry next sync
  &quot;]

InitDrainOpts--&gt;RunDrain[&quot;err = drainOptions.RunDrain(ctx)&quot;]
RunDrain--&gt;ChkDrainErr{&quot;err!=nil?&quot;}
ChkDrainErr--No--&gt;SetOpStateProcessing[&quot;
  opstate= v1alpha1.MachineStateProcessing
  description=machineutils.InitiateVMDeletion
// proceed with vm deletion&quot;]
ChkDrainErr--&quot;Yes &amp;&amp; forceDeletePods&quot;--&gt;SetOpStateProcessing
ChkDrainErr--Yes--&gt;SetOpStateFailed
SetOpStateProcessing--&gt;
InitLastOp[&quot;lastOp:=v1alpha1.LastOperation{
			Description:    description,
			State:          state,
			Type:           v1alpha1.MachineOperationDelete,
			LastUpdateTime: metav1.Now(),
		}
  //lastOp is actually the *next* op semantically&quot;]
SetOpStateFailed--&gt;InitLastOp
InitLastOp--&gt;UpdateMachineStatus[&quot;c.machineStatusUpdate(ctx,machine,lastOp,machine.Status.CurrentStatus,machine.Status.LastKnownState)&quot;]
--&gt;Return((&quot;machineutils.ShortRetry, err&quot;))
</pre>
<p>Note on above</p>
<ol>
<li>We skip the drain if node is set to ReadonlyFilesystem for over 5 minutes
<ol>
<li>Check TODO:  <code>ReadonlyFilesystem</code> is a MCM condition and not a k8s core node condition. Not sure if we are mis-using this field. TODO: Check this.</li>
</ol>
</li>
<li>Check TODO: Why do we check that node is not ready for 5m in order to skip the drain ? Shouldn't we skip the drain if node is simply not ready ? Why wait for 5m here ?/</li>
<li>See <a href="machine-controller/./node_drain.html#run-drain">Run Drain</a></li>
</ol>
<h2 id="controllerdeletevm"><a class="header" href="#controllerdeletevm">controller.deleteVM</a></h2>
<p>Called by <code>controller.triggerDeletionFlow</code></p>
<pre><code class="language-go">func (c *controller) deleteVM(ctx context.Context, dmReq *driver.DeleteMachineRequest) 
	(machineutils.RetryPeriod, error)

</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;Init[&quot;
machine = dmr.Machine
&quot;]
--&gt;CallDeleteMachine[&quot;
dmResp, err := c.driver.DeleteMachine(ctx, dmReq)
&quot;]
--&gt;ChkDelErr{&quot;err!=nil?&quot;}

ChkDelErr--No--&gt;SetSuccShortRetry[&quot;
retryPeriod = machineutils.ShortRetry
description = 'VM deletion was successful.'+ machineutils.InitiateNodeDeletion)
state = MachineStateProcessing
&quot;]
--&gt;InitLastOp[&quot;
lastOp := LastOperation{
Description:    description,
State:          state,
Type:           MachineOperationDelete,
LastUpdateTime: Now(),
},
&quot;]
--&gt;SetLastKnownState[&quot;
// useless since drivers impls dont set this?. 
// Use machine.Status.LastKnownState instead ?
lastKnownState = dmResp.LastKnownState
&quot;]
--&gt;UpdateMachineStatus[&quot;
//Discuss: Introduce finer grained phase for status ?
c.machineStatusUpdate(ctx,machine,lastOp,machine.Status.CurrentStatus,lastKnownState)&quot;]--&gt;Return((&quot;retryPeriod, err&quot;))

ChkDelErr--Yes--&gt;DecodeErrorStatus[&quot;
  errStatus,decodeOk= status.FromError(err)
&quot;]
DecodeErrorStatus--&gt;CheckDecodeOk{&quot;decodeOk ?&quot;}
CheckDecodeOk--No--&gt;SetFailed[&quot;
	state = MachineStateFailed
	description = 'machine decode error' + machineutils.InitiateVMDeletion
&quot;]--&gt;SetLongRetry[&quot;
retryPeriod= machineutils.LongRetry
&quot;]--&gt;InitLastOp

CheckDecodeOk--Yes--&gt;AnalyzeCode{status.Code?}
AnalyzeCode--Unknown, DeadlineExceeded,Aborted,Unavailable--&gt;SetDelFailed[&quot;
state = MachineStateFailed
description = VM deletion failed due to
+ err + machineutils.InitiateVMDeletion&quot;]
--&gt;SetShortRetry[&quot;
retryPeriod= machineutils.ShortRetry
&quot;]--&gt;InitLastOp

AnalyzeCode--NotFound--&gt;DelSuccess[&quot;
// we can proceed with deleting node.
description = 'VM not found. Continuing deletion flow'+ machineutils.InitiateNodeDeletion
state = MachineStateProcessing
&quot;]--&gt;SetShortRetry

AnalyzeCode--default--&gt;DelUnknown[&quot;
state = MachineStateFailed
description='VM deletion failed due to' + err 
+ 'Aborting..' + machineutils.InitiateVMDeletion
&quot;]--&gt;SetLongRetry

</pre>
<h2 id="controllerdeletenodeobject"><a class="header" href="#controllerdeletenodeobject">controller.deleteNodeObject</a></h2>
<p>NOTE: Should have just called this <code>controller.deleteNode</code> for naming consistency with other methods.</p>
<p>Called by <code>triggerDeletionFlow</code> after successfully deleting the VM.</p>
<pre><code class="language-go">func (c *controller) deleteNodeObject(ctx context.Context, 
machine *v1alpha1.Machine) 
	(machineutils.RetryPeriod, error) 
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;Init[&quot;
nodeName := machine.Labels['node']
&quot;]
--&gt;ChkNodeName{&quot;nodeName != ''&quot; ?}
ChkNodeName--Yes--&gt;DelNode[&quot;
err = c.targetCoreClient.CoreV1().Nodes().Delete(ctx, nodeName, metav1.DeleteOptions{})
&quot;]--&gt;ChkDelErr{&quot;err!=nil?&quot;}

ChkNodeName--No--&gt;NodeObjNotFound[&quot;
	state = MachineStateProcessing
	description = 'No node object found for' + nodeName + 'Continue'
	+ machineutils.InitiateFinalizerRemoval
&quot;]--&gt;InitLastOp

ChkDelErr--No--&gt;DelSuccess[&quot;
	state = MachineStateProcessing
	description = 'Deletion of Node' + nodeName + 'successful'
	 + machineutils.InitiateFinalizerRemoval 
&quot;]--&gt;InitLastOp

ChkDelErr--Yes&amp;&amp;!apierrorsIsNotFound--&gt;FailedNodeDel[&quot;
	state = MachineStateFailed
	description = 'Deletion of Node' + 
		nodeName + 'failed due to' + err + machineutils.InitiateNodeDeletion
&quot;]
--&gt;InitLastOp[&quot;
lastOp := LastOperation{
Description:    description,
State:          state,
Type:           MachineOperationDelete,
LastUpdateTime: Now(),
},
&quot;]
--&gt;UpdateMachineStatus[&quot;
c.machineStatusUpdate(ctx,machine,lastOp,machine.Status.CurrentStatus,machine.Status.LastKnownState)&quot;]
--&gt;Return((&quot;machineutils.ShortRetry, err&quot;))
</pre>
<h2 id="controllersyncmachinenodetemplates"><a class="header" href="#controllersyncmachinenodetemplates">controller.syncMachineNodeTemplates</a></h2>
<pre><code class="language-go">func (c *controller) syncMachineNodeTemplates(ctx context.Context, 
	machine *v1alpha1.Machine) 
		(machineutils.RetryPeriod, error) 
</code></pre>
<p>See <a href="machine-controller/../mcm_facilities.html#machinespec">MachineSpec</a></p>
<p><code>syncMachineNodeTemplates</code> syncs <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/apis/machine/v1alpha1#NodeTemplateSpec">machine.Spec.NodeTemplateSpec</a> between machine and corresponding node-object.  A <code>NodeTemplateSpec</code> just wraps a core <a href="machine-controller/../k8s_facilities.html#nodespec">NodeSpec</a> and <a href="machine-controller/../k8s_facilities.html#objectmeta">ObjectMetadata</a></p>
<p>Get the <code>node</code> for the given <code>machine</code> and then:</p>
<ol>
<li>Synchronize <code>node.Annotations</code> to <code>machine.Spec.NodeTemplateSpec.Annotations</code>. </li>
<li>Synchronize <code>node.Labels</code> to  <code>machine.Spec.NodeTemplateSpec.Labels</code></li>
<li>Synchronize <code>node.Spec.Taints</code> to <code>machine.Spec.NodeTemplateSpec.Spec.Taints</code></li>
<li>Update the <code>node</code> object if there were changes.</li>
</ol>
<p>Since, we should not delete third-party annotations on the node object, synchronizing deleted ALT from the machine object is a bit tricky and so we maintain yet another custom annotation <code>node.machine.sapcloud.io/last-applied-anno-labels-taints</code> (code constant <code>LastAppliedALTAnnotation</code>) on the <code>Node</code> object which is a JSON string of the <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.48.0/pkg/apis/machine/v1alpha1#NodeTemplateSpec">NodeTemplateSpec</a>. </p>
<ol>
<li>Before synchronizing, we un-marshall this <code>LastAppliedALTAnnotation</code> value into a <code>lastAppliedALT</code> of type <code>NodeTemplateSpec</code>. </li>
<li>While synchronizing annotations, labels and taints, we check respectively whether <code>lastAppliedALT.Annotations</code>, <code>lastAppliedALT.Labels</code> and <code>lastAppliedALT.Spec.Taints</code> hold keys that are NOT in the corresponding <code>machine.Spec.NodeTemplateSpec.Annotations</code>, <code>machine.Spec.NodeTemplateSpec.Labels</code> and <code> machine.Spec.NodeTemplateSpec.Spec.Taints</code>. 
<ol>
<li>If so, we delete keys from the corresponding <code> node.Annotations</code>, <code>node.Labels</code> and <code>node.Spec.Taints</code> respectively.</li>
<li>We maintain a boolean saying the <code>LastAppliedALTAnnotation</code> needs to be updated</li>
</ol>
</li>
<li>Just before updating the Node object, we check if <code>LastAppliedALTAnnotation</code> needs updation and if so we Jsonify <code>machine.Spec.NodeTemplateSpec</code> and override the new <code>LastAppliedALTAnnotation</code> to this value</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller/node_drain.html#node-drain">Node Drain</a>
<ul>
<li><a href="machine-controller/node_drain.html#drain-utilities">Drain Utilities</a>
<ul>
<li><a href="machine-controller/node_drain.html#volumeattachmenthandler">VolumeAttachmentHandler</a>
<ul>
<li><a href="machine-controller/node_drain.html#volumeattachmenthandleraddworker">VolumeAttachmentHandler.AddWorker</a></li>
<li><a href="machine-controller/node_drain.html#volumeattachmenthandlerdispatch">VolumeAttachmentHandler.dispatch</a></li>
<li><a href="machine-controller/node_drain.html#volumeattachmenthandler-initialization-in-mc">VolumeAttachmentHandler Initialization in MC</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="machine-controller/node_drain.html#drain">Drain</a>
<ul>
<li><a href="machine-controller/node_drain.html#drain-types">Drain Types</a>
<ul>
<li><a href="machine-controller/node_drain.html#drain-constants">Drain Constants</a></li>
<li><a href="machine-controller/node_drain.html#drainoptions">drain.Options</a></li>
</ul>
</li>
<li><a href="machine-controller/node_drain.html#drainpodvolumeinfo">drain.PodVolumeInfo</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsevictpod">drain.Options.evictPod</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsdeletepod">drain.Options.deletePod</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsgetpodsfordeletion">drain.Options.getPodsForDeletion</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsgetpvlist">drain.Options.getPVList</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsgetvolidsfromdriver">drain.Options.getVolIDsFromDriver</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsdoaccountingofpvs">drain.Options.doAccountingOfPvs</a></li>
<li><a href="machine-controller/node_drain.html#drainfilterpodswithpv">drain.filterPodsWithPv</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionswaitfordetach">drain.Options.waitForDetach</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionswaitforreattach">drain.Options.waitForReattach</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionswaitfordelete">drain.Options.waitForDelete</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsrundrain">drain.Options.RunDrain</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsevictpodswithoutpv">drain.Options.evictPodsWithoutPv</a>
<ul>
<li><a href="machine-controller/node_drain.html#drainoptionsevictpodwithoutpvinternal">drain.Options.evictPodWithoutPVInternal</a></li>
<li><a href="machine-controller/node_drain.html#ismisconfiguredpdb">isMisconfiguredPdb</a></li>
</ul>
</li>
<li><a href="machine-controller/node_drain.html#drainoptionsevictpodswithpv">drain.Options.evictPodsWithPv</a></li>
<li><a href="machine-controller/node_drain.html#drainoptionsevictpodswithpvinternal">drain.Options.evictPodsWithPVInternal</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="node-drain"><a class="header" href="#node-drain">Node Drain</a></h1>
<p>Node Drain code is in <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go">github.com/gardener/machine-controller-manager/pkg/util/provider/drain/drain.go</a></p>
<h2 id="drain-utilities"><a class="header" href="#drain-utilities">Drain Utilities</a></h2>
<h3 id="volumeattachmenthandler"><a class="header" href="#volumeattachmenthandler">VolumeAttachmentHandler</a></h3>
<p><a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/drain#VolumeAttachmentHandler">pkg/util/provider/drain.VolumeAttachmentHandler</a> is an handler used to distribute
incoming <a href="https://pkg.go.dev/k8s.io/api/storage/v1#VolumeAttachment">k8s.io/api/storage/v1.VolumeAttachment</a> requests to a number of workers where each worker is a channel of type <code>*VolumeAttachment</code>. </p>
<p>A <a href="https://pkg.go.dev/k8s.io/api/storage/v1#VolumeAttachment">k8s.io/api/storage/v1.VolumeAttachment</a> is a non-namespaced k8s object that captures the intent to attach or detach the specified volume to/from the specified node. See <a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/">VolumeAttachment</a></p>
<pre><code class="language-go">type VolumeAttachmentHandler struct {
	sync.Mutex
	workers []chan *storagev1.VolumeAttachment
}

// NewVolumeAttachmentHandler returns a new VolumeAttachmentHandler
func NewVolumeAttachmentHandler() *VolumeAttachmentHandler {
	return &amp;VolumeAttachmentHandler{
		Mutex:   sync.Mutex{},
		workers: []chan *storagev1.VolumeAttachment{},
	}
}

</code></pre>
<h4 id="volumeattachmenthandleraddworker"><a class="header" href="#volumeattachmenthandleraddworker">VolumeAttachmentHandler.AddWorker</a></h4>
<p><code>AddWorker</code> appends a buffered channel of size <code>20</code> of type <code>VolumeAttachment</code> to the <code>workers</code> slice  in <code>VolumeAttachmentHandler</code>
. There is an assumption that not more than 20 unprocessed objects would exist at a given time. On bufferring requests beyond this the channel will start dropping writes. See <code>dispatch</code> method.</p>
<pre><code class="language-go">func (v *VolumeAttachmentHandler) AddWorker() chan *storagev1.VolumeAttachment {
	// chanSize is the channel buffer size to hold requests.
	// This assumes 
	// On bufferring requests beyond this the channel will start dropping writes
	const chanSize = 20

	klog.V(4).Infof(&quot;Adding new worker. Current active workers %d - %v&quot;, len(v.workers), v.workers)

	v.Lock()
	defer v.Unlock()

	newWorker := make(chan *storagev1.VolumeAttachment, chanSize)
	v.workers = append(v.workers, newWorker)

	klog.V(4).Infof(&quot;Successfully added new worker %v. Current active workers %d - %v&quot;, newWorker, len(v.workers), v.workers)
	return newWorker
}

</code></pre>
<h4 id="volumeattachmenthandlerdispatch"><a class="header" href="#volumeattachmenthandlerdispatch">VolumeAttachmentHandler.dispatch</a></h4>
<p>The <code>dispatch</code> method is responsible for distributing incomding <code>VolumeAttachent</code>s to available channels.</p>
<pre><code class="language-go">func (v *VolumeAttachmentHandler) dispatch(obj interface{}) {
	if len(v.workers) == 0 {
		// As no workers are registered, nothing to do here.
		return
	}
	volumeAttachment := obj.(*storagev1.VolumeAttachment)
	v.Lock()
	defer v.Unlock()

	for i, worker := range v.workers {
		select {
		// submit volume attachment to the worker channel if channel is not full
		case worker &lt;- volumeAttachment:
		default:
			klog.Warningf(&quot;Worker %d/%v is full. Discarding value.&quot;, i, worker)
			// TODO: Umm..isn't this problematic if we miss this ?
		}
	}
}
</code></pre>
<p>The <code>Add|Update</code> methods below delegate to dispatch. The usage of this utility involves specifying the add/update methods below as the event handler callbacks on an instance of <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/informers/storage/v1#VolumeAttachmentInformer">k8s.io/client-go/informers/storage/v1.VolumeAttachmentInformer</a>. This way incoming volume attachments are distributed to several worker channels.</p>
<pre><code class="language-go">func (v *VolumeAttachmentHandler) AddVolumeAttachment(obj interface{}) {
	v.dispatch(obj)
}

func (v *VolumeAttachmentHandler) UpdateVolumeAttachment(oldObj, newObj interface{}) {
	v.dispatch(newObj)
}
</code></pre>
<h4 id="volumeattachmenthandler-initialization-in-mc"><a class="header" href="#volumeattachmenthandler-initialization-in-mc">VolumeAttachmentHandler Initialization in MC</a></h4>
<p>During construction of the MC controller struct, we initialize the callback methods on volume attachment handler using the volume attachment informer</p>
<pre><code class="language-go">func NewController(...) {
	//...
controller.volumeAttachmentHandler = drain.NewVolumeAttachmentHandler()
volumeAttachmentInformer.Informer().AddEventHandler(
	cache.ResourceEventHandlerFuncs{
			AddFunc:    controller.volumeAttachmentHandler.AddVolumeAttachment,
			UpdateFunc: controller.volumeAttachmentHandler.UpdateVolumeAttachment,
});
</code></pre>
<h2 id="drain"><a class="header" href="#drain">Drain</a></h2>
<h3 id="drain-types"><a class="header" href="#drain-types">Drain Types</a></h3>
<h4 id="drain-constants"><a class="header" href="#drain-constants">Drain Constants</a></h4>
<ul>
<li><code>PodEvictionRetryInterval</code> is the interval in which to retry eviction for pods</li>
<li><code>GetPvDetailsMaxRetries</code> is the number of max retries to get PV details using the <a href="https://pkg.go.dev/k8s.io/client-go/listers/core/v1#PersistentVolumeLister">PersistentVolumeLister</a> or <a href="https://pkg.go.dev/k8s.io/client-go/listers/core/v1#PersistentVolumeClaimLister">PersistentVolumeClaimLister</a></li>
<li><code>GetPvDetailsRetryInterval</code> is the interval in which to retry getting PV details</li>
</ul>
<pre><code class="language-go">const (
    PodEvictionRetryInterval = time.Second * 20
	GetPvDetailsMaxRetries = 3
	GetPvDetailsRetryInterval = time.Second * 5
)
</code></pre>
<h4 id="drainoptions"><a class="header" href="#drainoptions">drain.Options</a></h4>
<p><code>drain.Options</code> are configurable options while draining a node before deletion</p>
<p>NOTE: Unused fields/Fields with constant defaults omitted for brevity</p>
<pre><code class="language-go">type Options struct {
	client                       kubernetes.Interface
	kubernetesVersion            *semver.Version
	Driver                       driver.Driver
	drainStartedOn               time.Time
	drainEndedOn                 time.Time
	ErrOut                       io.Writer
	ForceDeletePods              bool
	MaxEvictRetries              int32
	PvDetachTimeout              time.Duration
	PvReattachTimeout            time.Duration
	nodeName                     string
	Out                          io.Writer
	pvcLister                    corelisters.PersistentVolumeClaimLister
	pvLister                     corelisters.PersistentVolumeLister
	pdbV1Lister                  policyv1listers.PodDisruptionBudgetLister
	nodeLister                   corelisters.NodeLister
	volumeAttachmentHandler      *VolumeAttachmentHandler
	Timeout                      time.Duration
}

</code></pre>
<h3 id="drainpodvolumeinfo"><a class="header" href="#drainpodvolumeinfo">drain.PodVolumeInfo</a></h3>
<p><code>drain.PodVolumeInfo</code> is the struct used to encapsulate the PV names and PV ID's for all the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVs</a> attached to the pod</p>
<pre><code class="language-go">PodVolumeInfo struct {
	persistentVolumeList []string
	volumeList           []string
}
</code></pre>
<p>NOTE: The struct fields are badly named.</p>
<ul>
<li><code>PodVolumeInfo.persistentVolumeList</code> is a slice of persistent volume names. This is from <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolumeSpec">PersistentVolumeSpec.VolumeName</a></li>
<li><code>PodVolumeInfo.volumeList</code> is a slice of persistent volume IDs. This is obtained using <a href="machine-controller/../src/mcm_facilities.html#driver">driver.GetVolumeIDs</a> given the PV Spec. This is generally the CSI volume id.</li>
</ul>
<h3 id="drainoptionsevictpod"><a class="header" href="#drainoptionsevictpod">drain.Options.evictPod</a></h3>
<pre><code class="language-go">func (o *Options) evictPod(ctx context.Context, pod *corev1.Pod, policyGroupVersion string) error 
</code></pre>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L363">drain.Options.evictPod</a> is a simple helper method to evict a Pod using <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/policy/v1#EvictionExpansion">Eviction API</a></p>
<ul>
<li>TODO: <code>GracePeriodSeconds</code> in the code is useless here and should be removed as it is always -1.</li>
<li>TODO: Currently this method uses old <code>k8s.io/api/policy/v1beta1</code>. It must be changed to  <code>k8s.io/api/policy/v1</code></li>
</ul>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot;&quot; ))
--&gt;InitTypeMeta[&quot;
		typeMeta:= metav1.TypeMeta{
			APIVersion: policyGroupVersion,
			Kind:       'Eviction',
		},
&quot;]
--&gt;InitObjectMeta[&quot;
		objectMeta := ObjectMeta: metav1.ObjectMeta{
			Name:      pod.Name,
			Namespace: pod.Namespace,
		},
&quot;]
--&gt;InitEviction[&quot;
eviction := &amp;.Eviction{TypeMeta: typeMeta, ObjectMeta: objectMeta }
&quot;]
--&gt;EvictPod[&quot;
 err := o.client.PolicyV1beta1().Evictions(eviction.Namespace).Evict(ctx, eviction)
&quot;]
--&gt;ReturnErr((&quot;return err&quot;))

style InitEviction text-align:left
</pre>
<h3 id="drainoptionsdeletepod"><a class="header" href="#drainoptionsdeletepod">drain.Options.deletePod</a></h3>
<p>Simple helper method to delete a Pod</p>
<pre><code class="language-go">func (o *Options) deletePod(ctx context.Context, pod *corev1.Pod) error {
</code></pre>
<p>Just delegates to <a href="https://pkg.go.dev/k8s.io/client-go@v0.25.2/kubernetes/typed/core/v1#PodInterface">PodInterface.Delete</a></p>
<pre><code class="language-go">o.client.CoreV1().Pods(pod.Namespace).Delete(ctx, pod.Name, metav1.DeleteOptions{} )
</code></pre>
<h3 id="drainoptionsgetpodsfordeletion"><a class="header" href="#drainoptionsgetpodsfordeletion">drain.Options.getPodsForDeletion</a></h3>
<p><code>drain.getPodsForDeletion</code> returns all the pods we're going to delete.  If there are  any pods preventing us from deleting, we return that list in an error.</p>
<pre><code class="language-go">func (o *Options) getPodsForDeletion(ctx context.Context) 
	(pods []corev1.Pod, err error)
</code></pre>
<ol>
<li>Get all pods associated with the node.
<pre><code class="language-go">podList, err := o.client.CoreV1().Pods(metav1.NamespaceAll).List(ctx, metav1.ListOptions{
 	FieldSelector: fields.SelectorFromSet(fields.Set{&quot;spec.nodeName&quot;: o.nodeName}).String()})
</code></pre>
</li>
<li>Iterate through <code>podList</code>.</li>
<li>Apply a bunch of pod filters. 
<ol>
<li>Remove mirror pods from consideration for deletion. See <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1314-node-restriction-pods/README.md#background">Static Pods</a></li>
<li>Local Storage Filter. Discuss: seems useless. If Pod has local storge, remove it from consideration. This filter iterates through <code>Pod.Spec.Volumes</code> slice and checks whether <code>Volume.EmptyDir</code> is non nil in order to determine</li>
<li>A Pod whose <code>Pod.Status.Phase</code> is <a href="https://pkg.go.dev/k8s.io/api/core/v1#PodSucceeded">PodSucceeded</a> or <a href="https://pkg.go.dev/k8s.io/api/core/v1#PodFailed">PodFailed</a> is eligible for deletion
<ol>
<li>If a Pod has a controller owner reference, it is eligible for deletion. (TODO: Unsure why this makes a difference anyways)</li>
</ol>
</li>
<li>The final pod filter <code>daemonsetFilter</code> seems useless. Discuss.</li>
</ol>
</li>
</ol>
<h3 id="drainoptionsgetpvlist"><a class="header" href="#drainoptionsgetpvlist">drain.Options.getPVList</a></h3>
<p>NOTE: Should be called <code>getPVNames</code>.  Gets a slice of the persistent volume names bound to the Pod through its claims.  Contains time.sleep and retry handling to a limit. Unsure if this is the best way. Discuss.</p>
<pre><code class="language-go">func (o *Options) getPVList(pod *corev1.Pod) (pvNames []string, err error) 
</code></pre>
<ol>
<li>Iterate over <code>pod.Spec.Volumes</code>.</li>
<li>If <code>volume.PersistentVolumeClaim</code> reference is not nil, gets the <code>PersistentVolumeClaim</code> using <code>o.pvcLister</code> using <code>vol.PersistentVolumeClaim.ClaimName</code>.
<ol>
<li>Implements error handling and retry till <code>GetPvDetailsMaxRetries</code> is reached with interval <code>GetPvDetailsRetryInterval</code> for the above.</li>
</ol>
</li>
<li>Adds <code>pvc.Spec.VolumeName</code> to <code>pvNames</code></li>
<li>Return <code>pvNames</code></li>
</ol>
<h3 id="drainoptionsgetvolidsfromdriver"><a class="header" href="#drainoptionsgetvolidsfromdriver">drain.Options.getVolIDsFromDriver</a></h3>
<p>Given a slice of PV Names, this method gets the corresponding volume ids from the driver. </p>
<ul>
<li>It does this by first getting the <a href="https://pkg.go.dev/k8s.io/api/core/v1#PersistentVolumeSpec">PersistentVolumeSpec</a> using <code>o.pvLister.Get(pvName)</code> for each PV name and adding to the <code>pvSpecs</code> slice of type <code>PersistentVolumeSpec</code>. See <a href="https://pkg.go.dev/k8s.io/client-go/listers/core/v1#PersistentVolumeLister">k8s.io/client-go/listers/core/v1.PersistentVolumeLister</a></li>
<li>Retry handling is implemented here while looking up pvName till <code>GetPvDetailsMaxRetries</code> is reached with sleep interval of <code>GetPvDetailsRetryInterval</code> between each retry attempt.</li>
<li>Once <code>pvSpecs</code> slice is populated it constructs a <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/driver#GetVolumeIDsRequest">driver.GetVolumeIDsRequest</a> from the same and then invokes <code>driver.GetVolumeIDs(driver.GetVolumeIDsRequest))</code> to obtain the <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/util/provider/driver#GetVolumeIDsResponse">driver.GetVolumeIDsResponse</a> and retruns <code>driver.GetVolumeIDsResponse.VolumeIDs</code></li>
</ul>
<p>TODO: BUG ? In case the PV is not found or retry limit is reached the slice of volume ids will not have a 1:1 correspondence with slice of PV names passed in.</p>
<pre><code class="language-go">func (o *Options) getVolIDsFromDriver(ctx context.Context, pvNames []string) ([]string, error)
</code></pre>
<h3 id="drainoptionsdoaccountingofpvs"><a class="header" href="#drainoptionsdoaccountingofpvs">drain.Options.doAccountingOfPvs</a></h3>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L509">drain.Options.doAccountingOfPvs</a> returns a map of the pod key <code>pod.Namespace + '/' + pod.Name</code> to a <a href="machine-controller/node_drain.html#drainpodvolumeinfo">PodVolumeInfo</a> struct which holds a slice of PV names and PV IDs.</p>
<p>NOTES:</p>
<ul>
<li>See <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L544">filterSharedPVs</a></li>
</ul>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD
Begin((&quot; &quot;))
--&gt;Init[&quot;
	podKey2VolNamesMap = make(map[string][]string)
	podKey2VolInfoMap = make(map[string]PodVolumeInfo)
&quot;]
--&gt;RangePods[&quot;
	for pod := range pods
&quot;]
--&gt;PopPod2VolNames[&quot;
	podKey2VolNamesMap[pod.Namespace + '/' pod.Name] = o.getPVList(pod)
&quot;]
--loop--&gt;RangePods
PopPod2VolNames--done--&gt;FilterSharedPVs[&quot;
	filterSharedPVs(podKey2VolNamesMap)
// filters out the PVs that are shared among pods.
&quot;]
--&gt;RangePodKey2VolNamesMap[&quot;
	for podKey, volNames := range podKey2VolNamesMap
&quot;]
--&gt;GetVolumeIds[&quot;
	volumeIds, err := o.getVolIDsFromDriver(ctx, volNames)
	if err != nil continue; //skip set of volumes
&quot;]
--&gt;InitPodVolInfo[&quot;
	podVolumeInfo := PodVolumeInfo{
			persistentVolumeList: volNames,
			volumeList:           volumeIds
	}
	//struct field names are bad.
&quot;]
--&gt;PopPodVolInfoMap[&quot;
	podVolumeInfoMap[podKey] = podVolumeInfo
&quot;]
--loop--&gt;RangePodKey2VolNamesMap
PopPodVolInfoMap--done--&gt;Return((&quot;return podVolumeInfoMap&quot;))
</pre>
<h3 id="drainfilterpodswithpv"><a class="header" href="#drainfilterpodswithpv">drain.filterPodsWithPv</a></h3>
<p>NOTE: should have been named <code>partitionPodsWithPVC</code></p>
<p>Utility function that iterates through given <code>pods</code> and for each <code>pod</code>, iterates through its <code>pod.Spec.Volumes</code>. For each such pod <code>volume</code> checks <code>volume.PersistentVolumeClaim</code>. If not nil, adds <code>pod</code> to slice <code>podsWithPV</code> else adds <code>pod</code> to slice <code>podsWithoutPV</code></p>
<pre><code class="language-go">func filterPodsWithPv(pods []corev1.Pod) 
    (podsWithPV []*corev1.Pod, podsWithoutPV []*corev1.Pod) 
</code></pre>
<h3 id="drainoptionswaitfordetach"><a class="header" href="#drainoptionswaitfordetach">drain.Options.waitForDetach</a></h3>
<pre><code class="language-go">func (o *Options) waitForDetach(ctx context.Context, 
	podVolumeInfo PodVolumeInfo, 
	nodeName string) error
</code></pre>
<p>Summary: </p>
<ol>
<li>Initiaze boolean <code>found</code> to true. (Representing that a volume is still attached to a node).</li>
<li>Begins a loop while <code>found</code> is true</li>
<li>Uses a <code>select </code> and checks to see if a signal is received from <code>context.Done()</code> (ie context cancelled). If so, return an error with the message that a timeout occurred while waiting for PV's to be detached.</li>
<li>Sets <code>found</code> to false.</li>
<li>Gets the <code>Node</code> associated with <code>nodeName</code> using the <code>nodeLister</code> and assign to <code>node</code>. If there is an error return from the function.</li>
<li>Gets the <code>node.Status.VolumesAttached</code> which returns a <a href="https://pkg.go.dev/k8s.io/api/core/v1#AttachedVolume">[]AttachedVolume</a> and assign to <code>attachedVols</code>.
<ol>
<li>Return <code>nil</code> if this slice is empty.</li>
</ol>
</li>
<li>Begin iteration <code>range podVolumeInfo.volumeList</code> assigning <code>volumeID</code> in parent iteration. Label this iteration as <code>LookUpVolume</code>.
<ol>
<li>Begin inner iteration over <code>attachedVols</code> assigning <code>attachedVol</code> in nested iteration</li>
<li>If <code>attachedVol.Name</code> is contained in <code>volumeID</code> then this volume is still attached to the node.
<ol>
<li>Set <code>found</code> to true</li>
<li>Sleep for <code>VolumeDetachPollInterval</code> seconds (5 seconds)</li>
<li>Break out of <code>LookUpVolume</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="drainoptionswaitforreattach"><a class="header" href="#drainoptionswaitforreattach">drain.Options.waitForReattach</a></h3>
<p>Purpose: Waits for persistent volume names in <code>podVolumeInfo.persistentVolumeList</code> to be re-attached (to another node).</p>
<p>But I am still confused on why we need to call this during the drain flow. Why should we wait for PVs to be attached to another node. After all, there is no guarantee they will be attached, right ?</p>
<pre><code class="language-go">func (o *Options) waitForReattach(ctx context.Context, 
	podVolumeInfo PodVolumeInfo, 
	previousNodeName string, 
	volumeAttachmentEventCh chan *VolumeAttachment) error 
</code></pre>
<ol>
<li>Construct a map: <code>var pvsWaitingForReattachments map[string]bool</code></li>
<li>Initiamize the above map by ranging through <code>podVolumeInfo.persistentVolumeList</code> taking the <code>persistentVolumeName</code> and set <code>pvsWaitingForReattachments[persistentVolumeName] = true</code></li>
<li>Start a <code>for</code> loop.
<ol>
<li>Commence a <code>select</code> with following cases:
<ol>
<li>Case: Check to see if context is closed/cancelled by  reading: <code>&lt;-ctx.Done()</code>. If so, return an error with the message that timeout occurred while waiting for PV's to reattach to another node.</li>
<li>Case: Obtain a <a href="https://pkg.go.dev/k8s.io/api/storage/v1#VolumeAttachment">*VolumeAttachment</a> by reading from channel:  <code>incomingEvent := &lt;-volumeAttachmentEventCh</code>
<ol>
<li>Get the <code>persistentVolumeName</code> associated with this attachment event. </li>
<li><code>persistentVolumeName := *incomingEvent.Spec.Source.PersistentVolumeName</code></li>
<li>Check if this persistent volume was being tracked: <code>pvsWaitingForReattachments[persistentVolumeName]</code> is present </li>
<li>Check if the volume was attached to another node</li>
<li><code>incomingEvent.Status.Attached &amp;&amp; incomingEvent.Spec.NodeName != previousNodeName</code></li>
<li>If above is true, then delete entry corresponding to <code>persistentVolumeName</code> from the <code>pvsWaitingForReattachments</code> map.</li>
</ol>
</li>
</ol>
</li>
<li>if <code>pvsWaitingForReattachments</code> is empty break from the loop.</li>
</ol>
</li>
<li>Log that the volumes in <code> podVolumeInfo.persistentVolumeList</code> have been successfully re-attached and return nil.</li>
</ol>
<h3 id="drainoptionswaitfordelete"><a class="header" href="#drainoptionswaitfordelete">drain.Options.waitForDelete</a></h3>
<p>NOTE: Ideally should have been named <code>waitForPodDisappearance</code></p>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L1068">pkg/util/provider/drain.Options.waitForDelete</a> is a helper method defined on <code>drain.Options</code> that leverages <a href="machine-controller/./k8s_facilities.html#waitpollimmediate">wait.PollImmediate</a> and the <code>getPodFn</code> (get pod by name and namespace) and checks that all pods have disappeared within <code>timeout</code>. The set of pods that did not disappear within timeout is returned as <code>pendingPods</code></p>
<pre><code class="language-go">func (o *Options) waitForDelete(
        pods []*corev1.Pod, interval, 
        timeout time.Duration,  
        getPodFn func(string, string) (*corev1.Pod, error)
    ) (pendingPods []*corev1.Pod, err error) 
</code></pre>
<h3 id="drainoptionsrundrain"><a class="header" href="#drainoptionsrundrain">drain.Options.RunDrain</a></h3>
<p>Context: <code>drain.Options.RunDrain</code> is called from the MC helper method <code>controller.drainNode</code> which in turn is called from <code>controller.triggerDeletionFlow</code> when the <code>machine.Status.LastOperation.Description</code> contains operation <code>machineutils.InitiateDrain</code>.</p>
<p>If <code>RunDrain</code> returns an error, then the drain is retried at a later time by putting back the machine key into the queue. Unless the <code>force-deletion</code> label on the machine object is true - in which case we proceed to VM deletion.</p>
<pre><code class="language-go">func (o *Options) RunDrain(ctx context.Context) error
</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD


GetNode[&quot;node, err = .client.CoreV1().Nodes().Get(ctx, o.nodeName, metav1.GetOptions{})&quot;]
--&gt;ChkGetNodeErr{err != nil}

ChkGetNodeErr--Yes--&gt;ReturnNilEarly((&quot;return nil
(case where deletion 
triggered during machine creation, 
so node is nil. 
TODO: should use apierrors.NotFound)&quot;))

ChkGetNodeErr--No--&gt;ChkNodeUnschedulable{node.Spec.Unschedulable?}

ChkNodeUnschedulable--Yes--&gt;GetPodsForDeletion[&quot;
    pods, err := o.getPodsForDeletion(ctx)
    if err!=nil return err&quot;]
ChkNodeUnschedulable--No--&gt;CloneNode[&quot;clone := node.DeepCopy()
		clone.Spec.Unschedulable = true&quot;]
        --&gt;UpdateNode[&quot;_, err = o.client.CoreV1().Nodes().Update(ctx, clone, metav1.UpdateOptions{})
        if err != nil return err
        &quot;]

UpdateNode--&gt;GetPodsForDeletion
GetPodsForDeletion--&gt;GetEvictionPGV[&quot;
    policyGroupVersion, err := SupportEviction(o.client)
    if err != nil return err
    &quot;]
--&gt;
DefineAttemptEvict[&quot;
attemptEvict := !o.ForceDeletePods &amp;&amp; len(policyGroupVersion) &gt; 0
// useless boolean which confuses matters considerably.
&quot;]
--&gt;
DefineGetPodFn[&quot;
getPodFn := func(namespace, name string) (*corev1.Pod, error) {
		return o.client.CoreV1().Pods(namespace).Get(ctx, name, metav1.GetOptions{})
}&quot;]
--&gt;
CreateReturnChannel[&quot;
    returnCh := make(chan error, len(pods))
	defer close(returnCh)
    &quot;]
--&gt;ChkForceDelPods{&quot;o.ForceDeletePods?&quot;}

ChkForceDelPods--Yes--&gt;EvictPodsWithoutPV[&quot;go o.evictPodsWithoutPv(ctx, attemptEvict, pods, policyGroupVersion, getPodFn, returnCh)
// go-routine feels un-necessary here.&quot;]
ChkForceDelPods--No--&gt;FilterPodsWithPv[&quot;
podsWithPv, podsWithoutPv := filterPodsWithPv(pods)
&quot;]

FilterPodsWithPv--&gt;EvictPodsWithPv[&quot;
go o.evictPodsWithPv(ctx, attemptEvict, podsWithPv, policyGroupVersion, getPodFn, returnCh)
&quot;]
--&gt;EvictPodsWithoutPV1[&quot;
	go o.evictPodsWithoutPv(ctx, attemptEvict, podsWithoutPv, policyGroupVersion, getPodFn, returnCh)	
&quot;]--&gt;CreateAggregateError

EvictPodsWithoutPV--&gt;CreateAggregateError[&quot;
	var errors []error
    errors =
	for i = 0; i &lt;  len(pods); ++i {
		err := &lt;-returnCh
		if err != nil {
			errors = append(errors, err)
		}
	}
	
&quot;]
--&gt;ReturnAggError((&quot;return\nerrors.NewAggregate(errors)&quot;))

</pre>
<p>Notes:</p>
<ol>
<li><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L1106">machine-controller-manager/pkg/util/provider/drain.SupportEviction</a> uses Discovery API to find out if the server support eviction subresource and if so return its groupVersion or &quot;&quot; if it doesn't.
<ol>
<li><a href="https://pkg.go.dev/k8s.io/kubectl/pkg/drain#CheckEvictionSupport">k8s.io/kubectl/pkg/drain.CheckEvictionSupport</a> already does this.</li>
</ol>
</li>
<li><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L400">attemptEvict boolean</a> usage is confusing. Stick to <code>drain.Options.ForceDeletePods</code></li>
<li>TODO: GAP? For cordoning a Node we currently just set <code>Node.Spec.Unschedulable</code>. But we are also supposed to set the taint. <code>node.kubernetes.io/unschedulable</code>. The spec way is supposed to be deprecated.</li>
</ol>
<h3 id="drainoptionsevictpodswithoutpv"><a class="header" href="#drainoptionsevictpodswithoutpv">drain.Options.evictPodsWithoutPv</a></h3>
<p>drain method that iterates through each given pod and for each pod launches a go-routine that simply delegates to <code>Options.evictPodsWithoutPv</code>.</p>
<pre><code class="language-go">func (o *Options) evictPodsWithoutPv(ctx context.Context, 
    pods []*corev1.Pod,
	policyGroupVersion string, //eviction API's GV
	getPodFn func(namespace, name string) (*corev1.Pod, error),
	returnCh chan error) {
    for _, pod := range pods {
		go o.evictPodWithoutPVInternal(ctx, attemptEvict, pod, policyGroupVersion, getPodFn, returnCh)
	}
	return
}
</code></pre>
<h4 id="drainoptionsevictpodwithoutpvinternal"><a class="header" href="#drainoptionsevictpodwithoutpvinternal">drain.Options.evictPodWithoutPVInternal</a></h4>
<p>drian method that  that either evicts or deletes a Pod with retry handling until <code>Options.MaxEvictRetries</code> is reached.</p>
<pre><code class="language-go">func (o *Options) evictPodWithoutPVInternal(
    ctx context.Context, 
    attemptEvict bool, 
    pod *corev1.Pod, 
    policyGroupVersion string, 
    getPodFn func(namespace, name string) (*corev1.Pod, error), 
    returnCh chan error) 

</code></pre>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD


RangePod[&quot;pod := range pods&quot;]
RangePod--&gt;EvictOrDelPod[&quot;go evictPodWithoutPVInternal(attemptEvict bool, pod, policyGroupVersion,pod,getPodFn,returnCh)&quot;]
EvictOrDelPod--&gt;Begin

subgraph &quot;evictPodWithoutPVInternal (evicts or deletes Pod) &quot;
Begin((&quot;Begin&quot;))--&gt;SetRetry[&quot;retry := 0&quot;]
SetRetry
--&gt;SetAttemptEvict[&quot;if retry &gt;= o.MaxEvictRetries {attemptEvict=false}&quot;]
--&gt;ChkAttemptEvict{&quot;attemptEvict ?&quot;}

ChkAttemptEvict--Yes--&gt;EvictPod[&quot;err=o.evictPod(ctx, pod, policyGroupVersion)&quot;]
ChkAttemptEvict--No--&gt;DelPod[&quot;err=o.deletePod(ctx, pod)&quot;]

EvictPod--&gt;ChkErr
DelPod--&gt;ChkErr

ChkErr{&quot;Check err&quot;}
ChkErr--&quot;Nil&quot;--&gt;ChkForceDelPods
ChkErr--&quot;IsTooManyRequests(err)&quot;--&gt;GetPdb[&quot;
    // Possible case where Pod couldn't be evicted because of PDB violation
    pdbs, err = pdbLister.GetPodPodDisruptionBudgets(pod)
    pdb=pdbs[0] if err !=nil &amp;&amp; len(pdbs) &gt; 0
&quot;]
ChkErr--&quot;IsNotFound(err)\n(pod evicted)&quot;--&gt;SendNilChannel--&gt;NilReturn
ChkErr--&quot;OtherErr&quot;--&gt;SendErrChannel
GetPdb--&gt;ChkMisConfiguredPdb{&quot;isMisconfiguredPdb(pdb)?&quot;}
ChkMisConfiguredPdb--Yes--&gt;SetPdbError[&quot;err=fmt.Errorf('pdb misconfigured')&quot;]
SetPdbError--&gt;SendErrChannel

ChkMisConfiguredPdb--No--&gt;SleepEvictRetryInterval[&quot;time.Sleep(PodEvictionRetryInterval)&quot;]
SleepEvictRetryInterval--&gt;IncRetry[&quot;retry+=1&quot;]--&gt;SetAttemptEvict


SendErrChannel--&gt;NilReturn

ChkForceDelPods{&quot;o.ForceDeletePods&quot;}
ChkForceDelPods--&quot;Yes\n(dont wait for\npod disappear)&quot;--&gt;SendNilChannel
ChkForceDelPods--No--&gt;GetPodTermGracePeriod[&quot;
    // TODO: discuss this, shouldn't pod grace period override drain ?
    timeout=Min(pod.Spec.TerminationGracePeriodSeconds,o.Timeout)
&quot;]
--&gt;SetBufferPeriod[&quot;bufferPeriod := 30 * time.Second&quot;]
--&gt;WaitForDelete[&quot;pendingPods=o.waitForDelete(pods, timeout,getPodFn)&quot;]
--&gt;ChkWaitForDelError{err != nil ?}

ChkWaitForDelError--Yes--&gt;SendErrChannel
ChkWaitForDelError--No--&gt;ChkPendingPodsLength{&quot;len(pendingPods) &gt; 0?&quot;}
ChkPendingPodsLength--Yes--&gt;SetTimeoutError[&quot;err = fmt.Errorf('pod term timeout')&quot;]
SetTimeoutError--&gt;SendErrChannel

ChkPendingPodsLength--No--&gt;SendNilChannel
end

SendNilChannel[&quot;returnCh &lt;- nil&quot;]
SendErrChannel[&quot;returnCh &lt;- err&quot;]
NilReturn((&quot;return&quot;))


</pre>
<h4 id="ismisconfiguredpdb"><a class="header" href="#ismisconfiguredpdb">isMisconfiguredPdb</a></h4>
<p>TODO: Discuss/Elaborate on why this is considered misconfigured.</p>
<pre><code class="language-go">func isMisconfiguredPdbV1(pdb *policyv1.PodDisruptionBudget) bool {
	if pdb.ObjectMeta.Generation != pdb.Status.ObservedGeneration {
		return false
	}

	return pdb.Status.ExpectedPods &gt; 0 &amp;&amp; 
        pdb.Status.CurrentHealthy &gt;= pdb.Status.ExpectedPods
        &amp;&amp; pdb.Status.DisruptionsAllowed == 0
}
</code></pre>
<h3 id="drainoptionsevictpodswithpv"><a class="header" href="#drainoptionsevictpodswithpv">drain.Options.evictPodsWithPv</a></h3>
<pre><code class="language-go">func (o *Options) evictPodsWithPv(ctx context.Context, 
    attemptEvict bool, 
    pods []*corev1.Pod,
	policyGroupVersion string,
	getPodFn func(namespace, name string) (*corev1.Pod, error),
	returnCh chan error)
</code></pre>
<p>NOTE</p>
<ul>
<li>See <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L580">drain.Options.evictPodsWithPv</a></li>
<li>This method basically delegates to <code>o.evictPodsWithPVInternal</code> with retry handling</li>
<li>TODO: Logic of this method can do with some refactoring!</li>
</ul>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;SortPods[&quot;
    sortPodsByPriority(pods)
    //Desc priority: pods[i].Spec.Priority &gt; *pods[j].Spec.Priority
&quot;]
--&gt;DoVolumeAccounting[&quot;
	podVolumeInfoMap := o.doAccountingOfPvs(ctx, pods)
&quot;]
--&gt;ChkAttemptEvict{attemptEvict ?}

ChkAttemptEvict--Yes--&gt;RetryTillLimit[&quot;
	until MaxEvictRetries
&quot;]
--&gt;
InvokeHelper[&quot;
	remainingPods, aborted = o.evictPodsWithPVInternal(ctx, attemptEvict, pods, podVolumeInfoMap, policyGroupVersion,  returnCh)
&quot;]
InvokeHelper--&gt;ChkAbort{&quot;
	aborted ||
	len(remainingPods) == 0
&quot;}
ChkAbort--Yes--&gt;RangeRemainingPods
ChkAbort--No--&gt;Sleep[&quot;
	pods = remainingPods
	time.Sleep(PodEvictionRetryInterval)
&quot;]
Sleep--loop--&gt;RetryTillLimit

RetryTillLimit--loopend--&gt;ChkRemaining{&quot;len(remainingPods) &gt; 0 &amp;&amp; !aborted ?&quot;}
ChkRemaining--Yes--&gt;InvokeHelper1[&quot;
// force delete pods
	remainingPods, _ = o.evictPodsWithPVInternal(ctx, false, pods, podVolumeInfoMap, policyGroupVersion, getPodFn, returnCh)
&quot;]

ChkAttemptEvict--No--&gt;InvokeHelper1
InvokeHelper1--&gt;RangeRemainingPods
ChkRemaining--No--&gt;RangeRemainingPods

RangeRemainingPods[&quot;pod := range remainingPods&quot;]
RangeRemainingPods--aborted?--&gt;SendNil[&quot;returnCh &lt;- nil&quot;]
RangeRemainingPods--attemptEvict?--&gt;SendEvictErr[&quot;returnCh &lt;- fmt.Errorf('pod evict error')&quot;]
RangeRemainingPods--else--&gt;SendDelErr[&quot;returnCh &lt;- fmt.Errorf('pod delete error')&quot;]


SendNil--&gt;NilReturn
SendEvictErr--&gt;NilReturn
SendDelErr--&gt;NilReturn
NilReturn((&quot;return&quot;))
</pre>
<h3 id="drainoptionsevictpodswithpvinternal"><a class="header" href="#drainoptionsevictpodswithpvinternal">drain.Options.evictPodsWithPVInternal</a></h3>
<p>FIXME: name case inconsistency with <code>evictPodsWithPv</code></p>
<p><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L646">drain.Options.evictPodsWithPVInternal</a> is a drain helper method that actually evicts/deletes pods and waits for volume detachment. It returns a <code>remainingPods</code> slice and a <code>fastTrack</code> boolean is meant to abort the pod eviction and exit the calling go-routine. (TODO: should be called <code>abort</code> or even better should use custom error here)</p>
<pre><code class="language-go">func (o *DrainOptions) evictPodsWithPVInternal(ctx context.Context,
    attemptEvict bool, 
    pods []*corev1.Pod, 
    volMap map[string][]string,
	policyGroupVersion string,
	returnCh chan error
    ) (remainingPods []*api.Pod, fastTrack bool)
</code></pre>
<ol>
<li>Uses <code>context.Deadline</code>  passing in <code>ctx</code> and a deadline time after the drain timeout to get a sub-context assigned to <code>mainContext</code> and a <code>CancelFunc</code>. Defer the obtained <code>CancelFunc</code>. (So it always invoked when the method terminates)</li>
<li>Maintain a pod slice <code>retryPods</code> which is initially empty.</li>
<li>Iterate through <code>pods</code> slice with <code>i</code> as index variable 
<ol>
<li>Apply a select with the one check case:
<ol>
<li>Check to see if <code>mainContext</code> is closed/cancelled. Attempt to read from the Done channel: <code>&lt;-mainContext.Done()</code>. If this case matches:
<ol>
<li>Send <code>nil</code> on the return error channel: <code>returnCh &lt;- nil</code></li>
<li>Compute <code>remainingPods</code> as <code>retryPods</code> slice appended with pods yet to be iterated: <code>pods[i+1:]...</code></li>
<li>Return <code>remainingPods, true</code>. (aborted is true)</li>
</ol>
</li>
</ol>
</li>
<li>Initiate the pod eviction start time: <code>podEvictionStartTime=time.Now()</code></li>
<li>Call <code>volumeAttachmentHandler.AddWorker()</code> to start tracking <code>VolumeAttachments</code>and obtain a <code>volumeAttachmentEventCh</code> receive channel that one can use to receive the attached or detached <code>*.VolumeAttachment</code>. </li>
<li>If <code>attemptEvict</code> is true, then call <a href="machine-controller/node_drain.html#drainoptionsevictpod">evictPod</a> else call <a href="machine-controller/node_drain.html#drainoptionsdeletepod">deletePod</a> helper method. Grab the <code>err</code> for eviction/deletion.</li>
<li>eviction/deletion had an error: Analyze the <code>err</code>:
<ol>
<li>If both <code>attemptEvict</code> is true and <code> apierrors.IsTooManyRequests(err)</code> is true, then this case is interpreted as an eviction failure due to PDB violation. 
<ol>
<li>We get the <a href="machine-controller/../k8s_facilities.html#pod-disruption-budget">PodDisruptionBudget</a> for the pod being iterated.</li>
<li>We check whether it is misconfigured. IF So we send an error on <code>returnCh</code> and close the <code>volumeAttachmentEventCh</code> using <code>volumeAttachmentHandler.DeleteWorker</code> and continue with next loop iteration. ie go to next pod.</li>
</ol>
</li>
<li>If just <code>apierrors.IsNotFound(err)</code> is true, this means that Pod is already gone from the node. We send <code>nil</code> on <code>returnCh</code> and call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code> and continue with next pod in iteration.</li>
<li>Otherwise we add the pod to the <code>retryPod</code> slice: <code>retryPods = append(retryPods, pod)</code>,  call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code> and continue with next pod in iteration.</li>
<li>(NOTE: Error handling can be optimized. too much repetition)</li>
</ol>
</li>
<li>Log that the evict/delete was successful.</li>
<li>Get the <a href="machine-controller/node_drain.html#drainpodvolumeinfo">PodVolumeInfo</a> from <code>volMap</code> using the pod key. </li>
<li>Obtain a context and cancellation function for volume detachment using <a href="https://pkg.go.dev/context#WithTimeout">context.Timeout</a> passing in the <code>mainContext</code> and detach timeout computed as the sum of the termination grace period of the pod (<code>pod.Spec.TerminationGracePeriodSeconds</code> if not nil) added to the <code>PvDetachTimeout</code> (from the drain options)</li>
<li>Invoke <a href="machine-controller/node_drain.html#drainoptionswaitfordetach">waitForDetach(ctx, podVolumeInfo, o.nodeName)</a> and grab the <code>err</code>.</li>
<li>Invoke the cancel function for detach. NOTE: THIS IS NICHT GUT.  The sub context should be created INSIDE waitForDetach with a defer for the cancelllation.</li>
<li>Analyze the detachment error.
<ol>
<li>If <code>apierrors.IsNotFound(err)</code> is true this indicates that the node is not found. 
<ol>
<li>Send <code>nil</code> on <code>returnCh</code></li>
<li>Call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code></li>
<li>Compute <code>remainingPods</code> as <code>retryPods</code> slice appended with pods yet to be iterated: <code>pods[i+1:]...</code> </li>
<li>Return <code>remainingPods, true</code>. (aborted is true)</li>
</ol>
</li>
<li>For other errors:
<ol>
<li>Send the <code>err</code> on the <code>returnCh</code>.</li>
<li>Call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code></li>
<li>Continue with next pod in iteration. </li>
</ol>
</li>
</ol>
</li>
<li>Obtain a context and cancellation function for volume re-attachment using <a href="https://pkg.go.dev/context#WithTimeout">context.Timeout</a> passing in the <code>mainContext</code> and <code>drain.Options.PvReattachTimeout</code>.</li>
<li>Invoke <a href="machine-controller/node_drain.html#drainoptionswaitforreattach">waitForReattach(ctx, podVolumeInfo, o.nodeName, volumeAttachmentEventCh)</a> and grab the returned <code>err</code>. </li>
<li>Invoke the cancel function for reattach.  NOTE: THIS IS NICHT GUT. The sub context should be created INSIDE <code>waitForReattach</code>.</li>
<li>Analyze the re-attachment error.
<ol>
<li>If err is a reattachment timeout error just log a warning. TODO: Confused on why we don't return an error on the return channel here.</li>
<li>Otherwise we Send the <code>err</code> on the <code>returnCh</code>.</li>
<li>Call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code> and continue with next pod in iteration</li>
</ol>
</li>
<li>YAWN. Someone is very fond of calling this again and again. Call <code>volumeAttachmentHandler.DeleteWorker(volumeAttachmentEventCh)</code></li>
<li>Log the time taken for pod eviction+vol detachment+vol attachment to another node using <code>time.Since(podEvictionStartTime)</code>.</li>
<li>Send <code>nil</code> on <code>returnCh</code></li>
</ol>
</li>
<li>pod iteration loop is done: <code>return retryPods, false</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="orphan--safety-jobs"><a class="header" href="#orphan--safety-jobs">Orphan / Safety Jobs</a></h1>
<p>Read MCM FAQ: <a href="https://github.com/gardener/machine-controller-manager/blob/master/docs/FAQ.md#what-is-safety-controller-in-mcm">What is Safety Controller in MCM</a></p>
<p>These are jobs that periodically run by pushing dummy keys onto their respective work-queues. The worker then picks up and dispatches to the reconcile functions.</p>
<pre><code class="language-go">    worker.Run(c.machineSafetyOrphanVMsQueue, &quot;ClusterMachineSafetyOrphanVMs&quot;, 
    worker.DefaultMaxRetries, 
    true, c.reconcileClusterMachineSafetyOrphanVMs, stopCh, &amp;waitGroup)

    worker.Run(c.machineSafetyAPIServerQueue, &quot;ClusterMachineAPIServer&quot;, 
    worker.DefaultMaxRetries, true, c.reconcileClusterMachineSafetyAPIServer, 
    stopCh, &amp;waitGroup)

</code></pre>
<h2 id="reconcileclustermachinesafetyorphanvms"><a class="header" href="#reconcileclustermachinesafetyorphanvms">reconcileClusterMachineSafetyOrphanVMs</a></h2>
<p>This techinically isn't a reconcilation loop. It is effectively just a job.</p>
<pre class="mermaid">%%{init: {'themeVariables': { 'fontSize': '10px'}, &quot;flowchart&quot;: {&quot;useMaxWidth&quot;: false }}}%%
flowchart TD

Begin((&quot; &quot;))
--&gt;RescheduleJob[&quot;
// Schedule Rerun
defer c.machineSafetyOrphanVMsQueue.AddAfter('', c.safetyOptions.MachineSafetyOrphanVMsPeriod.Duration)
&quot;]--&gt;
GetMC[&quot;Get Machine Classes and iterate&quot;]
--&gt;forEach[&quot;
asdf
&quot;]
--&gt;ListDriverMachines[&quot;
listMachineResp := driver.ListMachines(...)
&quot;]

ListDriverMachines--&gt;MachinesExist{
    #listMachineResp.MachineList &gt; 0 ?
}
MachinesExist--&gt;|yes| SyncCache[&quot;cache.WaitForCacheSync(stopCh, machineInformer.Informer().HasSynced)&quot;]

SyncCache--&gt;IterMachine[&quot;Iterate machineID,machineName in listMachineResp.MachineList&quot;]
--&gt;GetMachine[&quot;machine, err := Get Machine from Lister&quot;]
--&gt;ChkErr{Check err}

ChkErr--&gt;|err NotFound|DelMachine
ChkErr--&gt;|err nil|ChkMachineId

ChkMachineId{
    machine.Spec.ProviderID == machineID 
    OR
    machinePhase is empty or CrashLoopBackoff ?}
ChkMachineId--&gt;|yes|IterMachine
ChkMachineId--&gt;|no|DelMachine[&quot;
driver.DeleteMachine(ctx, &amp;driver.DeleteMachineRequest{
Machine:      machine, secretData...})
&quot;]

DelMachine--iterDone--&gt;ScheduleRerun
MachinesExist--&gt;|no| ReturnLongRetry[&quot;retryPeriod := machineutils.LongRetry&quot;]

--&gt;ScheduleRerun[&quot;
c.machineSafetyOrphanVMsQueue.AddAfter('', time.Duration(retryPeriod))
&quot;]
</pre>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller-manager/index.html#machine-controller-manager">Machine Controller Manager</a>
<ul>
<li><a href="machine-controller-manager/index.html#mcm-launch">MCM Launch</a></li>
</ul>
</li>
</ul>
<h1 id="machine-controller-manager"><a class="header" href="#machine-controller-manager">Machine Controller Manager</a></h1>
<p>The Machine Controller Manager handles reconciliation of <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineDeployment">MachineDeployment</a> and <a href="https://pkg.go.dev/github.com/gardener/machine-controller-manager@v0.47.0/pkg/apis/machine/v1alpha1#MachineSet">MachineSet</a> objects.</p>
<p>Ideally this should be the called <code>machine-deployment-controller</code> but the current name is a legacy holdover when all controllers were in one project module.</p>
<p>The Machine Controller Manager Entry Point is at <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/cmd/machine-controller-manager/controller_manager.go#L40">github.com/gardener/machine-controller-manager/cmd/machine-controller-manager/controller_manager.go</a></p>
<h2 id="mcm-launch"><a class="header" href="#mcm-launch">MCM Launch</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reconcile-cluster-machine-set"><a class="header" href="#reconcile-cluster-machine-set">Reconcile Cluster Machine Set</a></h1>
<p>A <code>MachineSet</code> is to a <code>Machine</code> in an analogue of what a <code>ReplicaSet</code> is to a <code>Pod</code>. A <code>MachineSet</code> ensures that the specified number of Machines are running at any given time. </p>
<p>A <code>MachineSet</code> is rarely rarely created directly. It is generally owned by its parent MachineDeployment and its ObjectMetadata.OwnerReferenes slice has a reference to the parent deployment.</p>
<p>The MCM controller <code>reconcileClusterMachineSet</code> is called from objects retrieved from the <code>machineSetQueue</code> as shown below.</p>
<pre><code class="language-go">worker.Run(c.machineSetQueue, 
&quot;ClusterMachineSet&quot;, 
worker.DefaultMaxRetries, true, c.reconcileClusterMachineSet, stopCh, &amp;waitGroup)
</code></pre>
<p>The following is the flow diagram for <code>func (c *controller) reconcileClusterMachineSet(key string) error</code> . As can be observed, it could be optimized better. For any error in the below,  the ms key is added back to the <code>machineSetQueue</code> according to the default rate limiting. </p>
<pre class="mermaid">%%{init: { 'themeVariables': { 'fontSize': '11px'},&quot;flowchart&quot;: {&quot;defaultRenderer&quot;: &quot;elk&quot;}} }%%
flowchart TD

Begin((&quot; &quot;))
--&gt;GetMachineSet[&quot;machineSet=Get MS From Lister&quot;]
--&gt;ValidateMS[&quot;validation.ValidateMachineSet(machineSet)&quot;]
--&gt;ChkDeltimestamp1{&quot;machineSet.DeletionTimestamp?&quot;}

ChkDeltimestamp1--&gt;|no| AddFinalizersIMissing[&quot;addFinalizersIfMissing(machineSet)&quot;]
ChkDeltimestamp1--&gt;|yes| GetAllMS[&quot;allMachineSets = list all machine sets&quot;]--&gt;
GetMSSelector[&quot;selector = LabelSelectorAsSelector(machineSet.Spec.Selector)&quot;]
--&gt;ClaimMachines[&quot;claimedMachines=claimMachines(machineSet, selector, allMachines)&quot;]
--&gt;SyncNT[&quot;synchronizeMachineNodeTemplates(claimedMachines, machineSet)&quot;]
--&gt;SyncMC[&quot;syncMachinesConfig(claimedMachines, machineSet)&quot;]
--&gt;SyncMCK[&quot;syncMachinesClassKind(claimedMachines, machineSet)&quot;]
--&gt;ChkDeltimestamp2{&quot;machineSet.DeletionTimestamp?&quot;}--&gt;|no| ScaleUpDown

ChkDeltimestamp2--&gt;|yes| ChkClaimedMachinesLen{&quot;len(claimedMachines) == 0?&quot;}
ChkClaimedMachinesLen--&gt;|yes| DelMSFinalizers[&quot;delFinalizers(machineSet)&quot;]
ChkClaimedMachinesLen--&gt;|no| TermMachines[&quot;terminateMachines(claimedMachines,machineSet)&quot;]--&gt;CalcMSStatus

DelMSFinalizers--&gt;CalcMSStatus
ScaleUpDown[&quot;manageReplicas(claimedMachines) // scale out/in machines&quot;]
--&gt;CalcMSStatus[&quot;calculateMachineSetStatus(claimedMachines, machineSet, errors)&quot;]
--&gt;UpdateMSStatus[&quot;updateMachineSetStatus(...)&quot;]
--&gt;enqueueMachineSetAfter[&quot;machineSetQueue.AddAfter(msKey, 10m)&quot;]

AddFinalizersIMissing--&gt;GetAllMS

</pre>
<h2 id="claimmachines"><a class="header" href="#claimmachines">claimMachines</a></h2>
<p><code>claimMachines</code> tries to take ownership of a machine - it associates a <code>Machine</code> with a <code>MachineSet</code> by setting <code>machine.metadata.OwnerReferences</code> and releasets the <code>Machine</code> if the MS's deletion timestamp has been set.</p>
<ol>
<li>Initialize an empty <code>claimedMachines []Machine</code> slice</li>
<li>Initialize an empty <code>errlist []erro</code></li>
<li>Iterate through <code>allMachines</code> and Get the <code>ownerRef</code>(the first element in <code>OwnerReferences</code> slice)</li>
<li>If the <code>ownerRef</code> is not <code>nil</code>
<ol>
<li>if the <code>ownerRef.UID</code> is diff from the <code>machineSet</code>s <code>UUID</code> skip the claim and continue. (Since the machine belongs to another machine set)</li>
<li>If the machine <code>selector</code> matches the labels of the <code>machineSet</code>, add to <code>claimedMachines</code> and continue</li>
<li>If the <code>machineSet.DeletionTimestamp</code> is set, skip and continue</li>
<li>Release the <code>Machine</code> by removing its <code>ownerReference</code></li>
</ol>
</li>
<li>If the <code>ownerRef</code> is <code>nil</code>
<ol>
<li>If the <code>machineSet.DeletionTimestamp</code> is set or if the machine <code>selector</code> does not mach the <code>machineSet</code>, skip and continue.</li>
<li>If the <code>machine.DeletionTimestamp</code> is set, skip and continue.</li>
<li>Adopt the machine, ie. set the <code>ownerReference</code> to the <code>machineSet</code> and add to <code>claimedMachines</code></li>
</ol>
<pre><code>ownerReferences:
 - apiVersion: machine.sapcloud.io/v1alpha1
   blockOwnerDeletion: true
   controller: true
   kind: MachineSet
   name: shoot--i034796--aw2-a-z1-8c99f
   uid: 20bc03c5-e95b-4df5-9faf-68be38cb8e1b
</code></pre>
</li>
<li>Returned <code>claimedMachines</code>.</li>
</ol>
<h2 id="synchronizemachinenodetemplates"><a class="header" href="#synchronizemachinenodetemplates">synchronizeMachineNodeTemplates</a></h2>
<pre><code class="language-go">func (c *controller) syncMachinesNodeTemplates(ctx context.Context, 
 claimedMachines []*Machine, machineSet *MachineSet) error 
</code></pre>
<ol>
<li>This iterates through the <code>claimeMachines</code> and copies the <code>machineset.Spec.Template.Spec.NodeTemplateSpec</code> to the <code>machine.Spec.NodeTemplateSpec</code></li>
<li>NOTE: Seems useless IO busy-work to me. When MC launches the <code>Machine</code>, it might as well access the owning <code>MachineSet</code> and get the <code>NodeSpec</code>.</li>
<li>The only reason to do this is to support independent <code>Machines</code> without owning <code>MachineSets</code>. We will need to see whether such a use-case is truly needed.</li>
</ol>
<p>NOTE: <code>NodeTemplate</code> describes common resource capabilities like <code>cpu</code>, <code>gpu</code>, <code>memory</code>, etc in terms of <a href="https://pkg.go.dev/k8s.io/api/core/v1#ResourceList">k8s.io/api/core/v1.ResourceList</a>. This is used by the <code>cluster-autoscaler</code> for scaling decisions. </p>
<h2 id="syncmachinesconfig"><a class="header" href="#syncmachinesconfig">syncMachinesConfig</a></h2>
<p>Copies  <code>machineset.Spec.Template.Spec.MachineConfiguration</code> to  <code>machine.Spec.MachineConfiguration</code> for all <code>claimedMachines</code>.</p>
<p>See <code>MachineConfiguration</code> inside <a href="http://localhost:3000/mcm_facilities.html#machinespec">MachineSpec</a></p>
<h2 id="syncmachinesclasskind"><a class="header" href="#syncmachinesclasskind">syncMachinesClassKind</a></h2>
<p>NOTE: This is useless and should be removed since we only have ONE kind of <code>MachineClass</code>. TODO: Discuss with Himanshu/Rishabh.</p>
<pre><code class="language-go">func (c *controller) syncMachinesClassKind(ctx context.Context, 
    claimedMachines []*Machine, machineSet *MachineSet) error 
</code></pre>
<p>Iterates through <code>claimedMachines</code> and sets <code>machine.Spec.Class.Kind = machineset.Spec.Template.Spec.Class.Kind</code> if not already set.</p>
<h2 id="managereplicas-scale-out--scale-in"><a class="header" href="#managereplicas-scale-out--scale-in">manageReplicas (scale-out / scale-in)</a></h2>
<pre><code class="language-go">func (c *controller) manageReplicas(ctx context.Context, 
    claimedMachines []Machine, machineSet *MachineSet) error
</code></pre>
<pre class="mermaid">%%{init: { 'themeVariables': { 'fontSize': '11px'},&quot;flowchart&quot;: {&quot;defaultRenderer&quot;: &quot;elk&quot;}} }%%
flowchart TD

Begin((&quot; &quot;))
--&gt;Init[&quot;activeMachines :=[], staleMachines:=[]&quot;]
--&gt;IterCLaimed[&quot;machine := range claimedMachines&quot;]
--loop--&gt;IsActiveOrFailed{&quot;IsMachineActiveOrFailed(machine)&quot;}

IsActiveOrFailed--&gt;|active| AppendActive[&quot;append(activeMachines,machine)&quot;]
IsActiveOrFailed--&gt;|failed| AppendFailed[&quot;append(staleMachines,machine)&quot;]

IterCLaimed--done--&gt;TermStaleMachines[&quot;terminateMachines(staleMachines,machineSet)&quot;]

TermStaleMachines--&gt;Delta[&quot;diff := len(activeMachines) - machineSet.Spec.Replicas&quot;]
Delta--&gt;ChkDelta{&quot;diff &lt; 0?&quot;}

ChkDelta--&gt;|yes| ScaleOut[&quot;numCreated:=slowStartBatch(-diff,..) // scale out&quot;]
ScaleOut--&gt;Log[&quot;Log numCreated/skipped/deleted&quot;]
ChkDelta--&gt;|no| GetMachinesToDelete[&quot;machinesToDel := getMachinesToDelete(activeMachines, diff)&quot;]
GetMachinesToDelete--&gt;TermMachines[&quot;terminateMachines(machinesToDel, machineSet)&quot;]
--&gt;Log--&gt;ReturnErr[&quot;return err&quot;]
</pre>
<h3 id="terminatemachines"><a class="header" href="#terminatemachines">terminateMachines</a></h3>
<pre><code class="language-go">func (c *controller) terminateMachines(ctx context.Context, 
    inactiveMachines []*Machine, machineSet *MachineSet) error {
</code></pre>
<ol>
<li>Invokes <code>controlMachineClient.Machines(namespace).Delete(ctx, machineID,..)</code> for each <code>Machine</code> in <code>inactiveMachines</code> and records an event.</li>
<li>The <code>machine.Status.Phase</code> is also set to <code>Terminating</code>.</li>
<li>This is done in parallel using <code>go-routines</code> a <code>WaitGroup</code> on length of <code>inactiveMachines</code> </li>
</ol>
<h3 id="slowstartbatch"><a class="header" href="#slowstartbatch">slowStartBatch</a></h3>
<pre><code class="language-go">func slowStartBatch(count int, initialBatchSize int, createFn func() error) (int, error)
</code></pre>
<ol>
<li>Initializes <code>remaining</code> to <code>count</code> and <code>successes</code> as <code>0</code>.</li>
<li>Method executes <code>fn</code> (which creates a <code>Machine</code> object) in parallel with number of go-routines starting with <code>batchSize := initialBatchSize</code> and then doubling <code>batchSize</code> size after the call to <code>fn</code>. 
<ol>
<li>For each batch iteration, a <code>wg sync.WaitGroup</code> is constructed with <code>batchSize</code>. Each batch execution waits for batch to be complete using <code>wg.Wait()</code></li>
<li>For each batch iteration, an <code>errCh</code> is constructed with size as  <code>batchSize</code> </li>
<li><code>batchSize</code> go-routines execute <code>fn</code> concurrently, sending errors on <code>errCh</code> and invoking <code>wg.Done()</code> when complete.</li>
<li><code>numErrorsInBatch = len(errCh)</code></li>
<li><code>successes</code> is <code>batchSize</code> minus <code>numErrorsInBatch</code></li>
<li>if <code>numErrorsInBatch &gt; 0</code>, abort, returning <code>successes</code> and first error from <code>errCh</code></li>
<li><code>remaining</code> is decremented by the <code>batchSize</code></li>
<li>Compute <code>batchSize</code> as <code>Min(remaining, 2*batchSize)</code></li>
<li>Continue iteration while <code>batchSize</code> is greater than <code>0</code>.</li>
<li>Return <code>successes, nil</code> when done.</li>
</ol>
</li>
<li><code>fn</code> is a lambda that creates a new <code>Machine</code> in which we do the below:
<ol>
<li>Create an <code>ownerRef</code> with the <code>machineSet.Name</code> and <code>machineSet.UID</code></li>
<li>Get the machine spec template using <a href="http://localhost:3000/mcm_facilities.html#machineset">machineSet.Spec.Template</a></li>
<li>Then create a <code>Machine</code> obj setting the machine spec and <code>ownerRef</code>. Use the <code>machineSet</code> name as the prefix for <code>GenerateName</code> in the <code>ObjectMeta</code>.</li>
<li>If any <code>err</code> return the same or <code>nil</code> if no error.</li>
<li>New <code>Machine</code> objects are persisted using <code>controlMachineClient.Machines(namespace).Create(ctx, machine, createOpts)</code></li>
</ol>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#reconcile-cluster-machine-deployment">Reconcile Cluster Machine Deployment</a>
<ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#rollout-rolling">Rollout Rolling</a>
<ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#1-get-new-machine-set-corresponding-to-machine-deployment-and-old-machine-sets">1. Get new machine set corresponding to machine deployment and old machine sets</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#2-taint-the-nodes-backing-the-old-machine-sets">2. Taint the nodes backing the old machine sets.</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#3-add-autoscaler-scale-down-annotations-to-nodes-of-old-machine-sets">3. Add AutoScaler Scale-Down annotations to Nodes of Old Machine Sets</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#4-reconcile-new-machine-set-by-calling-reconcilenewmachineset">4. Reconcile New Machine Set by calling <code>reconcileNewMachineSet</code></a></li>
</ul>
</li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#helper-methods">Helper Methods</a>
<ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#scalemachineset">scaleMachineSet</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#get-machine-sets-for-machine-deployment">Get Machine Sets for Machine Deployment</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#get-machine-map-for-machine-deployment">Get Machine Map for Machine Deployment</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#terminate-machine-sets-of-machine-edeployment">Terminate Machine Sets of Machine eDeployment</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#sync-deployment-status">Sync Deployment Status</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#gets-new-and-old-machinesets-and-sync-revision">Gets New and Old MachineSets and Sync Revision</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#overview">Overview</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#detail">Detail</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#annotate-nodes-backing-machine-sets">Annotate Nodes Backing Machine Sets</a></li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#claim-machines">Claim Machines</a>
<ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#helper-functions">Helper Functions</a>
<ul>
<li><a href="machine-controller-manager/reconcile-cluster-machine-deployment.html#compute-new-machine-set-new-replicas">Compute New Machine Set New Replicas</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="reconcile-cluster-machine-deployment"><a class="header" href="#reconcile-cluster-machine-deployment">Reconcile Cluster Machine Deployment</a></h1>
<pre><code class="language-go">func (dc *controller) reconcileClusterMachineDeployment(key string) error 
</code></pre>
<ul>
<li>Gets the deployment name.</li>
<li>Gets the <code>MachineDeployment</code></li>
<li>TODO: WEIRD: freeze labels and deletion timestamp</li>
<li>TODO: unclear why we do this</li>
</ul>
<pre><code class="language-go">	// Resync the MachineDeployment after 10 minutes to avoid missing out on missed out events
	defer dc.enqueueMachineDeploymentAfter(deployment, 10*time.Minute)
</code></pre>
<ul>
<li>Add finalizers if deletion time stamp is nil</li>
<li>TODO: Why is observed generation only updated conditionally in the below ? Shouldn't it be done always </li>
</ul>
<pre><code class="language-go">everything := metav1.LabelSelector{}
	if reflect.DeepEqual(d.Spec.Selector, &amp;everything) {
		dc.recorder.Eventf(d, v1.EventTypeWarning, &quot;SelectingAll&quot;, &quot;This deployment is selecting all machines. A non-empty selector is required.&quot;)
		if d.Status.ObservedGeneration &lt; d.Generation {
			d.Status.ObservedGeneration = d.Generation
			dc.controlMachineClient.MachineDeployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{})
		}
		return nil
	}
</code></pre>
<ul>
<li>Get <code>[]*v1alpha1.MachineSet</code> for this deployment using <code>getMachineSetsForMachineDeployment</code> and assign to <code>machineSets</code></li>
<li>if <code>deployment.DeletionTimestamp != nil</code> 
<ul>
<li>if there are no finalizers on deployment return nil</li>
<li>if <code>len(machineSets) == 0</code> delete the machine deployment finalizers and return nil</li>
<li>Call <code>dc.terminateMachineSets(ctx, machineSets)</code></li>
</ul>
</li>
</ul>
<h2 id="rollout-rolling"><a class="header" href="#rollout-rolling">Rollout Rolling</a></h2>
<pre><code class="language-go">func (dc *controller) rolloutRolling(ctx context.Context, 
    d *v1alpha1.MachineDeployment, 
    msList []*v1alpha1.MachineSet, 
    machineMap map[types.UID]*v1alpha1.MachineList) error 
</code></pre>
<h3 id="1-get-new-machine-set-corresponding-to-machine-deployment-and-old-machine-sets"><a class="header" href="#1-get-new-machine-set-corresponding-to-machine-deployment-and-old-machine-sets">1. Get new machine set corresponding to machine deployment and old machine sets</a></h3>
<pre><code class="language-go">newMS, oldMSs, err := dc.getAllMachineSetsAndSyncRevision(ctx, d, msList, machineMap, true)
allMSs := append(oldMSs, newMS)
</code></pre>
<h3 id="2-taint-the-nodes-backing-the-old-machine-sets"><a class="header" href="#2-taint-the-nodes-backing-the-old-machine-sets">2. Taint the nodes backing the old machine sets.</a></h3>
<p>This is a preference - the k8s scheduler will try to avoid placing a pod that does not tolerate thee taint on the node. Q: Why don't we use <code>NoSchedule</code> instead ? Any pods scheduled on this node will need to be drained - more work to be done.</p>
<pre><code class="language-go">dc.taintNodesBackingMachineSets(
		ctx,
		oldISs, &amp;v1.Taint{
			Key:    PreferNoScheduleKey,
			Value:  &quot;True&quot;,
			Effect: &quot;PreferNoSchedule&quot;,
		},
	)
</code></pre>
<h3 id="3-add-autoscaler-scale-down-annotations-to-nodes-of-old-machine-sets"><a class="header" href="#3-add-autoscaler-scale-down-annotations-to-nodes-of-old-machine-sets">3. Add AutoScaler Scale-Down annotations to Nodes of Old Machine Sets</a></h3>
<ol>
<li>Create the map. (TODO: Q: Why do we add 2 annotations ?)
<pre><code class="language-go"> clusterAutoscalerScaleDownAnnotations := make(map[string]string)
 clusterAutoscalerScaleDownAnnotations[&quot;cluster-autoscaler.kubernetes.io/scale-down-disabled&quot;]=&quot;true&quot;
 clusterAutoscalerScaleDownAnnotations[&quot;cluster-autoscaler.kubernetes.io/scale-down-disabled-by-mcm&quot;]=&quot;true&quot;
</code></pre>
</li>
<li>Call <code>annotateNodesBackingMachineSets(ctx, allMSs, clusterAutoscalerScaleDownAnnotations)</code></li>
</ol>
<h3 id="4-reconcile-new-machine-set-by-calling-reconcilenewmachineset"><a class="header" href="#4-reconcile-new-machine-set-by-calling-reconcilenewmachineset">4. Reconcile New Machine Set by calling <code>reconcileNewMachineSet</code></a></h3>
<pre><code class="language-go">	scaledUp, err := dc.reconcileNewMachineSet(ctx, allISs, newIS, d)
</code></pre>
<pre><code class="language-go">func (dc *controller) reconcileNewMachineSet(ctx context.Context, 
allMSs[]*v1alpha1.MachineSet, 
newMS *v1alpha1.MachineSet, 
deployment *v1alpha1.MachineDeployment) 
    (bool, error) 
</code></pre>
<ol>
<li>if <code>newMS.Spec.Replicates == deployment.spec.Replicates</code> return</li>
<li>if <code>newMS.Spec.Replicas &gt; deployment.Spec.Replicas</code>, we need to scale down. call <code>dc.scaleMachineSet(ctx, newMS, deployment.Spec.Replicas, &quot;down&quot;)</code></li>
<li>Compute <code>newReplicasCount</code> using <code>NewMSNewReplicas(deployment, allMSs, newMS)</code>.</li>
<li>Call <code>dc.scaleMachineSet(ctx, newMS, newReplicasCount, &quot;up&quot;)</code></li>
</ol>
<h2 id="helper-methods"><a class="header" href="#helper-methods">Helper Methods</a></h2>
<h3 id="scalemachineset"><a class="header" href="#scalemachineset">scaleMachineSet</a></h3>
<pre><code class="language-go">func (dc *controller) scaleMachineSet(ctx context.Context, 
    ms *v1alpha1.MachineSet, 
    newScale int32, 
    deployment *v1alpha1.MachineDeployment, 
    scalingOperation string) 
        (bool, *v1alpha1.MachineSet, error) {
sizeNeedsUpdate := (ms.Spec.Replicas) != newScale
}
</code></pre>
<p>TODO: fill me in.</p>
<h3 id="get-machine-sets-for-machine-deployment"><a class="header" href="#get-machine-sets-for-machine-deployment">Get Machine Sets for Machine Deployment</a></h3>
<pre><code class="language-go">func (dc *controller) getMachineSetsForMachineDeployment(ctx context.Context, 
        d *v1alpha1.MachineDeployment) 
    ([]*v1alpha1.MachineSet, error) 
</code></pre>
<ul>
<li>Get all machine sets using machine set lister.</li>
<li><code>NewMachineSetControllerRefManager</code> unclear</li>
</ul>
<h3 id="get-machine-map-for-machine-deployment"><a class="header" href="#get-machine-map-for-machine-deployment">Get Machine Map for Machine Deployment</a></h3>
<p>Returns a map from MachineSet UID to a list of Machines controlled by that MS, according to the Machine's ControllerRef.</p>
<pre><code class="language-go">func (dc *controller) 
    getMachineMapForMachineDeployment(d *v1alpha1.MachineDeployment, 
        machineSets []*v1alpha1.MachineSet) 
     (map[types.UID]*v1alpha1.MachineList, error) {
</code></pre>
<h3 id="terminate-machine-sets-of-machine-edeployment"><a class="header" href="#terminate-machine-sets-of-machine-edeployment">Terminate Machine Sets of Machine eDeployment</a></h3>
<pre><code class="language-go">
func (dc *controller) terminateMachineSets(ctx context.Context, machineSets []*v1alpha1.MachineSet) 

</code></pre>
<h3 id="sync-deployment-status"><a class="header" href="#sync-deployment-status">Sync Deployment Status</a></h3>
<pre><code class="language-go">func (dc *controller) syncStatusOnly(ctx context.Context, 
    d *v1alpha1.MachineDeployment, 
    msList []*v1alpha1.MachineSet, 
    machineMap map[types.UID]*v1alpha1.MachineList) error 
</code></pre>
<h3 id="gets-new-and-old-machinesets-and-sync-revision"><a class="header" href="#gets-new-and-old-machinesets-and-sync-revision">Gets New and Old MachineSets and Sync Revision</a></h3>
<pre><code class="language-go">func (dc *controller) getAllMachineSetsAndSyncRevision(ctx context.Context, 
    d *v1alpha1.MachineDeployment, 
    msList []*v1alpha1.MachineSet, 
    machineMap map[types.UID]*v1alpha1.MachineList, 
    createIfNotExisted bool) 
        (*v1alpha1.MachineSet, []*v1alpha1.MachineSet, error) 
</code></pre>
<h3 id="overview"><a class="header" href="#overview">Overview</a></h3>
<p><code>getAllMachineSetsAndSyncRevision</code> does the following:</p>
<ol>
<li>Get all old <code>MachineSets</code> the <code>MachineDeployment:</code> <code>d</code> targets, and calculate the max revision number among them (<code>maxOldV</code>).</li>
<li>Get new <code>MachineSet</code> this deployment targets ie whose machine template matches the deployment's and updates new machine set's revision number to (<code>maxOldV + 1</code>),
This is done only if its revision number is smaller than <code>(maxOldV + 1)</code>.  If this step failed, we'll update it in the next deployment sync loop.</li>
<li>Copy new <code>MachineSet</code>'s revision number to the <code>MachineDeployment</code> (update deployment's revision). If this step failed, we'll update it in the next deployment sync loop. </li>
</ol>
<h3 id="detail"><a class="header" href="#detail">Detail</a></h3>
<ul>
<li>TODO: describe me</li>
</ul>
<h3 id="annotate-nodes-backing-machine-sets"><a class="header" href="#annotate-nodes-backing-machine-sets">Annotate Nodes Backing Machine Sets</a></h3>
<pre><code class="language-go">func (dc *controller) annotateNodesBackingMachineSets(
    ctx context.Context, 
    machineSets []*v1alpha1.MachineSet, 
    annotations map[string]string) error 
</code></pre>
<ol>
<li>Iterate through the <code>machineSets</code>. Loop variable: <code>machineSet</code></li>
<li>List all the machines. TODO: EXPENSIVE ??</li>
</ol>
<pre><code class="language-go">allMachines, err := dc.machineLister.List(labels.Everything())
</code></pre>
<ol start="3">
<li>Get the Selector for the Machine Set</li>
</ol>
<pre><code class="language-go">   	selector, err := metav1.LabelSelectorAsSelector(machineSet.Spec.Selector)
</code></pre>
<ol start="4">
<li>Claim the Machines for the given <code>machineSet</code> using the selector</li>
</ol>
<pre><code class="language-go">    filteredMachines, err = dc.claimMachines(ctx, machineSet, selector, allMachines)
</code></pre>
<ol start="5">
<li>Iterate through <code>filteredMachines</code>, loop variable: <code>machine</code> and if <code>Node</code> is not empty, add or update annotations on node.</li>
</ol>
<pre><code class="language-go">if machine.Status.Node != &quot;&quot; {
err = AddOrUpdateAnnotationOnNode(
    ctx,
    dc.targetCoreClient,
    machine.Status.Node,
    annotations,
    )
}
</code></pre>
<h3 id="claim-machines"><a class="header" href="#claim-machines">Claim Machines</a></h3>
<p>Basically sets or unsets the owner reference of the machines matching selector to the deployment controller.</p>
<pre><code class="language-go">func (c *controller) claimMachines(ctx context.Context, 
    machineSet *v1alpha1.MachineSet, 
    selector labels.Selector, 
    allMachines []*v1alpha1.Machine) 
    ([]*v1alpha1.Machine, error) {
</code></pre>
<p>TODO: delegates to <code>MachineControllerRefManager.claimMachines</code></p>
<p>Sets or un-sets the owner reference of the machine object to the deployment controller. </p>
<h4 id="summary-2"><a class="header" href="#summary-2">Summary</a></h4>
<ul>
<li>iterates through <code>allMachines</code>. Checks if <code>selector</code> matches the machine labels: <code>m.Selector.Matches(labels.Set(machine.Labels)</code></li>
<li>Gets the <code>controllerRef</code> of the machine using <code>metav1.GetControllerOf(machine)</code></li>
<li>If <code>controllerRef</code> is not nil and  the <code>controllerRef.UID</code> matches the </li>
<li>If so, then this is an adoption and calls <code>AdoptMachine</code> which patches the machines owner reference using the below:</li>
</ul>
<pre><code class="language-go">addControllerPatch := fmt.Sprintf(
		`{&quot;metadata&quot;:{&quot;ownerReferences&quot;:[{&quot;apiVersion&quot;:&quot;machine.sapcloud.io/v1alpha1&quot;,&quot;kind&quot;:&quot;%s&quot;,&quot;name&quot;:&quot;%s&quot;,&quot;uid&quot;:&quot;%s&quot;,&quot;controller&quot;:true,&quot;blockOwnerDeletion&quot;:true}],&quot;uid&quot;:&quot;%s&quot;}}`,
		m.controllerKind.Kind,
		m.Controller.GetName(), m.Controller.GetUID(), machine.UID)
	err := m.machineControl.PatchMachine(ctx, machine.Namespace, machine.Name, []byte(addControllerPatch))
err := m.machineControl.PatchMachine(ctx, machine.Namespace, machine.Name, []byte(addControllerPatch))
</code></pre>
<h2 id="helper-functions"><a class="header" href="#helper-functions">Helper Functions</a></h2>
<h3 id="compute-new-machine-set-new-replicas"><a class="header" href="#compute-new-machine-set-new-replicas">Compute New Machine Set New Replicas</a></h3>
<p><code>NewMSNewReplicas</code> calculates the number of replicas a deployment's new machine set <em>should</em> have.</p>
<ol>
<li>The new MS is saturated: newMS's replicas == deployment's replicas</li>
<li>Max number of machines allowed is reached: deployment's replicas + maxSurge == allMS's replicas</li>
</ol>
<pre><code class="language-go">func NewMSNewReplicas(deployment *v1alpha1.MachineDeployment, 
    allMSs []*v1alpha1.MachineSet, 
    newMS *v1alpha1.MachineSet) (int32, error) 
    // MS was called IS earlier (instance set)
</code></pre>
<ol>
<li>Get the <code>maxSurge</code></li>
</ol>
<pre><code class="language-go">maxSurge, err = intstr.GetValueFromIntOrPercent(
    deployment.Spec.Strategy.RollingUpdate.MaxSurge,
    int(deployment.Spec.Replicas),
    true
)
</code></pre>
<ol start="2">
<li>Compute the <code>currentMachineCount</code>: iterate through all machine sets and sum up <code>machineset.Status.Replicas</code></li>
<li><code>maxTotalMachines = deployment.Spec.Replicas + maxSurge</code></li>
<li><code>if currentMachineCount &gt;= maxTotalMachines return newMS.Spec.Replicas</code> // cannot scale up.</li>
<li>Compute	<code>scaleUpCount := maxTotalMachines - currentMachineCount</code></li>
<li>Make sure <code>scaleUpCount</code> does not exceed desired deployment replicas	<code>scaleUpCount = int32(integer.IntMin(int(scaleUpCount), int(deployment.Spec.Replicas -newMS.Spec.Replicas))</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>üöß WIP at the moment. Lot more material to be added here from notes. Please do not read presently.</p>
<ul>
<li><a href="issues.html#issues">Issues</a>
<ul>
<li><a href="issues.html#design-issues">Design Issues</a>
<ul>
<li><a href="issues.html#bad-packaging">Bad Packaging</a></li>
<li><a href="issues.html#lastoperation-is-actually-next-operation">LastOperation is actually Next Operation</a></li>
<li><a href="issues.html#description-misused">Description misused</a></li>
</ul>
</li>
<li><a href="issues.html#gaps">Gaps</a>
<ul>
<li><a href="issues.html#deaddeprecated-code">Dead/Deprecated Code</a>
<ul>
<li><a href="issues.html#controllertriggerupdationflow">controller.triggerUpdationFlow</a>
<ul>
<li><a href="issues.html#safetyoptionsmachinedraintimeout">SafetyOptions.MachineDrainTimeout</a></li>
</ul>
</li>
<li><a href="issues.html#dup-code">Dup Code</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="issues.html#drainnode-handling">drainNode Handling</a></li>
<li><a href="issues.html#node-conditions">Node Conditions</a></li>
<li><a href="issues.html#volumeattachment">VolumeAttachment</a>
- <a href="issues.html#dead-reconcileclusternodekey">Dead? reconcileClusterNodeKey</a>
<ul>
<li><a href="issues.html#dead-machinego--triggerupdationflow">Dead? machine.go | triggerUpdationFlow</a></li>
</ul>
</li>
<li><a href="issues.html#duplicate-initialization-of-eventrecorder-in-mc">Duplicate Initialization of EventRecorder in MC</a>
<ul>
<li><a href="issues.html#q-internal-to-external-scheme-conversion">Q? Internal to External Scheme Conversion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="issues"><a class="header" href="#issues">Issues</a></h1>
<p>This section is very basic WIP atm. Please check after this warning has been removed. Lots more to be added here from notes and appropriately structured.</p>
<h2 id="design-issues"><a class="header" href="#design-issues">Design Issues</a></h2>
<h3 id="bad-packaging"><a class="header" href="#bad-packaging">Bad Packaging</a></h3>
<ul>
<li><code>package controller</code> is inside import path <code>github.com/gardener/machine-controller-manager/pkg/util/provider/machinecontroller</code></li>
</ul>
<h3 id="lastoperation-is-actually-next-operation"><a class="header" href="#lastoperation-is-actually-next-operation">LastOperation is actually Next Operation</a></h3>
<p>Badly named. TODO: Describe more.</p>
<h3 id="description-misused"><a class="header" href="#description-misused">Description misused</a></h3>
<p>Error Prone stuff like below due to misuse of description.</p>
<pre><code class="language-go">// isMachineStatusSimilar checks if the status of 2 machines is similar or not.
func isMachineStatusSimilar(s1, s2 v1alpha1.MachineStatus) bool {
	s1Copy, s2Copy := s1.DeepCopy(), s2.DeepCopy()
	tolerateTimeDiff := 30 * time.Minute

	// Since lastOperation hasn't been updated in the last 30minutes, force update this.
	if (s1.LastOperation.LastUpdateTime.Time.Before(time.Now().Add(tolerateTimeDiff * -1))) || (s2.LastOperation.LastUpdateTime.Time.Before(time.Now().Add(tolerateTimeDiff * -1))) {
		return false
	}

	if utilstrings.StringSimilarityRatio(s1Copy.LastOperation.Description, s2Copy.LastOperation.Description) &gt; 0.75 {
		// If strings are similar, ignore comparison
		// This occurs when cloud provider errors repeats with different request IDs
		s1Copy.LastOperation.Description, s2Copy.LastOperation.Description = &quot;&quot;, &quot;&quot;
	}

	// Avoiding timestamp comparison
	s1Copy.LastOperation.LastUpdateTime, s2Copy.LastOperation.LastUpdateTime = metav1.Time{}, metav1.Time{}
	s1Copy.CurrentStatus.LastUpdateTime, s2Copy.CurrentStatus.LastUpdateTime = metav1.Time{}, metav1.Time{}

	return apiequality.Semantic.DeepEqual(s1Copy.LastOperation, s2Copy.LastOperation) &amp;&amp; apiequality.Semantic.DeepEqual(s1Copy.CurrentStatus, s2Copy.CurrentStatus)
}

</code></pre>
<h2 id="gaps"><a class="header" href="#gaps">Gaps</a></h2>
<p>TODO: Not comprehensive. Lots more to be added here</p>
<h3 id="deaddeprecated-code"><a class="header" href="#deaddeprecated-code">Dead/Deprecated Code</a></h3>
<h4 id="controllertriggerupdationflow-1"><a class="header" href="#controllertriggerupdationflow-1">controller.triggerUpdationFlow</a></h4>
<p>This is unused and appears to be dead code.</p>
<h5 id="safetyoptionsmachinedraintimeout"><a class="header" href="#safetyoptionsmachinedraintimeout">SafetyOptions.MachineDrainTimeout</a></h5>
<p>This field is commented as deprecated but is still in <code>MCServer.AddFlags</code> and in the launch script of individual providers.
Ex</p>
<pre><code>go run
cmd/machine-controller/main.go
...
machine-drain-timeout=5m
</code></pre>
<h4 id="dup-code"><a class="header" href="#dup-code">Dup Code</a></h4>
<ul>
<li>Nearly all files in <code>pkg/controller/*.go</code></li>
<li>Ex: Types/func/smethods in <code>pkg/controller/machine_util.go</code>
<ul>
<li>Ex: Dup <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/controller/machine_util.go#L48">NodeTerminationCondition</a> in <code>pkg/controller/machine_util.go</code>. The one that is being actively used is <a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/machineutils/utils.go#L70">machineutils.NodeTerminationCondition</a></li>
</ul>
</li>
<li>Types/funcs/methods in <code>pkg/controller/drain.go</code> </li>
</ul>
<h2 id="drainnode-handling"><a class="header" href="#drainnode-handling">drainNode Handling</a></h2>
<ol>
<li>Does not set err when <code>c.machineStatusUpdate</code> is called</li>
<li><code>o.RunCordonOrUncordon</code> should use <code>apierrors.NotFound</code> while checking error returned by a get node op</li>
<li><a href="https://github.com/gardener/machine-controller-manager/blob/v0.47.0/pkg/util/provider/drain/drain.go#L400">attemptEvict bool usage</a> is confusing. Better design needed. <code>attemptEvict</code> is overridden in  <code>evictPodsWithoutPv</code>.</li>
<li>Misleading deep copy in <code>drain.Options.doAccountingOfPvs</code>
<pre><code class="language-go">for podKey, persistentVolumeList := range pvMap {
 	persistentVolumeListDeepCopy := persistentVolumeList
 	//...
</code></pre>
</li>
</ol>
<h2 id="node-conditions"><a class="header" href="#node-conditions">Node Conditions</a></h2>
<ul>
<li><code>CloneAndAddCondition</code> logic seems erroneous ?</li>
</ul>
<h2 id="volumeattachment-1"><a class="header" href="#volumeattachment-1">VolumeAttachment</a></h2>
<pre><code class="language-go">func (v *VolumeAttachmentHandler) dispatch(obj interface{}) {
//...
volumeAttachment := obj.(*storagev1.VolumeAttachment)
	if volumeAttachment == nil {
		klog.Errorf(&quot;Couldn't convert to volumeAttachment from object %v&quot;, obj)
		// Should return here.
	}
//...
</code></pre>
<h4 id="dead-reconcileclusternodekey"><a class="header" href="#dead-reconcileclusternodekey">Dead? reconcileClusterNodeKey</a></h4>
<p>This just delegates to <code>reconcileClusterNode</code> which does nothing..</p>
<pre><code class="language-go">func (c *controller) reconcileClusterNode(node *v1.Node) error {
	return nil
}

</code></pre>
<h3 id="dead-machinego--triggerupdationflow"><a class="header" href="#dead-machinego--triggerupdationflow">Dead? machine.go | triggerUpdationFlow</a></h3>
<p>Can't find usages</p>
<h2 id="duplicate-initialization-of-eventrecorder-in-mc"><a class="header" href="#duplicate-initialization-of-eventrecorder-in-mc">Duplicate Initialization of EventRecorder in MC</a></h2>
<p><code>pkg/util/provider/app.createRecorder</code> already dones this below.</p>
<pre><code class="language-go">func createRecorder(kubeClient *kubernetes.Clientset) record.EventRecorder {
eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: v1core.New(kubeClient.CoreV1().RESTClient()).Events(&quot;&quot;)})
	return eventBroadcaster.NewRecorder(kubescheme.Scheme, v1.EventSource{Component: controllerManagerAgentName})
}
</code></pre>
<p>We get the recorder from this eventBroadcaster and then pass it to the <code>pkg/util/provider/machinecontroller/controller.NewController</code> method which again does:</p>
<pre><code class="language-go">	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&amp;typedcorev1.EventSinkImpl{Interface: typedcorev1.New(controlCoreClient.CoreV1().RESTClient()).Events(namespace)})
</code></pre>
<p>The above is useless.</p>
<h3 id="q-internal-to-external-scheme-conversion"><a class="header" href="#q-internal-to-external-scheme-conversion">Q? Internal to External Scheme Conversion</a></h3>
<p>Why do we do this ?</p>
<pre><code class="language-go">internalClass := &amp;machine.MachineClass{}
	err := c.internalExternalScheme.Convert(class, internalClass, nil)
	if err != nil {
		return err
	}
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
